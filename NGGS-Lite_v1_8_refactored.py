#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NGGS-Lite v1.8.0: Neo-Gothic Text Generation System (Gemini Edition)

Purpose:
Generates high-quality Gothic-style text using Google Gemini,
incorporating iterative evaluation and improvement based on
ETI (Existential Tremor Index), RI (Readability Index), Layer Balance,
Phase Transitions, Subjectivity, and Emotion Arc.

v1.8.0 Key Improvements (Based on revised plan):
- **Testing Foundation**: Focus on establishing a robust testing framework
  (though tests themselves are implemented separately). Code structured for testability.
- **Evaluation Tuning**: Emphasis on validating and tuning existing v1.7 evaluation
  heuristics rather than implementing entirely new complex algorithms.
- **Standardized Integration (Basis)**: Defined JSON schema concept and basic
  NDGS input parser for essential context. File-based exchange assumed.
- **Vocabulary Feedback (Data Collection)**: Implemented mechanism to collect
  and output vocabulary usage data for future GLCAI analysis.
- **Refinement**: Continued code cleanup, configuration management, and error handling.
"""

# =============================================================================
# Part 1: Imports, Constants, Core Configuration (NGGSConfig)
# =============================================================================

import os
import sys
import json
import argparse
import logging
import logging.handlers
import time
import signal
import re
import random
import unicodedata
import traceback
import math
import statistics
import pathlib
from typing import (
    Dict, List, Any, Optional, Union, Tuple, Generic, TypeVar,
    Callable, Set, Type, Final
)
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from enum import Enum
from abc import ABC, abstractmethod

# --- Attempt to import Google Generative AI library ---
try:
    import google.generativeai as genai
    from google.api_core import exceptions as google_exceptions
    GEMINI_AVAILABLE: Final[bool] = True
except ImportError:
    print("ERROR: google-generativeai library not found.", file=sys.stderr)
    print("Please install it using: pip install google-generativeai", file=sys.stderr)
    # Define dummy classes for type hints and basic checks if library is missing
    class google_exceptions:  # type: ignore
        class GoogleAPIError(Exception): pass
        class ResourceExhausted(GoogleAPIError): pass
        class InternalServerError(GoogleAPIError): pass
        class ServiceUnavailable(GoogleAPIError): pass
        class DeadlineExceeded(GoogleAPIError): pass
        class InvalidArgument(GoogleAPIError): pass
        class PermissionDenied(GoogleAPIError): pass
        class Unauthenticated(GoogleAPIError): pass
        class NotFound(GoogleAPIError): pass
        class FailedPrecondition(GoogleAPIError): pass
    genai = None  # type: ignore
    GEMINI_AVAILABLE = False

# --- Attempt to import OpenAI library (Placeholder) ---
try:
    import openai  # type: ignore
    from openai import RateLimitError, APIError, APITimeoutError, BadRequestError, AuthenticationError  # type: ignore
    OPENAI_AVAILABLE: Final[bool] = True
except ImportError:
    openai = None
    # Define dummy classes for type hints if needed
    class RateLimitError(Exception): pass  # type: ignore
    class APIError(Exception): pass  # type: ignore
    class APITimeoutError(Exception): pass  # type: ignore
    class BadRequestError(Exception): pass  # type: ignore
    class AuthenticationError(Exception): pass  # type: ignore
    OPENAI_AVAILABLE = False

# --- Base Directory ---
# Resolve the directory where the script resides
try:
    BASE_DIR: Final[pathlib.Path] = pathlib.Path(__file__).resolve().parent
except NameError:
    # Fallback for environments where __file__ might not be defined (e.g., interactive)
    BASE_DIR = pathlib.Path.cwd()

# --- Core Configuration Class (NGGSConfig v1.8.0) ---
# Type Alias for complex structures
JsonDict = Dict[str, Any]


@dataclass
class NGGSConfig:
    """
    NGGS-Lite v1.8.0 Configuration - Centralized parameters.
    Focus on testability, refined defaults, and clear path definitions.
    This is the single, consolidated definition of NGGSConfig.
    """
    # --- System Version ---
    VERSION: str = "1.8.0"

    # --- LLM Engine Configuration ---
    LLM_ENGINE: str = "gemini"  # Primary engine ('gemini')

    # --- Gemini API Settings ---
    GEMINI_API_KEY_ENV: str = "GENAI_API_KEY"
    GEMINI_API_KEY: Optional[str] = os.environ.get(GEMINI_API_KEY_ENV)
    GEMINI_MODEL_NAME: str = "models/gemini-1.5-pro-latest"  # Default to latest Pro
    GEMINI_RPM_LIMIT: int = int(os.environ.get("GENAI_RPM_LIMIT", "30"))  # Requests Per Minute

    # --- API Retry & Timeout Settings (Common) ---
    API_MAX_RETRIES: int = 4  # Reduced default retries slightly
    API_BASE_RETRY_DELAY: float = 1.5  # seconds
    API_MAX_RETRY_DELAY: float = 45.0  # seconds
    API_RATE_LIMIT_DELAY: float = 10.0  # seconds (Initial delay on ResourceExhausted/RateLimitError)
    API_TIMEOUT: int = 300  # seconds for API call

    # --- Generation Parameters (Gemini Defaults - Refined v1.8) ---
    GENERATION_CONFIG_DEFAULT: JsonDict = field(default_factory=lambda: {
        "temperature": 0.80,  # Slightly lower default temp for more focused output
        "top_p": 0.95,
        "top_k": 40,  # Standard Top-K
        "max_output_tokens": 8192,  # Standard for recent models
        "candidate_count": 1,
        # "stop_sequences": [] # Typically not needed for single turn
    })
    # Evaluation should be less creative
    EVALUATION_TEMPERATURE: float = 0.20
    # Improvement loop temperature settings
    IMPROVEMENT_BASE_TEMPERATURE: float = 0.70  # Base temp for improvement loops
    IMPROVEMENT_MIN_TEMPERATURE: float = 0.55  # Minimum temp during improvement
    IMPROVEMENT_TEMP_DECREASE_PER_LOOP: float = 0.05  # How much temp decreases each loop

    # --- Text Generation & Evaluation Control ---
    DEFAULT_TARGET_LENGTH: int = 1200  # Increased default target length slightly
    DEFAULT_MAX_LOOPS: int = 3  # Default number of improvement loops
    DEFAULT_IMPROVEMENT_THRESHOLD: float = 4.3  # Overall score target to stop loops early

    # --- ETI (Extended) Evaluation Weights ---
    # Weights define relative importance of each ETI component
    EXTENDED_ETI_WEIGHTS: Dict[str, float] = field(default_factory=lambda: {
        "境界性": 0.15, "両義性": 0.15, "超越侵犯": 0.15,
        "不確定性": 0.15, "内的変容": 0.15,
        "位相移行": 0.15,  # Heuristic score for phase transition quality
        "主観性": 0.10  # Heuristic score for subjective depth
    })

    # --- Phase Distribution Targets & Scoring ---
    # Target ratios for different narrative phases
    PHASE_BALANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        "serif": 0.15, "live_report": 0.25, "monologue": 0.35,
        "narration": 0.20, "serif_prime": 0.05
    })
    PHASE_DEVIATION_TOLERANCE: float = 0.10  # Allowable deviation from target ratio
    PHASE_SCORE_DIVERSITY_BONUS: float = 0.75  # Max bonus for having multiple phases present
    PHASE_SCORE_BALANCE_PENALTY_FACTOR: float = 3.0  # Multiplier for penalty due to deviation
    PHASE_SCORE_FOCUS_TARGET_RATIO: float = 0.40  # Target ratio when focusing on a specific phase

    # --- Layer Distribution Targets & Scoring (Gothic-specific) ---
    # Target ratios for different descriptive layers
    LAYER_BALANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        "physical": 0.20, "sensory": 0.25, "psychological": 0.35,  # Increased psychological target
        "symbolic": 0.20
    })
    LAYER_DEVIATION_TOLERANCE: float = 0.08  # Allowable deviation from target ratio
    LAYER_BALANCE_PENALTY_FACTOR: float = 3.5  # Multiplier for penalty due to deviation

    # --- File Paths & Directory Settings ---
    DEFAULT_OUTPUT_DIR: str = "./nggs_lite_output_v1.8"  # Updated version in path
    DEFAULT_TEMPLATES_DIR: str = "./templates_v1.8"  # Versioned templates recommended
    DEFAULT_VOCAB_PATH: str = "./data/gothic_vocabulary_v1.8.json"  # Versioned vocab recommended
    GLCAI_VOCAB_PATH: str = "./data/glcai_export_latest.json"  # Standard path for GLCAI export
    VOCAB_LOAD_PRIORITY: List[str] = field(default_factory=lambda: ['glcai', 'default'])  # Priority: GLCAI > Default File > Internal (if any)
    RESUME_DIR_NAME: str = "resume"
    STATS_DIR_NAME: str = "stats"
    PROMPT_DIR_NAME: str = "prompts"
    HTML_REPORT_DIR_NAME: str = "reports"  # Dedicated dir for HTML reports
    GLCAI_FEEDBACK_DIR_NAME: str = "glcai_feedback"  # Dedicated dir for feedback files [v1.8]
    STATS_FILENAME: str = "generation_stats_v1_8.jsonl"  # Versioned stats file
    RESUME_SUFFIX: str = ".resume.json"  # More descriptive suffix
    BACKUP_SUFFIX_FORMAT: str = ".bak.{timestamp}"  # Timestamp format: YYYYMMDD_HHMMSS_ms
    LOCK_SUFFIX: str = ".lock"
    CORRUPTED_SUFFIX: str = ".corrupted"

    # --- HTML Report Settings ---
    REPORT_CHART_COLORS: Dict[str, str] = field(default_factory=lambda: {
        # Phase Colors (Adjusted for clarity)
        'serif': '#4e79a7',  # Blue
        'live_report': '#f28e2c',  # Orange
        'monologue': '#e15759',  # Red
        'narration': '#76b7b2',  # Teal
        'serif_prime': '#59a14f',  # Green
        # Layer Colors (Distinct)
        'physical': '#edc949',  # Yellow
        'sensory': '#af7aa1',  # Purple
        'psychological': '#ff9da7',  # Pink
        'symbolic': '#9c755f',  # Brown
        # Default/Other
        'other': '#bab0ab'  # Grey
    })

    # --- Logging Configuration ---
    LOG_LEVELS: Dict[str, int] = field(default_factory=lambda: {
        "DEBUG": logging.DEBUG, "INFO": logging.INFO,
        "WARNING": logging.WARNING, "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL
    })
    DEFAULT_LOG_LEVEL: str = "INFO"
    DEFAULT_LOG_FILE: Optional[str] = "nggs_lite_v1_8.log"  # Versioned log file
    LOG_FORMAT: str = '%(asctime)s.%(msecs)03dZ [%(levelname)-7s] [%(name)-25s:%(lineno)d] %(message)s'  # Added ms and UTC marker
    LOG_DATE_FORMAT: str = '%Y-%m-%dT%H:%M:%S'  # ISO 8601 format for UTC

    # --- Processing settings (Maintained for compatibility, but prefer new defaults) ---
    # These might be overridden by args or future config files
    FEEDBACK_LOOPS: int = 3  # Align with DEFAULT_MAX_LOOPS
    MIN_FEEDBACK_LOOPS: int = 1  # Minimum loops before threshold check applies
    MIN_SCORE_THRESHOLD: float = 4.3  # Align with DEFAULT_IMPROVEMENT_THRESHOLD
    SAVE_PROMPTS: bool = True  # Default to saving prompts

    # --- Constants Moved from Global Scope (Refined for v1.8) ---
    # Layer keywords (Used by NarrativeFlowFramework heuristic - review needed)
    LAYER_KEYWORDS: Dict[str, List[str]] = field(default_factory=lambda: {
        "physical": ["石", "壁", "床", "扉", "窓", "本", "階段", "建物", "部屋", "道", "体", "手", "足", "指", "物", "布", "金属", "木", "椅子", "机", "燭台", "鍵", "杯", "服", "人形", "街", "路地", "城", "塔", "地下室"],
        "sensory": ["光", "影", "音", "声", "匂い", "香り", "味", "風", "冷た", "熱", "湿", "暗", "明", "色", "霧", "月", "見", "聞", "触", "ざら", "ぬめ", "響き", "静寂", "囁き", "呻き", "雨", "足音", "きしみ", "赤", "黒", "白", "甘美", "異臭", "冷気", "暖か", "視線", "気配", "腐臭", "黴"],
        "psychological": ["思う", "考え", "感じ", "記憶", "夢", "不安", "恐怖", "悲し", "喜び", "怒り", "驚き", "戸惑", "心", "意識", "感情", "懐かし", "孤独", "混乱", "疑問", "葛藤", "忘却", "心理", "憂愁", "狂気", "苦悶", "絶望", "衝動", "予感", "直感", "眩暈", "罪悪", "後悔", "執着", "願望", "メランコリ", "虚無感", "既視感"],
        "symbolic": ["鏡", "象徴", "時間", "運命", "境界", "魂", "意味", "存在", "二重", "深淵", "影", "扉", "窓", "迷宮", "輪廻", "契約", "封印", "呪い", "永遠", "生", "死", "謎", "虚無", "回帰", "螺旋", "血", "薔薇", "時計", "仮面", "鍵", "アルカナ", "グリモワール", "宇宙", "秩序", "混沌", "宿命", "真実"]
    })

    # Subjectivity keywords (Used by SubjectiveEvaluator heuristic)
    SUBJECTIVE_INNER_KEYWORDS: List[str] = field(default_factory=lambda: [
        '思う', '感じる', '考える', '気がする', '気づく', '思い出す', '忘れる', '見える', '聞こえる',
        '匂いがする', '味がする', '触れる', '懐かしい', '不安', '恐怖', '喜び', '悲しみ', '怒り',
        '驚き', '戸惑い', '心', '気持ち', '記憶', '意識', '夢', '幻', '願い', '望み',
        '期待', '怯え', '悩み', '迷い', '混乱', '疑問', '疑い', '違和感', '理解', '認識',
        '知覚', '直感', '衝動', '欲望', '希望', '憧れ', '後悔', '寂しい', '切ない', '虚しい',
        'ぞくぞく', '予感', '確かめたい', '信じられない', 'なぜ', 'どうして'  # Added more
    ])
    SUBJECTIVE_FIRST_PERSON_PRONOUNS: List[str] = field(default_factory=lambda: [
        '私', '僕', '俺', 'わたし', 'わたくし', 'あたし', '自分', 'ぼく', 'おれ', 'わし', '我', 'われ'
    ])
    SUBJECTIVE_THIRD_PERSON_PRONOUNS: List[str] = field(default_factory=lambda: [
        '彼', '彼女', 'やつ', 'あいつ', 'その男', 'その女', '例の'  # Added more examples
    ])

    # Fallback improvement instruction (Refined v1.8)
    DEFAULT_FALLBACK_IMPROVEMENT: str = """
# デフォルト改善指示 (Fallback v1.8)
現在のテキストを、より質の高い新ゴシック文体へと改善してください。特に以下の点を意識してください。

1.  **主観的語りの深化**: 登場人物の内面（思考、感情、感覚）を、行動や環境描写と結びつけながら、より深く掘り下げてください。
2.  **ゴシック的雰囲気の醸成**: 廃墟、影、霧、古物などを効果的に描写し、ゴシック特有の不安、神秘、退廃、あるいは美と恐怖の混在する雰囲気を強めてください。
3.  **間接的表現と暗示**: 感情や状況を直接説明せず、環境描写、行動、サブテキストを通じて暗示的に表現してください。読者の解釈の余地を残してください。
4.  **位相移行の調整**: セリフ、実況、モノローグ、ナレーション等の語りの位相を、物語の効果を高めるように自然に移行させてください。単調な繰り返しを避けてください。
5.  **語彙の選択**: よりゴシック的な響きを持つ語彙（例：深淵、月光、囁き、古城、憂愁、影、両義性、境界）を文脈に合わせて適切に使用してください。
6.  **層バランスの考慮**: 物質層、感覚層、心理層、象徴層の描写がバランス良く含まれるように調整してください。 """

    # --- Utility Methods inside Config ---
    # Note: Static methods might be better placed in a separate utils module,
    # but kept here for single-file simplicity as per v1.7 structure.

    @staticmethod
    def get_default_paths(base_dir_override: Optional[pathlib.Path] = None) -> Dict[str, pathlib.Path]:
        """Get default path settings based on BASE_DIR."""
        base = base_dir_override if base_dir_override else BASE_DIR
        # Use class attributes for default directory/file names
        return {
            "DEFAULT_OUTPUT_DIR": base / NGGSConfig.DEFAULT_OUTPUT_DIR,
            "DEFAULT_TEMPLATES_DIR": base / NGGSConfig.DEFAULT_TEMPLATES_DIR,
            "DEFAULT_VOCAB_PATH": base / NGGSConfig.DEFAULT_VOCAB_PATH,
            "GLCAI_VOCAB_PATH": base / NGGSConfig.GLCAI_VOCAB_PATH,
        }

    @classmethod
    def load_from_file(cls, filepath: Union[str, pathlib.Path]) -> 'NGGSConfig':
        """Loads configuration from a JSON file, overriding defaults."""
        # Placeholder for future implementation if external config files are used
        # Currently, config is mainly driven by class defaults and args
        path = pathlib.Path(filepath).resolve()
        if not path.is_file():
            raise FileNotFoundError(f"Settings file not found: {path}")
        try:
            if path.suffix.lower() == ".json":
                with open(path, 'r', encoding='utf-8') as f:
                    settings_data = json.load(f)
            else:
                raise ValueError(f"Unsupported settings file format: {path.suffix}")

            instance = cls()  # Start with defaults
            for key, value in settings_data.items():
                if hasattr(instance, key):
                    # Basic type check/conversion might be needed here
                    # For nested dataclasses/dicts, recursive update might be required
                    setattr(instance, key, value)
            # Use proper logging once logger is set up
            # print(f"Configuration loaded from {filepath}")
            logging.getLogger("NGGS-Lite.Config").info(f"Configuration loaded from {filepath}")
            return instance
        except Exception as e:
            # Use proper logging once logger is set up
            # print(f"Error loading settings from {filepath}: {e}", file=sys.stderr)
            logging.getLogger("NGGS-Lite.Config").error(f"Error loading settings from {filepath}: {e}")
            # Define ConfigurationError or ensure it's defined before this class
            raise ConfigurationError(f"Error loading settings from {filepath}: {e}") from e

    @staticmethod
    def generate_job_id(prefix: str = "nggsv18_") -> str:
        """Generates a unique job ID with microseconds for better uniqueness."""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_%f")
        return f"{prefix}{timestamp}"

    @classmethod
    def initialize_base_directories(cls, base_output_dir: pathlib.Path, config: 'NGGSConfig') -> None:
        """Creates base output directories defined in the config."""
        # Use directory names from the passed config instance
        dirs_to_create = [
            base_output_dir,
            base_output_dir / config.RESUME_DIR_NAME,
            base_output_dir / config.STATS_DIR_NAME,
            base_output_dir / config.PROMPT_DIR_NAME,
            base_output_dir / config.HTML_REPORT_DIR_NAME,
            base_output_dir / config.GLCAI_FEEDBACK_DIR_NAME,  # Add feedback dir [v1.8]
        ]
        # Ensure logger is available or use print
        logger_init = logging.getLogger("NGGS-Lite.Init")
        logger_init.info(f"Ensuring base directories exist under: {base_output_dir}")
        for dir_path in dirs_to_create:
            try:
                dir_path.mkdir(parents=True, exist_ok=True)
                logger_init.debug(f"Directory verified/created: {dir_path}")
            except PermissionError as e:
                err_msg = f"ディレクトリ作成権限エラー: {dir_path}"
                logger_init.critical(err_msg)
                raise ConfigurationError(err_msg) from e
            except Exception as e:
                err_msg = f"ディレクトリ作成中のエラー: {dir_path} - {e}"
                logger_init.critical(err_msg)
                raise ConfigurationError(err_msg) from e
        logger_init.debug("Base directory initialization complete.")

# =============================================================================
# Part 1 End: Imports, Constants, Core Configuration (NGGSConfig Base)
# (NGGSConfig class definition is now complete and consolidated here)
# =============================================================================
# =============================================================================
# Part 2: Core Exceptions and Result Type
# (NGGSConfig class was consolidated and moved to Part 1)
# =============================================================================

# --- Custom Exception Classes (Hierarchy Refined v1.8) ---
class NGGSError(Exception):
    """Base exception class for NGGS-Lite."""
    def __init__(self, message: str, details: Optional[JsonDict] = None):
        super().__init__(message)
        self.details = details if details is not None else {}

    def get_context(self) -> JsonDict:
        """Returns error context details."""
        return self.details

    def __str__(self) -> str:
        base_message = super().__str__()
        if self.details:
            try:
                # Limit detail length for concise logging
                details_str = ", ".join(
                    f"{k}={str(v)[:50]}{'...' if len(str(v)) > 50 else ''}"
                    for k, v in self.details.items()
                )
                return f"{base_message} (Context: {details_str})"
            except Exception:  # Fallback if details formatting fails
                return f"{base_message} (Context available but failed to format)"
        return base_message


class ConfigurationError(NGGSError):
    """Errors related to configuration or setup."""
    pass


class FileProcessingError(NGGSError):
    """Errors during file reading or writing."""
    pass


class TemplateError(NGGSError):
    """Errors related to template loading or validation."""
    pass


class VocabularyError(NGGSError):
    """Errors related to vocabulary processing."""
    pass


class LLMError(NGGSError):
    """Base class for errors during LLM interaction."""
    pass


class GenerationError(LLMError):
    """Errors specifically during the text generation phase."""
    pass


class EvaluationError(LLMError):
    """Errors during text evaluation or parsing evaluation results."""
    pass


class JsonParsingError(EvaluationError):
    """Specific error for failures in parsing JSON from LLM evaluation."""
    pass


class PhaseTransitionError(NGGSError):
    """Errors related to narrative phase analysis or transition."""
    pass


class StrategyError(NGGSError):
    """Errors related to improvement strategy selection or generation."""
    pass


class IntegrationError(NGGSError):  # New for v1.8
    """Errors related to interactions between NGGS and other systems (NDGS/GLCAI)."""
    pass

# --- Error Severity Definition ---
class ErrorSeverity(Enum):
    """Represents the severity of an error."""
    FATAL = "fatal"          # Stops the entire process
    LOOP = "loop"            # Skips the current generation/improvement loop
    RECOVERABLE = "recoverable"  # Allows fallback or default values

# --- Error Severity Mapping (v1.8 Adjusted) ---
# Maps exception types to their severity for centralized handling.
# Note: google_exceptions will be handled based on GEMINI_AVAILABLE flag
_ERROR_SEVERITY_MAP_BASE: Dict[Type[Exception], ErrorSeverity] = {
    # NGGS-Lite Specific
    ConfigurationError: ErrorSeverity.FATAL,
    FileProcessingError: ErrorSeverity.FATAL,  # Most file errors are fatal for setup/output
    TemplateError: ErrorSeverity.FATAL,  # Templates are essential
    VocabularyError: ErrorSeverity.RECOVERABLE,  # Can proceed with defaults potentially
    IntegrationError: ErrorSeverity.RECOVERABLE,  # Allow processing to continue if integration fails? Or LOOP? -> Recoverable for now.
    GenerationError: ErrorSeverity.LOOP,  # Skip loop on generation failure
    EvaluationError: ErrorSeverity.RECOVERABLE,  # Use default scores if base eval fails
    JsonParsingError: ErrorSeverity.RECOVERABLE,  # Use default scores
    PhaseTransitionError: ErrorSeverity.RECOVERABLE,  # Use default score/dist
    StrategyError: ErrorSeverity.RECOVERABLE,  # Fallback to standard LLM instructions
    LLMError: ErrorSeverity.LOOP,  # General LLM errors are loop-skippable

    # Python Standard / Dependencies
    FileNotFoundError: ErrorSeverity.FATAL,  # If essential file missing
    PermissionError: ErrorSeverity.FATAL,
    IOError: ErrorSeverity.FATAL,
    OSError: ErrorSeverity.FATAL,  # Includes directory errors
    json.JSONDecodeError: ErrorSeverity.RECOVERABLE,  # Context-dependent recovery
    TimeoutError: ErrorSeverity.LOOP,  # Network/API timeout
    ConnectionError: ErrorSeverity.LOOP,  # Network connection issue
    TypeError: ErrorSeverity.RECOVERABLE,  # Often data format issues
    KeyError: ErrorSeverity.RECOVERABLE,  # Often dict access issues
    ValueError: ErrorSeverity.RECOVERABLE,  # Context-dependent (e.g., safety block vs data issue)
    AttributeError: ErrorSeverity.RECOVERABLE,
    IndexError: ErrorSeverity.RECOVERABLE,
    RuntimeError: ErrorSeverity.LOOP,  # Often recoverable within a loop
    NotImplementedError: ErrorSeverity.FATAL,  # Feature not ready

    # Default for unmapped exceptions
    Exception: ErrorSeverity.FATAL
}

# Add Gemini-specific exceptions if the library is available
ERROR_SEVERITY_MAP: Dict[Type[Exception], ErrorSeverity] = _ERROR_SEVERITY_MAP_BASE.copy()
if GEMINI_AVAILABLE:
    ERROR_SEVERITY_MAP.update({
        google_exceptions.ResourceExhausted: ErrorSeverity.LOOP,
        google_exceptions.InternalServerError: ErrorSeverity.LOOP,
        google_exceptions.ServiceUnavailable: ErrorSeverity.LOOP,
        google_exceptions.DeadlineExceeded: ErrorSeverity.LOOP,
        google_exceptions.InvalidArgument: ErrorSeverity.FATAL,  # Bad request - unlikely retryable
        google_exceptions.PermissionDenied: ErrorSeverity.FATAL,
        google_exceptions.Unauthenticated: ErrorSeverity.FATAL,
        google_exceptions.NotFound: ErrorSeverity.FATAL,  # e.g., Model not found
        google_exceptions.FailedPrecondition: ErrorSeverity.FATAL,
    })
else:
    # If Gemini is not available, map the dummy google_exceptions
    # (This assumes the dummy classes are defined as in Part 1)
    ERROR_SEVERITY_MAP.update({
        # Assuming google_exceptions refers to the dummy class defined when import fails
        google_exceptions.ResourceExhausted: ErrorSeverity.LOOP,  # type: ignore
        google_exceptions.InternalServerError: ErrorSeverity.LOOP,  # type: ignore
        google_exceptions.ServiceUnavailable: ErrorSeverity.LOOP,  # type: ignore
        google_exceptions.DeadlineExceeded: ErrorSeverity.LOOP,  # type: ignore
        google_exceptions.InvalidArgument: ErrorSeverity.FATAL,  # type: ignore
        google_exceptions.PermissionDenied: ErrorSeverity.FATAL,  # type: ignore
        google_exceptions.Unauthenticated: ErrorSeverity.FATAL,  # type: ignore
        google_exceptions.NotFound: ErrorSeverity.FATAL,  # type: ignore
        google_exceptions.FailedPrecondition: ErrorSeverity.FATAL,  # type: ignore
    })


# --- Result Type (Generic for Success/Error Handling) ---
T = TypeVar('T')
E = TypeVar('E', bound=Exception)


@dataclass(frozen=True)
class Result(Generic[T, E]):
    """
    Generic Result type inspired by Rust's std::result::Result.
    Holds either success (Ok) or error (Fail).
    Ensures error is always an Exception instance. Immutable.
    """
    _success: bool
    _value: Optional[T] = field(default=None, compare=False, repr=False)  # Don't show value in repr for brevity
    _error: Optional[E] = field(default=None, compare=False)

    @property
    def is_ok(self) -> bool:
        return self._success

    @property
    def is_err(self) -> bool:
        return not self._success

    @property
    def value(self) -> Optional[T]:
        return self._value if self.is_ok else None

    @property
    def error(self) -> Optional[E]:
        return self._error if self.is_err else None

    @classmethod
    def ok(cls, value: T) -> 'Result[T, Exception]':  # Default error type is Exception
        # Ensure value is not None for Ok Results intended to hold a value
        # This behavior can be debated; for now, allow Ok(None)
        # if value is None:
        #     # Optional: Raise or log warning if Ok is created with None value unintentionally
        #     # logging.getLogger("NGGS-Lite.Core").warning("Created Ok Result with None value.")
        #     pass
        return cls(_success=True, _value=value, _error=None)

    @classmethod
    def fail(cls, error: E) -> 'Result[Any, E]':  # Value type is Any for Fail
        if not isinstance(error, Exception):
            # Ensure the error is always an exception instance
            err_msg = f"Result.fail requires an Exception, got {type(error)}. Wrapping in NGGSError."
            try:
                # Use a generic logger name if NGGS-Lite specific logger isn't configured yet
                logger_core = logging.getLogger("NGGS-Lite.Core") if logging.getLogger("NGGS-Lite.Core").hasHandlers() else logging.getLogger(__name__)
                logger_core.error(err_msg)
            except Exception:  # Logger might not be ready
                print(f"ERROR (Result.fail): {err_msg}", file=sys.stderr)
            # Wrap non-exception errors
            error = NGGSError(err_msg, details={"original_type": str(type(error))})  # type: ignore
        return cls(_success=False, _value=None, _error=error)

    # Keep original Err method alias for backward compatibility if used
    @classmethod
    def Err(cls, error: E) -> 'Result[Any, E]':
        # pylint: disable=invalid-name
        return cls.fail(error)

    def unwrap(self) -> T:
        """Returns the value if Ok, otherwise raises the error."""
        if self.is_ok:
            # Allow unwrap to return None if that was the intended Ok value
            # if self._value is None: # This check can be too restrictive
            #     raise TypeError("Called unwrap on an Ok Result containing None where a non-None value was expected.")
            return self._value  # type: ignore # Caller must handle potential None if T is not Optional[Something]
        else:
            if self.error is not None:
                raise self.error
            else:
                # This case should theoretically not happen due to fail() validation
                raise NGGSError("Called unwrap on an Err Result with no error object.")

    def unwrap_or(self, default: T) -> T:
        """Returns the value if Ok, otherwise returns the default."""
        return self._value if self.is_ok and self._value is not None else default

    def expect(self, msg: str) -> T:
        """Returns the value if Ok, otherwise raises NGGSError with custom message."""
        if self.is_ok:
            # Allow expect to return None if that was the intended Ok value
            return self._value  # type: ignore
        else:
            # Raise NGGSError, preserving original error details if possible
            original_error_str = str(self.error) if self.error else "None"
            error_details = {"original_error": original_error_str}
            if isinstance(self.error, NGGSError):
                error_details.update(self.error.get_context())

            raise NGGSError(msg, details=error_details) from self.error

# --- LLM Response Dataclass ---
@dataclass
class LLMResponse:
    """Standardized data class for LLM responses."""
    text: str
    metadata: JsonDict = field(default_factory=dict)  # Model, tokens, etc.

    def to_result(self) -> Result[str, GenerationError]:
        """Converts LLMResponse to a Result object."""
        if not self.text or not self.text.strip():
            details = {"metadata": self.metadata} if self.metadata else {}
            return Result.fail(GenerationError("LLM returned empty text content", details=details))
        return Result.ok(self.text.strip()) # Return stripped text for consistency


# =============================================================================
# Part 2 End: Core Exceptions and Result Type
# =============================================================================
# =============================================================================
# Part 3: Utility Functions (v1.8 Refined)
# =============================================================================

# --- Helper Classes (Defined here for use in Utils & JSON serialization) ---

class SafeDict(dict):
    """
    A dictionary subclass that returns '{key}' instead of raising KeyError.
    Used primarily for string formatting with potentially missing keys.
    """
    def __missing__(self, key: str) -> str:
        # Use basic logging as logger instance might not be configured yet
        try:
            # Attempt to use the NGGS-Lite logger if available
            logger_utils = logging.getLogger("NGGS-Lite.Utils")
            if logger_utils.hasHandlers():  # Check if logger is configured
                logger_utils.debug(f"SafeDict missing key: {key}")
            # else: # Fallback print if logger isn't ready (avoid for production)
            # print(f"DEBUG (SafeDict): Missing key: {key}", file=sys.stderr)
        except Exception:
            # Fallback if logging access itself causes an error (e.g., during very early init)
            # print(f"DEBUG (SafeDict - logging error): Missing key: {key}", file=sys.stderr)
            pass  # Ignore if logging is not yet set up or causes issues
        return f'{{{key}}}'  # Return the key itself if missing


class CompactJSONEncoder(json.JSONEncoder):
    """
    Custom JSON encoder to handle specific types (Path, EvaluationResult-like, timedelta, datetime)
    and produce compact output, suitable for logs or prompts.
    """
    def default(self, obj: Any) -> Any:
        if isinstance(obj, pathlib.Path):
            return str(obj)
        # Use duck typing for EvaluationResult-like objects
        # Check for attributes common to EvaluationResult structure
        if hasattr(obj, 'scores') and hasattr(obj, 'reasoning'):
            # Safely get attributes, providing defaults if missing
            scores_dict = getattr(obj, 'scores', {})
            analysis_str = getattr(obj, 'analysis', None) # analysis can be None
            components_dict = getattr(obj, 'components', {})
            distribution_dict = getattr(obj, 'distribution', {})
            confidence_dict = getattr(obj, 'confidence', {}) # Added confidence for v1.8

            return {
                # Ensure scores is a dict, handle potential non-dict scores attribute
                "scores": scores_dict if isinstance(scores_dict, dict) else {},
                # Truncate analysis safely using utility function
                "analysis": truncate_text(analysis_str, 150) if analysis_str else None,
                # Ensure components and distribution are dicts
                "components": components_dict if isinstance(components_dict, dict) else {},
                "distribution": distribution_dict if isinstance(distribution_dict, dict) else {},
                "confidence": confidence_dict if isinstance(confidence_dict, dict) else {},
            }
        if isinstance(obj, timedelta):
            return obj.total_seconds()
        if isinstance(obj, datetime):  # Handle datetime objects
            # Ensure timezone-aware, default to UTC if naive
            if obj.tzinfo is None:
                obj = obj.replace(tzinfo=timezone.utc)
            return obj.isoformat().replace('+00:00', 'Z') # Standard ISO format with Z for UTC

        # Let the base class default method raise the TypeError for other types
        try:
            return super().default(obj)
        except TypeError:
            # Represent other unserializable objects concisely
            return f"<{type(obj).__name__} object (unserializable)>"


# --- Logging Setup Function ---
# Global logger instance, configured by setup_logging
logger: logging.Logger = logging.getLogger("NGGS-Lite")  # Placeholder

def setup_logging(
    log_level_str: str = "INFO", # Default to INFO if config not available early
    log_file: Optional[str] = "nggs_lite_v1_8.log", # Default log file name
    name: str = "NGGS-Lite",
    config_instance: Optional[NGGSConfig] = None # Changed 'config' to 'config_instance'
    ) -> logging.Logger:
    """
    Sets up application-wide logging using NGGSConfig defaults or provided config.
    Logs to both console and optionally to a rotating file.

    Args:
        log_level_str: Logging level string (e.g., "INFO", "DEBUG").
        log_file: Path to the log file, or None to disable file logging.
                  Set to 'NONE' (case-insensitive) to explicitly disable.
        name: Name for the logger instance.
        config_instance: Optional NGGSConfig instance to use for settings.

    Returns:
        Initialized logging.Logger instance.
    """
    # Use provided config_instance or create a default NGGSConfig if None
    # This ensures NGGSConfig is available for defaults.
    effective_config: NGGSConfig
    if config_instance is None:
        try:
            effective_config = NGGSConfig()  # Create default config if not provided
        except Exception as e:
            # Cannot even create default config, critical failure
            print(f"CRITICAL: Failed to create default NGGSConfig for logging: {e}", file=sys.stderr)
            # Fallback to basic logging config
            logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
            return logging.getLogger(name)
    else:
        effective_config = config_instance

    # Get defaults from the effective_config
    final_log_level_str = log_level_str if log_level_str else effective_config.DEFAULT_LOG_LEVEL
    final_log_file = log_file if log_file is not None else effective_config.DEFAULT_LOG_FILE
    log_format = effective_config.LOG_FORMAT
    log_date_format = effective_config.LOG_DATE_FORMAT

    log_level = effective_config.LOG_LEVELS.get(final_log_level_str.upper(), logging.INFO)
    root_logger = logging.getLogger(name)  # Get logger instance

    # --- Configure root logger ---
    # Check if handlers already exist (e.g., if called multiple times)
    if root_logger.hasHandlers():
        for handler in root_logger.handlers[:]:
            try:
                handler.flush()
                handler.close()
            except Exception:
                pass  # Ignore errors during handler close
            root_logger.removeHandler(handler)

    root_logger.setLevel(log_level)  # Set level on the logger itself

    # Use UTC time for logs
    logging.Formatter.converter = time.gmtime
    formatter = logging.Formatter(log_format, datefmt=log_date_format)

    # Console Handler (always enabled)
    console_handler = logging.StreamHandler(sys.stdout)  # Use stdout for consistency
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # Determine effective log file path
    effective_log_file_path_str = None
    if final_log_file and isinstance(final_log_file, str) and final_log_file.upper() != 'NONE':
        effective_log_file_path_str = final_log_file

    # Rotating File Handler (optional)
    log_init_message = ""
    if effective_log_file_path_str:
        try:
            log_path = pathlib.Path(effective_log_file_path_str).resolve()
            log_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists

            # Use RotatingFileHandler
            file_handler = logging.handlers.RotatingFileHandler(
                log_path, maxBytes=5 * 1024 * 1024, backupCount=3, encoding='utf-8'
            )
            file_handler.setLevel(log_level)
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)
            log_init_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level)}. "
                f"Logging to console and file: {log_path}"
            )
        except PermissionError:
            # Use root_logger here as it's already configured with console handler
            root_logger.error(f"Permission denied to write log file: {effective_log_file_path_str}. File logging disabled.")
            log_init_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level)}. "
                f"Logging to console ONLY (File permission error)."
            )
        except Exception as e:
            root_logger.error(f"Failed to set up file logging to {effective_log_file_path_str}: {e}", exc_info=True)
            log_init_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level)}. "
                f"Logging to console ONLY (File setup error)."
            )
    else:
        log_init_message = f"Logging setup complete. Level: {logging.getLevelName(log_level)}. Logging to console only."

    # Log initialization message safely
    if root_logger.hasHandlers():
        root_logger.info(f"--- NGGS-Lite v{effective_config.VERSION} Logging Initialized ---")
        root_logger.info(log_init_message)
    else:  # Fallback if even console handler failed
        print(f"--- NGGS-Lite v{effective_config.VERSION} Logging Initialized ---", file=sys.stderr)
        print(log_init_message, file=sys.stderr)

    return root_logger

# --- Initialize logger globally ---
# This allows other modules to import and use the same logger instance
# It will be configured using defaults first, then reconfigured in main() if NGGSConfig is available
# Attempt to initialize with a default NGGSConfig instance.
try:
    _initial_config_for_logger = NGGSConfig()
    logger = setup_logging(config_instance=_initial_config_for_logger)
except Exception as _e:
    # Fallback if NGGSConfig itself fails at this very early stage
    print(f"Error during initial logger setup with NGGSConfig: {_e}", file=sys.stderr)
    logger = setup_logging() # Setup with hardcoded defaults


# --- Text Processing Utilities ---
def truncate_text(text: Optional[Any], max_length: int = 100) -> str:
    """
    Truncates text to max_length, adding ellipsis if needed.
    Handles None input and attempts to convert non-string input safely.

    Args:
        text: The input data, ideally a string or None.
        max_length: The maximum desired length of the output string.

    Returns:
        The truncated string with ellipsis, or the original string if shorter,
        or an empty string if input is None or cannot be converted to string.
    """
    if text is None:
        return ""
    # Ensure input is a string
    if not isinstance(text, str):
        try:
            text_str = str(text)
        except Exception:
            # Log error if conversion fails and logger is available
            try:
                logging.getLogger("NGGS-Lite.Utils").warning(
                    f"Failed to convert non-string input to string in truncate_text: {type(text)}"
                )
            except Exception: # Logger might not be ready
                pass
            return ""
    else:
        text_str = text

    if len(text_str) <= max_length:
        return text_str
    else:
        # Simple truncation with ellipsis
        if max_length < 3: # Not enough space for ellipsis
            return text_str[:max_length]
        return text_str[:max_length - 3] + "..."


def get_metric_display_name(key: Optional[str]) -> str:
    """
    Converts internal metric keys to human-readable Japanese names (v1.8).
    Handles None input and unknown keys gracefully.

    Args:
        key: The internal metric key (e.g., "overall_quality").

    Returns:
        A Japanese display name or a formatted version of the key.
    """
    if not key or not isinstance(key, str):
        return "不明な指標"

    # Expanded map for v1.8 including ETI/RI components and potential parameters
    display_map = {
        # Base Eval (LLM)
        "gothic_atmosphere": "ゴシック的雰囲気", "stylistic_gravity": "文体の荘重さ/質",
        "indirect_emotion": "感情表現の間接性", "vocabulary_richness": "語彙の質と豊かさ",
        "overall_quality": "全体的な質",
        "subjective_depth": "主観的語りの深さ", "phase_transition": "位相移行の自然さ",
        "colloquial_gothic_blend": "口語/ゴシック融合度", "layer_balance": "層バランス",
        "emotion_arc_quality": "感情変容の質",
        # Extended ETI Components & Totals
        "境界性": "境界性", "両義性": "両義性", "超越侵犯": "超越侵犯",
        "不確定性": "不確定性", "内的変容": "内的変容",
        "位相移行": "位相移行(ETI)", "主観性": "主観性(ETI)", # Renamed from original for consistency
        "eti_total_calculated": "ETI計算値", "eti_from_llm": "ETI (LLM評価)",
        "eti": "ETIスコア", # Allow short form for 'eti_total_calculated'
        # RI Components & Totals
        "clarity": "構造的明瞭性", "visualRhythm": "視覚的リズム",
        "emotionalFlow": "感情フロー", "cognitiveLoad": "認知処理負荷",
        "interpretiveResonance": "解釈的余韻",
        "ri_total_calculated": "RI計算値",
        "ri": "RIスコア", # Allow short form for 'ri_total_calculated'
        # Subjectivity Components & Totals
        "first_person_score": "一人称使用スコア", "inner_expression_score": "内的表現スコア",
        "monologue_quality_score": "モノローグ質スコア", "consistency_score": "視点一貫性スコア",
        "subjective_score": "主観性評価総合",
        "subjective": "主観性スコア", # Allow short form for 'subjective_score'
        # Derived Scores (from TextProcessor calculation)
        "phase_score": "位相スコア", "layer_balance_score": "層バランススコア",
        "emotion_arc_score": "感情変容スコア", "colloquial_score": "口語/ゴシック融合スコア",
        "phase": "位相スコア", # Short form
        "layer": "層バランススコア", # Short form
        "emotion": "感情変容スコア", # Short form
        "colloquial": "口語/ゴシック融合スコア", # Short form
        # Parameter names
        "target_length": "目標文字数", "perspective_mode": "視点モード",
        "phase_focus": "位相焦点", "colloquial_level": "口語レベル",
        "emotion_arc": "感情の弧", "max_loops": "最大ループ数",
        "improvement_threshold": "改善閾値", "llm_engine": "LLMエンジン",
        "llm_model": "LLMモデル", "skip_initial_generation": "初期生成スキップ",
        # Distribution keys
        "serif": "セリフ", "live_report": "実況", "monologue": "モノローグ",
        "narration": "ナレーション", "serif_prime": "セリフ（変容後）",
        "physical": "物質層", "sensory": "感覚層", "psychological": "心理層",
        "symbolic": "象徴層", "other": "その他"
    }

    # Return mapped name or formatted fallback
    if key in display_map:
        return display_map[key]
    else:
        # Simple fallback: Replace underscores with spaces, title case
        s = key.replace('_', ' ').strip()
        # Capitalize first letter of each word
        return ' '.join(word.capitalize() for word in s.split()) if s else "不明なキー"


# --- File I/O Utilities (Using pathlib, enhanced error handling) ---
def safe_read_file(file_path: Union[str, pathlib.Path], encoding: str = 'utf-8') -> Result[str, FileProcessingError]:
    """Reads a text file safely, returning Result object."""
    logger_io = logging.getLogger("NGGS-Lite.IO.Read")
    try:
        path = pathlib.Path(file_path).resolve()
        if not path.exists():
            # Specific error for file not found
            raise FileNotFoundError(f"指定されたファイルが見つかりません: {path}")
        if not path.is_file():
            # Specific error if path is a directory
            raise IsADirectoryError(f"指定されたパスはファイルではありません: {path}")
        # Explicit permission check
        if not os.access(path, os.R_OK):
            raise PermissionError(f"ファイルへの読み取り権限がありません: {path}")

        # Read content
        content = path.read_text(encoding=encoding, errors='replace')
        logger_io.debug(f"Successfully read {len(content)} chars from: {path.name}")
        return Result.ok(content)

    except (FileNotFoundError, IsADirectoryError, PermissionError) as e:
        # These are specific file access issues, more critical than general IO errors
        err = FileProcessingError(f"ファイルアクセスエラー: {e}", details={"path": str(file_path)})
        logger_io.error(str(err)) # Log as error
        return Result.fail(err)
    except IOError as e:  # Catch broader IO errors (e.g., disk full during read?)
        err = FileProcessingError(f"ファイル読み込みIOエラー {file_path}: {e}", details={"path": str(file_path)})
        logger_io.error(str(err), exc_info=False)  # Less verbose traceback for common IO errors
        return Result.fail(err)
    except Exception as e:  # Catch unexpected errors
        err = FileProcessingError(f"ファイル読み込み中の予期せぬエラー {file_path}: {e}", details={"path": str(file_path)})
        logger_io.error(str(err), exc_info=True)
        return Result.fail(err)


def safe_write_file(
    file_path: Union[str, pathlib.Path],
    content: str,
    encoding: str = 'utf-8',
    create_backup: bool = False
    ) -> Result[pathlib.Path, FileProcessingError]:
    """Writes text content safely to a file, with optional backup."""
    logger_io = logging.getLogger("NGGS-Lite.IO.Write")
    backup_path_obj: Optional[pathlib.Path] = None  # Track backup path

    try:
        path = pathlib.Path(file_path).resolve()
        parent_dir = path.parent

        # Ensure parent directory exists and is writable
        if not parent_dir.exists():
            logger_io.info(f"Creating parent directory: {parent_dir}")
            parent_dir.mkdir(parents=True, exist_ok=True) # This can raise OSError or PermissionError
        elif not parent_dir.is_dir():
            raise NotADirectoryError(f"Parent path is not a directory: {parent_dir}")

        # Explicit permission check for directory (more reliable than relying on write_text to fail)
        if not os.access(parent_dir, os.W_OK | os.X_OK):  # Check write and execute (needed for dir access)
            raise PermissionError(f"ディレクトリへの書き込み/アクセス権限がありません: {parent_dir}")

        # Create backup if requested and file exists
        if create_backup and path.exists() and path.is_file():
            backup_result = backup_file(path)  # Use dedicated backup function
            if backup_result.is_err:
                # Log backup failure but proceed with writing
                logger_io.warning(f"Backup creation failed for {path.name}, proceeding with write: {backup_result.error}")
            else:
                backup_path_obj = backup_result.unwrap()  # Store backup path

        # Write content
        try:
            path.write_text(content, encoding=encoding, errors='replace')
            logger_io.debug(f"Successfully wrote {len(content)} chars to: {path.name}")
            return Result.ok(path)  # Return the written path on success
        except Exception as write_err: # Broad catch for write_text errors (IOError, OSError, etc.)
            logger_io.error(f"Failed to write content to {path.name}: {write_err}", exc_info=False)
            # Attempt to restore backup if write failed and backup exists
            if backup_path_obj and backup_path_obj.exists():
                logger_io.info(f"Attempting to restore backup for {path.name}...")
                try:
                    # shutil.move is more robust for cross-filesystem moves if needed
                    # but rename should work for same filesystem.
                    backup_path_obj.rename(path)
                    logger_io.warning(f"Write failed, restored original file from backup: {path.name}")
                except Exception as restore_err:
                    logger_io.error(f"CRITICAL: Write failed AND backup restore failed for {path.name}: {restore_err}")
            # Re-raise the original write error wrapped in FileProcessingError
            raise FileProcessingError(f"File write error: {write_err}", details={"path": str(path)}) from write_err

    except (NotADirectoryError, PermissionError) as e: # Errors related to directory structure/permissions
        err = FileProcessingError(f"ディレクトリ/権限エラー: {e}", details={"path": str(file_path)})
        logger_io.error(str(err))
        return Result.fail(err)
    except OSError as e:  # Covers mkdir errors and other OS-level file system errors
        err = FileProcessingError(f"ファイル書き込み準備中のOSエラー {file_path}: {e}", details={"path": str(file_path)})
        logger_io.error(str(err), exc_info=True)
        return Result.fail(err)
    except Exception as e:  # Catch unexpected errors during setup or other operations
        err = FileProcessingError(f"ファイル書き込み中の予期せぬエラー {file_path}: {e}", details={"path": str(file_path)})
        logger_io.error(str(err), exc_info=True)
        return Result.fail(err)


def backup_file(filepath: pathlib.Path) -> Result[pathlib.Path, FileProcessingError]:
    """Creates a timestamped backup of a file using pathlib."""
    logger_io = logging.getLogger("NGGS-Lite.IO.Backup")
    if not filepath.is_file():
        err = FileProcessingError(f"バックアップ元が見つからないか、ファイルではありません: {filepath}", details={"path": str(filepath)})
        logger_io.warning(str(err)) # Log as warning as this might be a recoverable issue elsewhere
        return Result.fail(err)

    try:
        # Use a default NGGSConfig instance to get the format string,
        # as a config object might not be readily available here.
        # This assumes NGGSConfig can be instantiated without external dependencies at this point.
        try:
            config_for_backup = NGGSConfig()
            backup_suffix_format = config_for_backup.BACKUP_SUFFIX_FORMAT
        except Exception: # Fallback if NGGSConfig instantiation fails
            backup_suffix_format = ".bak.{timestamp}"
            logger_io.warning("Could not get backup format from NGGSConfig, using default.")

        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_%f")[:-3] # Milliseconds
        backup_suffix = backup_suffix_format.format(timestamp=timestamp)
        backup_path = filepath.with_name(f"{filepath.name}{backup_suffix}")

        # Check for potential naming collisions (highly unlikely with milliseconds)
        retries = 0
        while backup_path.exists() and retries < 5: # Limit retries to avoid infinite loop
            retries += 1
            logger_io.warning(f"Backup file already exists, adding extra suffix: {backup_path.name}")
            backup_path = filepath.with_name(f"{filepath.name}{backup_suffix}.extra{retries}")
        if backup_path.exists(): # Still exists after retries
            raise FileProcessingError(f"Failed to find unique backup name for {filepath.name} after {retries} retries.")


        # Use shutil.copy2 for atomicity if possible and to preserve metadata
        import shutil
        try:
            shutil.copy2(str(filepath), str(backup_path))  # Preserves metadata
            logger_io.info(f"Created backup (via copy): {backup_path.name}")
        except OSError as copy_err:
            # If copy fails, log but don't prevent further operations
            raise FileProcessingError(f"Backup file copy failed for {filepath.name}: {copy_err}") from copy_err

        return Result.ok(backup_path)
    except Exception as e:
        err = FileProcessingError(f"バックアップ作成エラー {filepath.name}: {e}", details={"path": str(filepath)})
        logger_io.error(str(err), exc_info=True)
        return Result.fail(err)

# =============================================================================
# Part 3 End: Utility Functions (Logging, Text, File I/O)
# =============================================================================
# =============================================================================
# Part 4: Utility Functions (Continued), LLM Client (Initialization)
# =============================================================================

# --- Template & Vocabulary Utilities (Refined for v1.8) ---

# Define default template content (as multiline strings for embedding)
# These should ideally be loaded from actual template files, but included here
# for single-file distribution.
DEFAULT_GENERATION_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体テキスト生成 (NGGS-Lite v1.8)

## 1. 目標設定
- **文体**: 新ゴシック文体（廃墟、影、内省、象徴性を重視）
- **視点**: {perspective_mode}
- **目標文字数**: 約{target_length}字
- **位相焦点**: {phase_focus}
- **口語レベル**: {colloquial_level}
- **感情の弧 (指定があれば)**: {emotion_arc}

## 2. 参考語彙リスト (適宜活用)
{vocabulary_list_str}

## 3. 入力テキスト (コンテキストとして使用)
```text
{input_text}
```

## 4. 物語構成・流れ (指定があれば)
{narrative_flow_section}

## 5. 生成指示
上記の設定と入力テキストを踏まえ、指定された視点とスタイルで、目標文字数程度の新しいゴシック文体テキストを生成してください。以下の点を特に意識してください。

- **ゴシック的雰囲気**: 不安、神秘、退廃、美と恐怖の混在などを醸成する。
- **主観的描写**: 指定された視点からの内的体験（思考、感情、感覚）を深く描写する。
- **間接的表現**: 感情や状況を暗示的に表現し、読者の解釈に委ねる余地を残す。
- **位相移行**: セリフ、実況、モノローグ、ナレーション等を自然かつ効果的に移行させる。
- **層バランス**: 物質・感覚・心理・象徴の四層描写をバランス良く含める。
- **語彙選択**: ゴシック的な語彙を文脈に合わせて効果的に使用する。

出力は生成されたテキスト本文のみとしてください。説明や前置きは不要です。
"""

DEFAULT_EVALUATION_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体 テキスト評価 (NGGS-Lite v1.8)

## 評価対象テキスト
```text
{generated_text}
```

## 評価基準 (各項目1.0～5.0点で評価)
- **gothic_atmosphere (ゴシック的雰囲気)**: 廃墟、影、神秘、不安、退廃、美と恐怖の混在などの雰囲気が醸成されているか？ (1:希薄 - 5:濃厚)
- **stylistic_gravity (文体の荘重さ/質)**: ゴシック特有の重厚さ、格調高さ、あるいは独特の美意識が文体に現れているか？ (1:軽薄 - 5:荘重/洗練)
- **indirect_emotion (感情表現の間接性)**: 感情が直接的な説明でなく、描写や行動、サブテキストを通じて効果的に暗示されているか？ (1:直接的すぎる - 5:極めて間接的/暗示的)
- **vocabulary_richness (語彙の質と豊かさ)**: ゴシック的な語彙が適切かつ効果的に使用され、文体に深みを与えているか？ (1:貧弱/不適切 - 5:豊か/効果的)
- **overall_quality (全体的な質)**: 上記要素を総合し、新ゴシック文体テキストとしての完成度、独自性、魅力を評価。 (1:低い - 5:極めて高い)
- **eti (存在論的震え指数)**: 境界性、両義性、超越侵犯、不確定性、内的変容など、読者に存在論的な不安や感覚の揺らぎを与える要素の度合い。 (1:皆無 - 5:強烈)
- **subjective_depth (主観的語りの深さ)**: 登場人物の内的世界（思考、感情、感覚）への没入感、主観的体験の描写の深さ。 (1:浅い/客観的 - 5:深い/没入的)
- **phase_transition (位相移行の自然さ)**: セリフ、実況、モノローグ、ナレーション等の語りの位相（モード）間の移行が自然で効果的か？ (1:不自然/唐突 - 5:極めて自然/効果的)
- **colloquial_gothic_blend (口語/ゴシック融合度)**: 日常的な口語表現とゴシック的な文語表現が、意図したレベル（例：中程度）で自然に融合しているか？ (1:不自然/乖離 - 5:極めて自然/融合)
- **layer_balance (層バランス)**: 物質層、感覚層、心理層、象徴層の描写が、作品の意図に対して適切なバランスで含まれているか？ (1:偏り大 - 5:理想的バランス)
- **emotion_arc_quality (感情変容の質)**: テキスト全体を通じて描かれる感情の変化や流れ（指定があればその弧）が、説得力を持って自然に描かれているか？ (1:平板/不自然 - 5:説得力があり自然)

## 出力形式
必ず以下のJSON形式で、各項目のスコア(float)と評価理由(string)を記述してください。理由には具体的なテキスト箇所を引用すると尚良いです。

```json
{{
  "scores": {{
    "gothic_atmosphere": <float 1.0-5.0>,
    "stylistic_gravity": <float 1.0-5.0>,
    "indirect_emotion": <float 1.0-5.0>,
    "vocabulary_richness": <float 1.0-5.0>,
    "overall_quality": <float 1.0-5.0>,
    "eti": <float 1.0-5.0>,
    "subjective_depth": <float 1.0-5.0>,
    "phase_transition": <float 1.0-5.0>,
    "colloquial_gothic_blend": <float 1.0-5.0>,
    "layer_balance": <float 1.0-5.0>,
    "emotion_arc_quality": <float 1.0-5.0>
  }},
  "reasoning": {{
    "gothic_atmosphere": "<具体的な評価理由>",
    "stylistic_gravity": "<具体的な評価理由>",
    "indirect_emotion": "<具体的な評価理由>",
    "vocabulary_richness": "<具体的な評価理由>",
    "overall_quality": "<全体的な評価と改善点>",
    "eti": "<ETI評価の根拠>",
    "subjective_depth": "<主観描写の評価根拠>",
    "phase_transition": "<位相移行の評価根拠>",
    "colloquial_gothic_blend": "<口語/文語バランスの評価根拠>",
    "layer_balance": "<層バランスの評価根拠>",
    "emotion_arc_quality": "<感情変容の評価根拠>"
  }}
}}
```
"""

DEFAULT_IMPROVEMENT_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体テキスト改善 (NGGS-Lite v1.8)

## 1. 改善対象テキスト (抜粋)
```text
{original_text}
```

## 2. 前回の評価結果 (JSON形式)
```json
{evaluation_results_json}
```

### 要約:
- 特に評価が低い項目: {low_score_items_str}
- 特に評価が高い項目: {high_score_items_str}
- 層バランス分析: {layer_distribution_analysis}
- 位相分布分析: {phase_distribution_analysis}

## 3. 目標設定
- **文体**: 新ゴシック文体
- **視点**: {perspective_mode}
- **位相焦点**: {phase_focus}
- **口語レベル**: {colloquial_level}
- **感情の弧 (指定があれば)**: {emotion_arc}

## 4. 参考語彙リスト (特に低評価項目に関連するものを活用)
{vocabulary_list_str}

## 5. 改善指示 (具体的な指示があれば以下に追加される)
{improvement_section}

## 6. 全体的な改善指示
上記の評価結果（特に低評価項目）と改善指示を踏まえ、改善対象テキスト全体を全面的に書き直し、より質の高い新ゴシック文体テキストへと改善してください。特に以下の点を意識してください。

- **弱点の克服**: 評価で指摘された弱点を重点的に修正・改善する。
- **強みの維持・強化**: 評価で高かった点は維持しつつ、さらに洗練させる。
- **全体的な質向上**: ゴシック的雰囲気、文体の質、感情表現、語彙、主観性、位相移行、層バランス、口語性などを総合的に向上させる。
- **設定の遵守**: 視点、位相焦点、口語レベル、感情の弧などの設定を正確に反映させる。

出力は改善されたテキスト本文のみとしてください。説明やコメントは不要です。
"""

# Global dictionary for default templates
DEFAULT_TEMPLATES: Final[Dict[str, str]] = {
    "generation": DEFAULT_GENERATION_TEMPLATE,
    "evaluation": DEFAULT_EVALUATION_TEMPLATE,
    "improvement": DEFAULT_IMPROVEMENT_TEMPLATE,
}

# Global dictionary for default vocabulary (as fallback)
DEFAULT_LAYERED_VOCABULARY: Final[Dict[str, List[str]]] = {
    "physical": ["城壁", "石畳", "尖塔", "地下牢", "書物", "肖像画", "古時計", "燭台"],
    "sensory": ["冷気", "黴臭", "月光", "影", "囁き", "鐘の音", "真紅", "漆黒", "霧雨"],
    "psychological": ["憂愁", "狂気", "不安", "孤独", "記憶", "忘却", "既視感", "罪悪感", "苦悶"],
    "symbolic": ["鏡", "時計", "螺旋階段", "迷宮", "仮面", "血", "薔薇", "鴉", "運命", "深淵"]
}


def load_template(
    file_path: Union[str, pathlib.Path],
    template_name: str,
    default_templates: Dict[str, str]  # Pass defaults explicitly
) -> Result[str, TemplateError]:
    """Loads a template file, falling back to defaults if necessary."""
    logger_tmpl = logging.getLogger("NGGS-Lite.TemplateLoader")
    default_template = default_templates.get(template_name)
    path_obj = pathlib.Path(file_path)  # Convert to Path object early

    # Try loading from file
    load_result = safe_read_file(path_obj)
    load_error: Optional[Exception] = None  # Store potential error

    if load_result.is_ok:
        content = load_result.unwrap()
        if content and content.strip():
            # Validate loaded template (basic check)
            validate_res = validate_template(content)  # Optional: Pass expected keys if known
            if validate_res.is_ok:
                logger_tmpl.info(f"Successfully loaded template '{template_name}' from file: {path_obj.name}")
                return Result.ok(content)
            else:
                load_error = validate_res.error
                logger_tmpl.error(f"Template file '{path_obj.name}' validation failed: {load_error}")
        else:
            load_error = TemplateError(f"Template file '{path_obj.name}' is empty.")
            logger_tmpl.warning(str(load_error))
    elif isinstance(load_result.error, FileNotFoundError):
        # Only log info if file not found, not other errors like permission denied
        logger_tmpl.info(f"Template file '{path_obj.name}' not found. Checking for default.")
        load_error = load_result.error  # Keep track of the error
    else:
        # Log other file processing errors (Permission, IO, etc.)
        load_error = load_result.error
        logger_tmpl.error(f"Failed to read template file '{path_obj.name}': {load_error}")

    # Fallback to default template
    if default_template:
        if load_error:  # Log if fallback is due to previous error
            logger_tmpl.warning(f"Using default template for '{template_name}' due to error: {load_error}")
        # Validate default template
        validate_res_default = validate_template(default_template)
        if validate_res_default.is_ok:
            logger_tmpl.info(f"Used default template for '{template_name}'.")
            return Result.ok(default_template)
        else:
            # Default template itself is invalid - critical
            err = TemplateError(f"Default template '{template_name}' is invalid: {validate_res_default.error}")
            logger_tmpl.critical(str(err))
            return Result.fail(err)
    else:
        # No file and no default available
        final_error_msg = f"Template '{template_name}' not found in '{path_obj.name}' and no default available."
        if load_error:
            final_error_msg += f" (Original error: {load_error})"
        final_error_obj = TemplateError(final_error_msg)
        logger_tmpl.error(str(final_error_obj))
        return Result.fail(final_error_obj)


def validate_template(template: Optional[str], expected_keys: Optional[List[str]] = None) -> Result[bool, TemplateError]:
    """Performs basic validation on a template string."""
    if not template or not isinstance(template, str) or not template.strip():
        return Result.fail(TemplateError("Template must be a non-empty string"))

    # Check for basic brace balance
    try:
        balance = 0
        for char in template:
            if char == '{':
                balance += 1
            elif char == '}':
                balance -= 1
            if balance < 0:
                raise ValueError("Unmatched '}'")
        if balance != 0:
            raise ValueError("Unmatched '{'")
    except ValueError as e:
        return Result.fail(TemplateError(f"Template format error (brace balance): {e}"))

    # Check for expected keys if provided
    if expected_keys:
        try:
            # Find placeholders using regex, allowing simple identifiers
            placeholders = set(re.findall(r"\{([a-zA-Z0-9_]+)\}", template))
            missing_keys = [key for key in expected_keys if key not in placeholders]
            if missing_keys:
                return Result.fail(TemplateError(f"Template missing required placeholder(s): {', '.join(missing_keys)}"))
        except re.error as e:
            # Log regex error but potentially continue if keys aren't critical
            logging.getLogger("NGGS-Lite.Utils").warning(f"Regex error during template key check: {e}")

    # Try a dummy format operation to catch syntax errors
    try:
        # Extract keys again for dummy data
        placeholders_for_dummy = set(re.findall(r"\{([a-zA-Z0-9_]+)\}", template))
        dummy_data = {key: "dummy" for key in placeholders_for_dummy}
        # Use SafeDict to handle keys present in template but not needed for dummy check
        template.format_map(SafeDict(dummy_data))
        return Result.ok(True)
    except (ValueError, KeyError) as e:  # Format errors
        return Result.fail(TemplateError(f"Template format syntax error (dummy format): {e}"))
    except Exception as e:
        return Result.fail(TemplateError(f"Unexpected template validation error: {e}"))


def load_vocabulary(
    config: NGGSConfig  # Pass config for paths and priority
) -> Result[Union[List[str], JsonDict], VocabularyError]:
    """
    Loads vocabulary based on priority defined in NGGSConfig (v1.8).
    Tries GLCAI path first, then default NGGS path, then internal default.
    Ensures the returned structure (list or dict) is consistent if needed.

    Args:
        config: The NGGSConfig object containing vocabulary paths and settings.

    Returns:
        Result containing the loaded vocabulary data (list or dict) or VocabularyError.
    """
    logger_vocab_loader = logging.getLogger("NGGS-Lite.VocabLoader")
    loaded_data: Optional[Union[List[str], JsonDict]] = None
    last_error: Optional[Exception] = None

    priority = config.VOCAB_LOAD_PRIORITY

    for source_type in priority:
        file_path_str: Optional[str] = None
        file_path: Optional[pathlib.Path] = None
        source_desc: str = ""

        # Get path based on source type
        if source_type == 'glcai':
            file_path_str = config.GLCAI_VOCAB_PATH
            source_desc = "GLCAI Vocabulary File"
        elif source_type == 'default':
            file_path_str = config.DEFAULT_VOCAB_PATH
            source_desc = "Default NGGS Vocabulary File"
        else:
            logger_vocab_loader.warning(f"Unknown vocabulary source type in priority list: {source_type}")
            continue

        # Resolve path
        if file_path_str:
            try:
                file_path = pathlib.Path(file_path_str).resolve()
            except Exception as path_err:
                logger_vocab_loader.warning(f"Invalid path string for {source_desc}: '{file_path_str}' - {path_err}")
                file_path = None  # Ensure file_path is None if resolution fails

        if file_path:
            logger_vocab_loader.info(f"Attempting to load vocabulary from {source_desc}: {file_path}")
            read_result = safe_read_file(file_path)
            if read_result.is_ok:
                content = read_result.unwrap()
                if content and content.strip():
                    try:
                        data = json.loads(content)
                        # Basic validation: Must be list or dict
                        if isinstance(data, (list, dict)):
                            logger_vocab_loader.info(f"Successfully loaded and parsed vocabulary from {source_desc}: {file_path}")
                            return Result.ok(data)
                        else:
                            last_error = VocabularyError(f"Vocabulary data in {file_path.name} is not a list or dictionary.")
                            logger_vocab_loader.warning(str(last_error))
                    except json.JSONDecodeError as e:
                        last_error = VocabularyError(f"Error parsing vocabulary JSON ({file_path.name}): {e}")
                        logger_vocab_loader.warning(str(last_error))
                    except Exception as e:
                        last_error = VocabularyError(f"Unexpected error loading vocabulary {file_path.name}: {e}")
                        logger_vocab_loader.error(str(last_error), exc_info=True)
                else:  # Empty file
                    last_error = VocabularyError(f"Vocabulary file is empty: {file_path.name}")
                    logger_vocab_loader.warning(str(last_error))
            else:  # File not found, permission error, etc.
                last_error = read_result.error
                # Only log warning if error wasn't FileNotFoundError (already logged by safe_read_file)
                if not isinstance(last_error, FileNotFoundError):
                    logger_vocab_loader.warning(f"Could not read {source_desc} ({file_path.name}): {last_error}")
        else:
            logger_vocab_loader.info(f"Path for source type '{source_type}' is not configured or invalid.")
            if last_error is None:
                last_error = VocabularyError(f"Path for source type '{source_type}' not configured.")

    # If loop completes without loading successfully
    logger_vocab_loader.warning("Failed to load vocabulary from configured file paths.")
    # Fallback to internal default
    internal_default = DEFAULT_LAYERED_VOCABULARY  # Use the global default
    if internal_default and isinstance(internal_default, dict):  # Check if it's a valid structure
        logger_vocab_loader.warning(f"Using internal default layered vocabulary due to error: {last_error or 'No valid files found'}")
        return Result.ok(internal_default)
    else:
        final_error_obj = last_error if isinstance(last_error, Exception) else VocabularyError(str(last_error))
        if not final_error_obj:
            final_error_obj = VocabularyError("No vocabulary files found and no valid internal default available.")
        logger_vocab_loader.error(str(final_error_obj))
        return Result.fail(final_error_obj)


# --- Instruction Extraction Utility ---
def extract_improvement_instructions(instruction_text: str) -> Optional[str]:
    """
    Extracts structured improvement instructions from LLM response.
    (Logic primarily from v1.7, minor refinement v1.8)
    """
    logger_extract = logging.getLogger("NGGS-Lite.InstructionExtractor")
    if not instruction_text or not isinstance(instruction_text, str):
        return None

    # Patterns ordered by specificity (Headers > Keywords > Lists)
    patterns = [
        # 1. Section headers (more specific)
        r'(?:^|\n)##\s*(?:改善(?:プロンプト)?指示|具体的な改善指示)\s*?\n([\s\S]+?)(?=\n##\s*\w+|$)',
        # 2. Keyword phrases indicating instructions
        r'(?:以下|次)の(?:点|観点|項目)を(?:改善|修正|強化|調整).*?\n([\s\S]+?)(?=\n\n|\n##|$)',
        # 3. Numbered lists (more robust)
        r'(?:^|\n)\s*(\d+[.\uFF0E\uff61]\s+[\s\S]+?)(?=\n\s*\d+[.\uFF0E\uff61]|\n\n|$)',
        # 4. Bullet lists (more robust)
        r'(?:^|\n)\s*([\*\u30FB・\-]\s+[\s\S]+?)(?=\n\s*[\*\u30FB・\-]\s+|\n\n|$)'
    ]
    extracted_blocks: List[str] = []
    text_to_search = instruction_text

    for i, pattern in enumerate(patterns):
        try:
            # Find all non-overlapping matches for this pattern
            matches = list(re.finditer(pattern, text_to_search, re.MULTILINE | re.IGNORECASE))
            if matches:
                logger_extract.debug(f"Instruction extraction pattern {i + 1} matched {len(matches)} time(s).")
                for match in matches:
                    # Group 1 usually contains the actual instruction content
                    instruction_content = match.group(1).strip()
                    # Basic check to avoid empty matches or just markers
                    if instruction_content and len(instruction_content.replace("*", "").replace("-", "").strip()) > 5:
                        extracted_blocks.append(instruction_content)
        except re.error as e:
            logger_extract.warning(f"Regex error during instruction extraction (pattern {i + 1}): {e}")

    if extracted_blocks:
        # Post-processing: Deduplicate, clean, and potentially select the best block(s)
        unique_instructions = []
        seen = set()
        for instr in extracted_blocks:
            # Normalize whitespace for deduplication check
            norm_instr = re.sub(r'\s+', ' ', instr).strip().lower()
            if norm_instr not in seen and len(norm_instr) > 10:  # Min length check
                unique_instructions.append(instr.strip())
                seen.add(norm_instr)

        if len(unique_instructions) > 1:
            logger_extract.info(f"Extracted {len(unique_instructions)} unique instruction blocks. Joining them.")
            final_instructions = "\n\n---\n\n".join(unique_instructions)
        elif len(unique_instructions) == 1:
            final_instructions = unique_instructions[0]
            logger_extract.info("Extracted 1 unique instruction block.")
        else:  # If all blocks were short/duplicates
            final_instructions = None
            logger_extract.warning("Extracted blocks were duplicates or too short.")
        return final_instructions if final_instructions else None
    else:
        # Fallback if no patterns matched: Check if the whole text looks like instructions
        stripped_text = instruction_text.strip()
        # Check for list-like structures or significant length
        if len(stripped_text) > 50 and (re.match(r'^\s*(\d+\.|[\*・\-])', stripped_text) or len(stripped_text) > 150):
            logger_extract.warning("No specific instruction pattern matched. Using entire response as fallback instructions.")
            return stripped_text
        else:
            logger_extract.warning("Could not extract meaningful improvement instructions.")
            return None


# --- Utility function for exponential backoff ---
def exponential_backoff(attempt: int, base_delay: float, max_delay: float) -> float:
    """
    Calculates exponential backoff delay with full jitter.

    Args:
        attempt: The current retry attempt number (1-based).
        base_delay: The base delay in seconds.
        max_delay: The maximum delay in seconds.

    Returns:
        The calculated delay in seconds.
    """
    if attempt < 1:
        attempt = 1  # Ensure attempt is at least 1

    # Calculate capped exponential delay (2^(attempt-1) since attempt is 1-based)
    # Cap the exponent to avoid overflow with large attempt numbers, e.g., max exponent of 7-8 for reasonable delays
    capped_exponent = min(attempt - 1, 7) # Max delay will be base_delay * 128
    max_cap_delay = min(max_delay, base_delay * (2 ** capped_exponent))


    # Apply full jitter: random delay between 0 and max_cap_delay
    # Add a small minimum delay to avoid 0 delay.
    min_jitter_delay = 0.1  # seconds
    delay = random.uniform(min_jitter_delay, max(min_jitter_delay, max_cap_delay))

    return round(delay, 3)  # Return rounded delay


# =============================================================================
# Part 4: LLM Client (Initialization)
# =============================================================================
class LLMClient:
    """
    LLM APIとの通信を処理するクラス (v1.8: Gemini対応、強化されたリトライ/エラー処理)。
    設定に基づき、主に Gemini を呼び出す。
    """
    def __init__(self, config: NGGSConfig, use_mock: bool = False):
        """
        LLMクライアントを初期化します。

        Args:
            config: NGGSConfig オブジェクト。APIキー、モデル名などを含む。
            use_mock: Trueの場合、APIキーやライブラリに関わらずモックモードを強制。
        """
        self.config = config
        self.use_mock = use_mock
        self.api_key_source: str = "N/A"
        self.call_count: int = 0
        self.last_call_time: float = 0.0  # For RPM limiting
        self._mock_generate_internal: Optional[Callable[[str, float], LLMResponse]] = None
        self.logger = logging.getLogger("NGGS-Lite.LLMClient")

        self.client: Optional[Any] = None  # Can be Gemini client or other types
        # Determine engine, forcing mock if requested
        self.current_engine: str = "mock" if use_mock else config.LLM_ENGINE

        if self.use_mock:
            self.logger.info("LLMClientは強制的にモックモードを使用します。")
            self._setup_mock()
            self.api_key_source = "Mock Forced"
            # No further initialization needed for mock
            return

        # Initialize based on selected engine
        if self.current_engine == "gemini":
            if not GEMINI_AVAILABLE:
                # Log critical error and raise ConfigurationError immediately
                self.logger.critical("Geminiエンジンが選択されましたが、google-generativeaiライブラリが利用できません。")
                raise ConfigurationError("google-generativeaiライブラリがインストールされていません。")
            self._initialize_gemini()
        # Add other engine initializations here if needed (e.g., OpenAI)
        elif self.current_engine == "openai":
            if not OPENAI_AVAILABLE:
                self.logger.critical("OpenAIエンジンが選択されましたが、openaiライブラリが利用できません。")
                raise ConfigurationError("openaiライブラリがインストールされていません。")
            self._initialize_openai() # Placeholder
        else:
            # If engine is not supported, raise ConfigurationError
            raise ConfigurationError(
                f"サポートされていないLLMエンジンが設定されています: {self.current_engine}. "
                f"サポートされているのは 'gemini', 'openai'(プレースホルダ) です。"
            )

        # Check if initialization failed (client still None after attempt)
        if self.client is None and not self.use_mock:
            self.logger.warning(
                f"LLMエンジン '{self.current_engine}' の初期化に失敗しました "
                f"(APIキーソース/エラー: {self.api_key_source}). モックモードにフォールバックします。"
            )
            self.use_mock = True # Force mock mode
            self._setup_mock()
            self.api_key_source = f"初期化エラー ({self.current_engine}) -> モック"
            self.current_engine = "mock"  # Update engine state

        # Log final initialization status
        self.logger.info(
            f"LLMClient初期化完了。 Engine: {self.current_engine}, "
            f"Model: {self._get_model_name()}, APIキーソース: {self.api_key_source}"
        )

    def _get_model_name(self) -> str:
        """Returns the configured model name for the current engine."""
        if self.current_engine == "gemini":
            return self.config.GEMINI_MODEL_NAME
        elif self.current_engine == "openai":
            # Placeholder: Add OpenAI model name from config if implemented
            # return self.config.OPENAI_MODEL_NAME
            return "mock_openai_model_placeholder"
        elif self.current_engine == "mock":
            return f"mock_model_v{self.config.VERSION}"  # Versioned mock model
        else:
            # Should not be reached if init validation works
            self.logger.error(f"不明なエンジンタイプ '{self.current_engine}' のためモデル名を取得できません。")
            return "unknown_model"

    def _initialize_gemini(self) -> None:
        """Initializes the Gemini client using genai library."""
        self.logger.info(f"Geminiクライアント初期化中 (Model: {self.config.GEMINI_MODEL_NAME})...")
        api_key = self.config.GEMINI_API_KEY  # Get key from config
        if not api_key:
            self.logger.error(f"Gemini APIキー (環境変数 '{self.config.GEMINI_API_KEY_ENV}' または設定ファイル) が見つかりません。")
            self.api_key_source = "Gemini Key Missing"
            self.client = None # Ensure client is None
            return # Do not proceed without key

        try:
            # Configure the API key globally for the genai library
            if genai: # Check if genai module was successfully imported
                genai.configure(api_key=api_key)
                # Create the GenerativeModel instance
                # TODO: Consider adding safety_settings from config if needed
                self.client = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME)
                self.api_key_source = f"env:{self.config.GEMINI_API_KEY_ENV} (or config)"
                self.logger.info("Geminiクライアントの初期化に成功しました。")
            else:
                # This case should be caught by GEMINI_AVAILABLE check earlier
                self.logger.critical("Geminiライブラリ(genai)が利用できません。初期化スキップ。")
                self.api_key_source = "Gemini Lib Not Loaded"
                self.client = None

        except google_exceptions.PermissionDenied:
            self.logger.error("Gemini APIキーが無効か、必要な権限がありません。")
            self.api_key_source = "Gemini Auth Error"
            self.client = None  # Ensure client is None on error
        except google_exceptions.InvalidArgument as e:
            self.logger.error(f"Gemini初期化中の無効な引数 (モデル名を確認してください): {e}")
            self.api_key_source = "Gemini Invalid Argument"
            self.client = None
        except Exception as e:
            # Catch any other unexpected errors during initialization
            self.logger.error(f"Geminiクライアント初期化中に予期せぬエラー: {type(e).__name__} - {e}", exc_info=True)
            self.api_key_source = "Gemini Init Error"
            self.client = None

    def _initialize_openai(self) -> None:
        """Initializes the OpenAI client (placeholder)."""
        self.logger.warning("OpenAI初期化はv1.8ではプレースホルダであり、実装されていません。")
        # If OpenAI support is added, implementation goes here.
        # Example:
        # api_key = self.config.OPENAI_API_KEY
        # if not api_key:
        #     self.logger.error("OpenAI APIキーが見つかりません。")
        #     self.api_key_source = "OpenAI Key Missing"
        #     return
        # try:
        #     self.client = openai.OpenAI(api_key=api_key) # Or AsyncOpenAI
        #     self.api_key_source = "OpenAI Key Provided"
        #     self.logger.info("OpenAIクライアントの初期化に成功しました (プレースホルダ)。")
        # except Exception as e:
        #     self.logger.error(f"OpenAIクライアント初期化エラー: {e}")
        #     self.api_key_source = "OpenAI Init Error"
        self.client = None # Ensure client is None for placeholder
        raise NotImplementedError("OpenAIクライアント初期化はv1.8では実装されていません。")

# LLMClient._setup_mock, _adaptive_delay, _handle_api_error, generate methods will follow in Part 5.
# =============================================================================
# Part 4 End: Utilities (Continued), LLM Client Initialization
# =============================================================================
# =============================================================================
# Part 5: LLM Client (Generation Logic)
# (Continues LLMClient class from Part 4)
# =============================================================================

class LLMClient:  # Re-open class to add methods
    # --- Methods from Part 4 (__init__, _get_model_name, _initialize_*) ---
    def __init__(self, config: NGGSConfig, use_mock: bool = False):
        """
        LLMクライアントを初期化します。
        (Docstring from Part 4)
        """
        self.config = config
        self.use_mock = use_mock
        self.api_key_source: str = "N/A"
        self.call_count: int = 0
        self.last_call_time: float = 0.0  # For RPM limiting
        self._mock_generate_internal: Optional[Callable[[str, float], LLMResponse]] = None
        self.logger = logging.getLogger("NGGS-Lite.LLMClient")

        self.client: Optional[Any] = None  # Can be Gemini client or other types
        self.current_engine: str = "mock" if use_mock else config.LLM_ENGINE

        if self.use_mock:
            self.logger.info("LLMClientは強制的にモックモードを使用します。")
            self._setup_mock()
            self.api_key_source = "Mock Forced"
            return

        if self.current_engine == "gemini":
            if not GEMINI_AVAILABLE:
                self.logger.critical("Geminiエンジンが選択されましたが、google-generativeaiライブラリが利用できません。")
                raise ConfigurationError("google-generativeaiライブラリがインストールされていません。")
            self._initialize_gemini()
        elif self.current_engine == "openai":
            if not OPENAI_AVAILABLE:
                self.logger.critical("OpenAIエンジンが選択されましたが、openaiライブラリが利用できません。")
                raise ConfigurationError("openaiライブラリがインストールされていません。")
            self._initialize_openai() # Placeholder
        else:
            raise ConfigurationError(
                f"サポートされていないLLMエンジンが設定されています: {self.current_engine}. "
                f"サポートされているのは 'gemini', 'openai'(プレースホルダ) です。"
            )

        if self.client is None and not self.use_mock:
            self.logger.warning(
                f"LLMエンジン '{self.current_engine}' の初期化に失敗しました "
                f"(APIキーソース/エラー: {self.api_key_source}). モックモードにフォールバックします。"
            )
            self.use_mock = True
            self._setup_mock()
            self.api_key_source = f"初期化エラー ({self.current_engine}) -> モック"
            self.current_engine = "mock"

        self.logger.info(
            f"LLMClient初期化完了。 Engine: {self.current_engine}, "
            f"Model: {self._get_model_name()}, APIキーソース: {self.api_key_source}"
        )

    def _get_model_name(self) -> str:
        """現在のエンジン用に設定されたモデル名を返します。"""
        if self.current_engine == "gemini":
            return self.config.GEMINI_MODEL_NAME
        elif self.current_engine == "openai":
            return "mock_openai_model_placeholder" # 実装時にConfigから取得
        elif self.current_engine == "mock":
            return f"mock_model_v{self.config.VERSION}"
        else:
            self.logger.error(f"不明なエンジンタイプ '{self.current_engine}' のためモデル名を取得できません。")
            return "unknown_model"

    def _initialize_gemini(self) -> None:
        """genaiライブラリを使用してGeminiクライアントを初期化します。"""
        self.logger.info(f"Geminiクライアント初期化中 (Model: {self.config.GEMINI_MODEL_NAME})...")
        api_key = self.config.GEMINI_API_KEY
        if not api_key:
            self.logger.error(f"Gemini APIキー (環境変数 '{self.config.GEMINI_API_KEY_ENV}') が見つかりません。")
            self.api_key_source = "Gemini Key Missing"
            self.client = None
            return

        try:
            if genai:
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME)
                self.api_key_source = f"env:{self.config.GEMINI_API_KEY_ENV} (or config)"
                self.logger.info("Geminiクライアントの初期化に成功しました。")
            else:
                self.logger.critical("Geminiライブラリ(genai)が利用できません。初期化スキップ。")
                self.api_key_source = "Gemini Lib Not Loaded"
                self.client = None
        except google_exceptions.PermissionDenied:
            self.logger.error("Gemini APIキーが無効か、必要な権限がありません。")
            self.api_key_source = "Gemini Auth Error"; self.client = None
        except google_exceptions.InvalidArgument as e:
            self.logger.error(f"Gemini初期化中の無効な引数 (モデル名を確認してください): {e}")
            self.api_key_source = "Gemini Invalid Argument"; self.client = None
        except Exception as e:
            self.logger.error(f"Geminiクライアント初期化中に予期せぬエラー: {type(e).__name__} - {e}", exc_info=True)
            self.api_key_source = "Gemini Init Error"; self.client = None

    def _initialize_openai(self) -> None:
        """OpenAIクライアントを初期化します (プレースホルダ)。"""
        self.logger.warning("OpenAI初期化はv1.8ではプレースホルダであり、実装されていません。")
        self.client = None
        raise NotImplementedError("OpenAIクライアント初期化はv1.8では実装されていません。")

    # --- Methods for Part 5 ---
    def _setup_mock(self) -> None:
        """テスト用のモック生成関数をセットアップします。"""
        def mock_generate_internal(prompt: str, temperature: float) -> LLMResponse:
            self.logger.debug(f"モック生成実行中 (温度: {temperature:.2f})...")
            time.sleep(random.uniform(0.02, 0.08))  # Simulate slight delay
            response_text = f"Mocked Response v{self.config.VERSION} (temp={temperature:.2f})\n"
            prompt_lower = prompt.lower()

            # Simulate different response types based on prompt keywords
            if "評価" in prompt_lower or "evaluation" in prompt_lower:
                # Simulate evaluation response (simple JSON structure)
                mock_eval = {
                    "scores": {
                        "overall_quality": round(random.uniform(2.0, 4.8), 1),
                        "gothic_atmosphere": round(random.uniform(1.5, 4.5), 1),
                        "stylistic_gravity": round(random.uniform(2.5, 4.9), 1),
                        "eti": round(random.uniform(1.0, 5.0), 1),
                        "subjective_depth": round(random.uniform(2.0, 4.7), 1),
                        "phase_transition": round(random.uniform(2.2, 4.3), 1),
                    },
                    "reasoning": {
                        "overall_quality": "これはモック評価の理由です。v1.8。",
                        "gothic_atmosphere": "ゴシック雰囲気は適度に表現されています（モック）。"
                    }
                }
                try:
                    response_text += json.dumps(mock_eval, ensure_ascii=False, indent=2)
                except Exception:
                    response_text += '{"error": "モックJSON生成失敗"}'
            elif "改善指示" in prompt_lower or "improvement" in prompt_lower:
                # Simulate improvement instruction response
                response_text += "### 改善プロンプト指示 (Mock v1.8)\n- 内的表現の探求をさらに深めてください。\n- 象徴層と感覚層の描写を連携させてください。\n- 位相移行をより滑らかにしてください。"
            else:
                # Simulate standard text generation
                response_text += f"これはモック生成されたテキストです。プロンプトの開始部分: '{truncate_text(prompt, 50)}'"

            metadata = {"mock": True, "engine": f"mock_v{self.config.VERSION}", "temperature": temperature}
            return LLMResponse(text=response_text, metadata=metadata)

        self._mock_generate_internal = mock_generate_internal
        self.logger.info(f"モック生成関数 (v{self.config.VERSION}) セットアップ完了。")

    def _adaptive_delay(self) -> None:
        """RPM制限に基づいてAPIリクエスト間の適応的遅延を管理します。"""
        limit = 0
        # Get RPM limit based on the currently active engine
        if self.current_engine == "gemini":
            limit = self.config.GEMINI_RPM_LIMIT
        elif self.current_engine == "openai":
            # limit = self.config.OPENAI_RPM_LIMIT # Example if implemented
            pass

        if limit <= 0:
            # No rate limiting configured for this engine
            return

        min_interval = 60.0 / limit
        now = time.monotonic()

        if self.last_call_time > 0:
            elapsed = now - self.last_call_time
            if elapsed < min_interval:
                wait_time = min_interval - elapsed
                self.logger.debug(f"RPM {limit} limit ({self.current_engine}): Waiting {wait_time:.2f}s...")
                time.sleep(wait_time)
                # Update time after waiting
                now = time.monotonic()

        # Update last call time *before* the actual call attempt
        self.last_call_time = now

    def _handle_api_error(self, exc: Exception, attempt: int) -> Tuple[bool, float]:
        """
        APIエラーを分類し、リトライのステータス/遅延を決定します。
        例外からRetry-Afterヘッダーが利用可能な場合はそれを優先します。

        Args:
            exc: API呼び出し中にキャッチされた例外。
            attempt: 現在のリトライ試行回数 (1ベース)。

        Returns:
            Tuple[bool, float]: (リトライすべきか, 遅延秒数)
        """
        should_retry = False
        # Use utility function for exponential backoff with jitter
        delay = exponential_backoff(
            attempt=attempt,
            base_delay=self.config.API_BASE_RETRY_DELAY,
            max_delay=self.config.API_MAX_RETRY_DELAY
        )
        error_type_name = type(exc).__name__
        error_message_lower = str(exc).lower()  # For keyword checks

        # --- Check for Retry-After information (Placeholder for specific library features) ---
        suggested_delay_seconds: Optional[float] = None
        # if hasattr(exc, 'retry_after') and isinstance(exc.retry_after, (int, float)):
        #    suggested_delay_seconds = float(exc.retry_after)

        # --- Classify Gemini Errors ---
        if GEMINI_AVAILABLE and isinstance(exc, google_exceptions.GoogleAPIError):
            if isinstance(exc, google_exceptions.ResourceExhausted):
                calculated_rate_delay = self.config.API_RATE_LIMIT_DELAY * (1.5 ** (attempt - 1))
                final_delay = suggested_delay_seconds if suggested_delay_seconds is not None else calculated_rate_delay
                delay = min(self.config.API_MAX_RETRY_DELAY * 1.5, max(delay, final_delay))
                log_msg_verb = "server suggested" if suggested_delay_seconds is not None else "calculated rate"
                self.logger.warning(f"API Resource Exhausted (Attempt {attempt}). Using {log_msg_verb} delay. Retrying after {delay:.1f}s.")
                should_retry = True
            elif isinstance(exc, (google_exceptions.InternalServerError, google_exceptions.ServiceUnavailable, google_exceptions.DeadlineExceeded)):
                delay = max(delay, suggested_delay_seconds) if suggested_delay_seconds is not None else delay
                self.logger.warning(f"Retryable Gemini API error ({error_type_name}, Attempt {attempt}). Retrying after {delay:.1f}s.")
                should_retry = True
            elif isinstance(exc, (google_exceptions.InvalidArgument, google_exceptions.PermissionDenied,
                                  google_exceptions.Unauthenticated, google_exceptions.NotFound,
                                  google_exceptions.FailedPrecondition)):
                self.logger.error(f"Non-retryable Gemini API error ({error_type_name}): {exc}")
                should_retry = False
            else: # Other GoogleAPIError
                self.logger.error(f"Unhandled Google API error ({error_type_name}): {exc}", exc_info=False)
                should_retry = False # Default to non-retryable for unclassified Google errors
        elif isinstance(exc, ValueError) and ("コンテンツ生成ブロック" in str(exc) or "safety" in error_message_lower):
            # Safety/blocking errors are non-retryable for the same prompt
            self.logger.error(f"コンテンツ生成が安全設定またはAPIによりブロックされました: {exc}")
            should_retry = False
        # --- Classify OpenAI Errors (Placeholder) ---
        elif OPENAI_AVAILABLE and openai and isinstance(exc, openai.APIError):
            if isinstance(exc, openai.RateLimitError):
                # Similar logic to ResourceExhausted
                calculated_rate_delay = self.config.API_RATE_LIMIT_DELAY * (1.5 ** (attempt - 1))
                final_delay = suggested_delay_seconds if suggested_delay_seconds is not None else calculated_rate_delay
                delay = min(self.config.API_MAX_RETRY_DELAY * 1.5, max(delay, final_delay))
                self.logger.warning(f"OpenAI Rate Limit Error (Attempt {attempt}). Retrying after {delay:.1f}s.")
                should_retry = True
            elif isinstance(exc, (openai.APIError, openai.APITimeoutError)): # General retryable OpenAI errors
                delay = max(delay, suggested_delay_seconds) if suggested_delay_seconds is not None else delay
                self.logger.warning(f"Retryable OpenAI API error ({error_type_name}, Attempt {attempt}). Retrying after {delay:.1f}s.")
                should_retry = True
            elif isinstance(exc, (openai.BadRequestError, openai.AuthenticationError)):
                self.logger.error(f"Non-retryable OpenAI API error ({error_type_name}): {exc}")
                should_retry = False
            else: # Other OpenAI APIError
                self.logger.error(f"Unhandled OpenAI API error ({error_type_name}): {exc}", exc_info=False)
                should_retry = False
        else:
            # Unexpected errors (treat as non-retryable by default)
            self.logger.error(f"API関連の予期せぬエラー ({error_type_name}): {exc}", exc_info=False)
            should_retry = False

        return should_retry, delay

    # --- Generate Method and Helpers ---
    def generate(self, prompt: str, temperature: Optional[float] = None) -> Result[str, LLMError]:
        """
        設定されたLLMエンジンを使用してテキストを生成します (v1.8ではGeminiに焦点)。
        堅牢なリトライメカニズムを備えています。

        Args:
            prompt: 入力プロンプト文字列。
            temperature: この特定の呼び出しに対するオプションのtemperatureオーバーライド。

        Returns:
            生成されたテキスト (Ok) またはLLMError (Err) を含むResultオブジェクト。
        """
        self.logger.debug(
            f"LLMClient.generate呼び出し (Engine: {self.current_engine}, "
            f"Temp: {temperature if temperature is not None else 'Default'}, Mock: {self.use_mock})"
        )
        self.call_count += 1

        # --- Mock Mode Handling ---
        if self.use_mock:
            if self._mock_generate_internal:
                try:
                    # Use override temp or default from config
                    temp_to_use = temperature if temperature is not None \
                                  else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.80)
                    mock_response = self._mock_generate_internal(prompt, temp_to_use)
                    return mock_response.to_result()
                except Exception as e:
                    err = GenerationError(f"モック生成が予期せず失敗しました: {e}", details={"prompt_start": truncate_text(prompt, 50)})
                    self.logger.error(str(err), exc_info=True)
                    return Result.fail(err)
            else:
                return Result.fail(ConfigurationError("モックモードが有効ですが、内部モック関数が設定されていません。"))

        # --- API Call with Retry Logic ---
        last_exception: Optional[Exception] = None
        generation_result_text: Optional[str] = None

        for attempt in range(self.config.API_MAX_RETRIES + 1):
            actual_attempt = attempt + 1  # 1-based for logging and delay calculation
            try:
                self._adaptive_delay()
                start_time = time.monotonic()
                call_temp = temperature if temperature is not None \
                              else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.80)

                if self.current_engine == "gemini":
                    if not self.client or not (GEMINI_AVAILABLE and isinstance(self.client, genai.GenerativeModel)):
                        raise LLMError("Geminiクライアントが正しく初期化されていないか、利用できません。")
                    generation_result_text = self._call_gemini(prompt, call_temp)
                elif self.current_engine == "openai":
                    # if not self.client or not (OPENAI_AVAILABLE and isinstance(self.client, openai.OpenAI)): # Adjust type check
                    #     raise LLMError("OpenAIクライアントが正しく初期化されていないか、利用できません。")
                    generation_result_text = self._call_openai(prompt, call_temp) # Placeholder
                else:
                    raise ConfigurationError(f"generate()呼び出し時に無効なエンジンが選択されました: {self.current_engine}")

                duration = time.monotonic() - start_time
                self.logger.info(f"{self.current_engine} API呼び出し成功 {duration:.2f}秒 ({actual_attempt}回目)")

                if generation_result_text is not None and generation_result_text.strip():
                    return Result.ok(generation_result_text.strip())
                else:
                    last_exception = GenerationError(
                        f"{self.current_engine} が空のコンテンツを返しました。",
                        details={"prompt_start": truncate_text(prompt, 50)}
                    )
                    self.logger.warning(str(last_exception))
                    break # Exit retry loop for empty content

            except Exception as e:
                last_exception = e
                should_retry, delay = self._handle_api_error(e, actual_attempt)

                if not should_retry:
                    err = GenerationError(
                        f"{self.current_engine} API呼び出し失敗 (リトライ不可): {last_exception}",
                        details={"engine": self.current_engine, "prompt_start": truncate_text(prompt, 50)}
                    )
                    is_safety_block = isinstance(e, ValueError) and ("コンテンツ生成ブロック" in str(e) or "safety" in str(e).lower())
                    log_exc_info = not is_safety_block and not (GEMINI_AVAILABLE and isinstance(e, (google_exceptions.InvalidArgument, google_exceptions.PermissionDenied)))
                    self.logger.error(f"{err}", exc_info=log_exc_info)
                    return Result.fail(err)

                if attempt >= self.config.API_MAX_RETRIES:
                    err = GenerationError(
                        f"{self.current_engine} API呼び出しが {self.config.API_MAX_RETRIES + 1} 回の試行後も失敗しました。 "
                        f"最後のエラー: {type(last_exception).__name__}",
                        details={"last_error": str(last_exception), "engine": self.current_engine}
                    )
                    self.logger.error(str(err))
                    return Result.fail(err)

                self.logger.warning(
                    f"{self.current_engine} API呼び出しを {delay:.1f} 秒後にリトライします... "
                    f"(試行 {actual_attempt + 1}/{self.config.API_MAX_RETRIES + 1})"
                )
                time.sleep(delay)

        final_err_msg = f"{self.current_engine} 生成失敗。"
        if last_exception:
            final_err_msg += f" 最後のエラー: {type(last_exception).__name__} - {str(last_exception)}"
        else:
            final_err_msg += " 予期せぬ理由 (おそらく空応答)。"

        final_err = GenerationError(
            final_err_msg,
            details={"last_error": str(last_exception) if last_exception else "N/A", "engine": self.current_engine}
        )
        self.logger.error(str(final_err))
        return Result.fail(final_err)

    def _call_gemini(self, prompt: str, temperature: float) -> str:
        """Gemini APIを呼び出し、応答を処理する内部メソッド。"""
        if not self.client or not GEMINI_AVAILABLE or not genai or not isinstance(self.client, genai.GenerativeModel):
            raise LLMError("Geminiクライアントが利用できません。_call_gemini内部チェック。")

        gen_config_dict = self.config.GENERATION_CONFIG_DEFAULT.copy()
        gen_config_dict["temperature"] = temperature
        valid_genai_config_keys = {"temperature", "top_p", "top_k", "candidate_count", "max_output_tokens", "stop_sequences"}
        config_for_api = {k: v for k, v in gen_config_dict.items() if k in valid_genai_config_keys}

        try:
            final_generation_config = genai.types.GenerationConfig(**config_for_api)
        except TypeError as e:
            self.logger.error(f"無効なGemini GenerationConfigパラメータ: {config_for_api}")
            raise ConfigurationError(f"無効なGemini GenerationConfigパラメータ: {e}") from e

        self.logger.debug(f"Gemini API呼び出し中 (Temp: {temperature:.2f}). Prompt長: {len(prompt)}")
        if len(prompt) > 1000:
            self.logger.debug(f"Promptプレビュー: {truncate_text(prompt, 200)}")

        try:
            # Gemini APIは通常、コンテンツのリストを期待します
            response = self.client.generate_content(
                contents=[prompt],
                generation_config=final_generation_config,
                request_options={'timeout': self.config.API_TIMEOUT}
                # safety_settings=[...] # 必要に応じてここに追加
            )
        except Exception as api_call_error:
            self.logger.error(f"Gemini generate_content呼び出し失敗: {type(api_call_error).__name__} - {api_call_error}", exc_info=False)
            raise api_call_error # generate()のメインリトライループで処理させる

        # --- Process Response ---
        try:
            # 1. prompt_feedbackで即時ブロッキングを確認
            prompt_feedback = getattr(response, "prompt_feedback", None)
            if prompt_feedback and hasattr(prompt_feedback, "block_reason") and prompt_feedback.block_reason:
                block_reason_name = prompt_feedback.block_reason.name if hasattr(prompt_feedback.block_reason, 'name') else str(prompt_feedback.block_reason)
                safety_ratings_list = getattr(prompt_feedback, "safety_ratings", [])
                ratings_str = ", ".join([f"{r.category.name}={r.probability.name}" for r in safety_ratings_list if hasattr(r, 'category') and hasattr(r, 'probability')])
                raise ValueError(f"コンテンツ生成ブロック(Gemini Prompt): 理由='{block_reason_name}', 安全性評価='{ratings_str or 'N/A'}'")

            # 2. candidatesの存在確認
            if not response.candidates:
                response_level_finish_reason_obj = getattr(response, 'finish_reason', None)
                response_level_finish_reason = response_level_finish_reason_obj.name if hasattr(response_level_finish_reason_obj, 'name') else str(response_level_finish_reason_obj)
                prompt_feedback_str = str(prompt_feedback) if prompt_feedback else "N/A"
                self.logger.warning(f"Gemini応答に候補が含まれていません。Finish Reason(応答レベル): {response_level_finish_reason}. Prompt Feedback: {prompt_feedback_str}")
                return ""

            # 3. 最初の候補を処理 (通常は単一生成で1つのみ)
            candidate = response.candidates[0]

            # 4. 候補のfinish_reasonを確認
            finish_reason_obj = getattr(candidate, "finish_reason", None)
            finish_reason_name = finish_reason_obj.name if hasattr(finish_reason_obj, 'name') else str(finish_reason_obj)

            if finish_reason_obj == genai.types.Candidate.FinishReason.SAFETY:
                safety_ratings = getattr(candidate, "safety_ratings", [])
                ratings_str = ", ".join([f"{r.category.name}={r.probability.name}" for r in safety_ratings if hasattr(r, 'category') and hasattr(r, 'probability')])
                raise ValueError(f"コンテンツ生成ブロック(Gemini Candidate): 理由='SAFETY', 安全性評価='{ratings_str or 'N/A'}'")
            elif finish_reason_obj is not None and finish_reason_obj != genai.types.Candidate.FinishReason.STOP:
                self.logger.warning(f"Gemini生成候補の終了理由: {finish_reason_name}")

            # 5. content partsからテキストを安全に抽出
            content = getattr(candidate, "content", None)
            parts = getattr(content, "parts", []) if content else []

            if not parts:
                if finish_reason_obj == genai.types.Candidate.FinishReason.STOP:
                    self.logger.warning("Gemini候補はSTOPで終了しましたが、content partsがありません。")
                else:
                    self.logger.debug(f"Gemini候補にはcontent partsがありません。終了理由: {finish_reason_name}")
                return ""

            result_text = "".join(part.text for part in parts if hasattr(part, "text"))
            return result_text

        except AttributeError as e:
            self.logger.error(f"Gemini応答の解析中に属性エラー: {e}", exc_info=False)
            raise LLMError(f"Gemini応答の構造解析エラー: {e}") from e
        except Exception as e:
            self.logger.error(f"Gemini応答処理中に予期せぬエラー: {e}", exc_info=True)
            raise LLMError(f"Gemini応答処理エラー: {e}") from e

    def _call_openai(self, prompt: str, temperature: float) -> str:
        """OpenAI APIを呼び出す内部メソッド (プレースホルダ)。"""
        self.logger.warning("OpenAI呼び出し関数はv1.8ではプレースホルダであり、実装されていません。")
        raise NotImplementedError("OpenAIクライアントインタラクションはv1.8では実装されていません。")
        # Example structure if implemented:
        # if not self.client or not OPENAI_AVAILABLE or not openai or not isinstance(self.client, openai.OpenAI):
        #     raise LLMError("OpenAI client not available.")
        # try:
        #     response = self.client.chat.completions.create(
        #         model=self.config.OPENAI_MODEL_NAME, # From config
        #         messages=[{"role": "user", "content": prompt}],
        #         temperature=temperature
        #     )
        #     return response.choices[0].message.content or ""
        # except Exception as e:
        #     self.logger.error(f"OpenAI API call failed: {e}", exc_info=True)
        #     raise e # Let retry loop handle

# =============================================================================
# Part 5 End: LLM Client (Generation Logic)
# =============================================================================
# =============================================================================
# Part 6: Vocabulary Management (Initialization and Processing)
# =============================================================================

logger_vocab_manager = logging.getLogger("NGGS-Lite.VocabularyManager")  # Logger specific to vocabulary manager

@dataclass
class VocabularyItem:
    """
    Represents a single vocabulary item with associated metadata.
    v1.8: Ensures consistency and handles layer initialization.
    """
    word: str
    # Categories can be broad groupings
    categories: List[str] = field(default_factory=list)
    # Example sentence demonstrating usage
    example: Optional[str] = None
    # Specific tags for filtering (e.g., colloquialism level, era)
    tags: List[str] = field(default_factory=list)
    # Layers based on the four-layer model
    layers: Set[str] = field(default_factory=set)  # Validated against VALID_LAYERS
    # Usage count for weighted sampling
    usage_count: int = 0
    # Optional: Source of the word (e.g., 'glcai', 'default')
    source: Optional[str] = None

    def __post_init__(self):
        """Ensures layers attribute is always a set after initialization."""
        if self.layers is None:  # Handle potential None if default_factory isn't used correctly
            self.layers = set()
        elif not isinstance(self.layers, set):
            try:
                # Attempt conversion from list or other iterable
                self.layers = set(self.layers)
            except TypeError:
                # Log warning and default to empty set if conversion fails
                logger_vocab_manager.warning(
                    f"Could not convert layers '{self.layers}' to set for word '{self.word}'. "
                    f"Defaulting to empty set."
                )
                self.layers = set()
        # Optional: Validate against VALID_LAYERS here? Or rely on VocabularyManager.
        # valid_layers = VocabularyManager.VALID_LAYERS # Need access to class constant
        # self.layers = {layer for layer in self.layers if layer in valid_layers}


class VocabularyManager:
    """
    Manages the Gothic vocabulary, supporting different data formats and filtering.
    v1.8: Refined loading logic supporting priority (GLCAI > Default File > Internal),
          improved parsing and item handling.
    """
    # Define valid layers as a class constant for clarity
    VALID_LAYERS: Final[Set[str]] = {"physical", "sensory", "psychological", "symbolic"}

    def __init__(self, config: NGGSConfig):
        """
        Initializes the Vocabulary Manager using paths and priority from NGGSConfig.

        Args:
            config: The NGGSConfig object containing vocabulary paths and settings.
        """
        if not isinstance(config, NGGSConfig):
            # Logger might not be fully configured yet
            err_msg = "VocabularyManager requires a valid NGGSConfig instance."
            try:
                logging.getLogger("NGGS-Lite.VocabularyManager").critical(err_msg)
            except Exception:
                print(f"CRITICAL (VocabularyManager): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg)

        self.config = config
        self.logger = logger_vocab_manager  # Use the dedicated vocabulary logger
        self.items: List[VocabularyItem] = []
        # Initialize layered_vocabulary with keys from VALID_LAYERS
        # Maps layer name to a list of VocabularyItem objects belonging to that layer
        self.layered_vocabulary: Dict[str, List[VocabularyItem]] = {
            layer: [] for layer in self.VALID_LAYERS
        }
        # Set for quick checking of existing words (case-sensitive)
        self.all_words: Set[str] = set()
        self.loaded_source_description: str = "Unknown"

        self.logger.info("Initializing VocabularyManager...")
        # Load vocabulary based on configured priority using the utility function
        load_result = load_vocabulary(self.config)  # Pass config to the utility

        if load_result.is_ok:
            vocab_data = load_result.unwrap()
            # Determine the source description based on successful load priority if possible
            # This part is complex; simplify for now. Assume successful load from *a* source.
            # The load_vocabulary function itself doesn't return the specific source it succeeded with.
            # We can infer it by checking which file exists if needed, but for now, a generic message.
            if config.GLCAI_VOCAB_PATH and pathlib.Path(config.GLCAI_VOCAB_PATH).exists():
                self.loaded_source_description = f"GLCAI File ({config.GLCAI_VOCAB_PATH})"
            elif config.DEFAULT_VOCAB_PATH and pathlib.Path(config.DEFAULT_VOCAB_PATH).exists():
                self.loaded_source_description = f"Default NGGS File ({config.DEFAULT_VOCAB_PATH})"
            else:
                self.loaded_source_description = "Internal Default Vocabulary"

            self._process_loaded_data(vocab_data, self.loaded_source_description)
        else:
            # Error should have been logged by load_vocabulary utility
            self.logger.error(f"VocabularyManager initialization failed: {load_result.error}")
            # Initialize with empty vocab if loading fails entirely, but log the source of failure.
            self.loaded_source_description = f"Failed to load ({load_result.error})"

        self.logger.info(
            f"VocabularyManager initialized with {len(self.items)} unique items. "
            f"Source: {self.loaded_source_description}"
        )
        # Log counts per layer for debugging
        for layer, items_in_layer in self.layered_vocabulary.items():
            self.logger.debug(f"  Layer '{layer}': {len(items_in_layer)} items")

    def _process_loaded_data(self, loaded_data: Union[List[str], JsonDict], loaded_from: str) -> None:
        """Processes the vocabulary data after it has been loaded."""
        if not loaded_data:
            self.logger.error(f"Vocabulary data is empty or invalid after loading from {loaded_from}.")
            return

        self.logger.info(f"Processing vocabulary data loaded from {loaded_from}...")
        try:
            # Check data type and call appropriate parser
            if isinstance(loaded_data, dict) and any(k in self.VALID_LAYERS for k in loaded_data.keys()):
                # Likely the default NGGS layered format (keys are layer names)
                self._parse_layered_dict(loaded_data)
            elif isinstance(loaded_data, list) and loaded_data:
                first_item = loaded_data[0]
                if isinstance(first_item, dict):
                    # Likely the structured list format (e.g., from GLCAI export)
                    self._parse_structured_list(loaded_data)
                elif isinstance(first_item, str):
                    # Simple flat list of words
                    self._parse_flat_list(loaded_data)
                else:
                    self.logger.warning(
                        f"Unrecognized format for list items in vocabulary data from {loaded_from}. "
                        f"Type: {type(first_item)}"
                    )
            elif isinstance(loaded_data, dict):  # Dictionary, but not clearly layered by top-level keys
                self.logger.warning(
                    f"Vocabulary data is a dictionary but not in the expected layered format (keys are not layer names). "
                    f"Attempting to parse as unknown dictionary format (values might be word lists). Source: {loaded_from}"
                )
                self._parse_unknown_dict_format(loaded_data)
            else:
                self.logger.error(
                    f"Cannot parse vocabulary data from {loaded_from}, unexpected top-level type: {type(loaded_data)}"
                )
        except Exception as e:
            # Catch errors during the parsing process itself
            self.logger.error(f"Error processing vocabulary data structure from {loaded_from}: {e}", exc_info=True)

    def _add_item(self, item: VocabularyItem) -> None:
        """
        Adds a validated vocabulary item to the manager's lists and sets.
        Ensures word uniqueness and validates/infers layers.
        """
        if not item.word or not isinstance(item.word, str):
            self.logger.debug(f"Skipping invalid VocabularyItem: word is missing or not a string ({item})")
            return

        word = item.word.strip()
        if not word:
            self.logger.debug("Skipping invalid VocabularyItem: word is empty after stripping.")
            return
        # Check for duplicates (case-sensitive)
        if word in self.all_words:
            # self.logger.debug(f"Skipping duplicate vocabulary word: '{word}'") # Can be verbose
            return

        item.word = word  # Ensure stored word is stripped

        # --- Layer Validation and Inference ---
        original_layers = item.layers.copy()  # Keep original for logging if needed
        valid_input_layers = {layer for layer in item.layers if layer in self.VALID_LAYERS}

        if not valid_input_layers:
            # If no valid layers were provided, infer them
            inferred_layers = self._infer_layers(word)
            item.layers = {layer for layer in inferred_layers if layer in self.VALID_LAYERS}
            # Log if inference was needed due to invalid input layers
            if original_layers:  # Log only if user provided *something* invalid
                invalid_provided = original_layers - self.VALID_LAYERS
                if invalid_provided:
                    self.logger.debug(
                        f"Word '{word}': Invalid input layers {invalid_provided}. Inferred layers: {item.layers}"
                    )
        else:
            # Use only the valid layers provided by the user/data source
            item.layers = valid_input_layers

        # --- Default Layer Assignment (if still no layer) ---
        if not item.layers:
            # If still no layers after validation/inference, assign a default
            default_layer = "psychological"  # Sensible default
            self.logger.debug(
                f"Word '{word}' assigned to default layer '{default_layer}' as no layer could be inferred or validated."
            )
            item.layers = {default_layer}

        # --- Category Inference (if needed and not already provided) ---
        if not item.categories: # Only infer if categories list is empty
            item.categories = self._infer_categories(word)
            # self.logger.debug(f"Inferred categories for '{word}': {item.categories}")

        # Add the validated item
        self.items.append(item)
        self.all_words.add(word)

        # Add to the layered dictionary
        for layer_name in item.layers:
            # Layer validation happened above, key should exist
            if layer_name in self.layered_vocabulary:
                self.layered_vocabulary[layer_name].append(item)
            else:
                # This indicates a potential bug if reached
                self.logger.error(
                    f"Internal Error: Attempted to add item '{word}' to unexpected layer '{layer_name}' "
                    f"which is not in VALID_LAYERS. Skipping layer assignment for this layer."
                )
        # Logging added item can be very verbose, disable by default
        # self.logger.debug(f"Added vocab item: {item.word} (Layers: {item.layers}, Categories: {item.categories})")

    def _parse_flat_list(self, vocab_list: List[str]) -> None:
        """Parses a simple list of vocabulary words."""
        self.logger.debug(f"Parsing vocabulary from flat list ({len(vocab_list)} potential words).")
        added_count = 0
        for word_str in vocab_list:
            if isinstance(word_str, str) and word_str.strip():
                # Create item, inference will happen in _add_item
                item = VocabularyItem(word=word_str.strip())
                # Store the original word before adding to check if it was added
                original_word_for_check = item.word
                self._add_item(item)
                # Check if the word was actually added (not duplicate/empty)
                if original_word_for_check in self.all_words and any(i.word == original_word_for_check for i in self.items):
                    added_count += 1
        self.logger.info(f"Parsed {added_count} unique words from flat list.")

    def _parse_layered_dict(self, layered_dict: JsonDict) -> None:
        """Parses vocabulary from a dictionary keyed by layer."""
        self.logger.debug("Parsing vocabulary from layered dictionary.")
        added_count = 0
        total_words_processed = 0
        for layer_key, words_in_layer in layered_dict.items():
            if layer_key in self.VALID_LAYERS:
                if isinstance(words_in_layer, list):
                    total_words_processed += len(words_in_layer)
                    for word_str in words_in_layer:
                        if isinstance(word_str, str) and word_str.strip():
                            # Assign the layer explicitly
                            item = VocabularyItem(word=word_str.strip(), layers={layer_key})
                            original_word_for_check = item.word
                            self._add_item(item)
                            if original_word_for_check in self.all_words and any(i.word == original_word_for_check for i in self.items):
                                added_count += 1
                else:
                    self.logger.warning(f"Value for valid layer '{layer_key}' is not a list, skipping.")
            else:
                self.logger.warning(f"Ignoring invalid layer key '{layer_key}' in layered dictionary.")
        self.logger.info(
            f"Parsed {added_count} unique words from {len(layered_dict)} potential layers "
            f"in layered dictionary (Total words processed: {total_words_processed})."
        )

    def _parse_structured_list(self, structured_list: List[JsonDict]) -> None:
        """Parses vocabulary from a list of structured dictionaries (e.g., GLCAI format)."""
        self.logger.debug(f"Parsing vocabulary from structured list ({len(structured_list)} items).")
        added_count = 0
        skipped_count = 0
        for item_dict in structured_list:
            if not isinstance(item_dict, dict):
                skipped_count += 1
                self.logger.debug(f"Skipping non-dictionary item in structured vocab list: {item_dict}")
                continue

            word_str = item_dict.get("word")  # Standard key
            if not word_str:
                word_str = item_dict.get("term")  # Fallback key

            if isinstance(word_str, str) and word_str.strip():
                # Extract other fields safely
                categories = item_dict.get("categories", [])
                if not categories and isinstance(item_dict.get("tags"), list): # Fallback to tags for categories if categories is empty
                    categories = item_dict.get("tags", [])
                tags = item_dict.get("tags", []) # Get tags separately as well
                layers_data = item_dict.get("layers", [])
                example = item_dict.get("example")
                usage_count_raw = item_dict.get("usage_count", 0)
                source = item_dict.get("source")  # e.g., 'glcai', 'manual'

                # --- Data Type Validation and Conversion ---
                if not isinstance(categories, list):
                    categories = [str(categories)] if categories is not None else []
                if not isinstance(tags, list):
                    tags = [str(tags)] if tags is not None else []
                if not isinstance(example, (str, type(None))):
                    example = str(example) if example is not None else None
                if not isinstance(source, (str, type(None))):
                    source = str(source) if source is not None else None

                layers_set: Set[str] = set()
                if isinstance(layers_data, (list, set)):
                    layers_set = {str(layer_item) for layer_item in layers_data if isinstance(layer_item, str) and layer_item.strip()}
                elif isinstance(layers_data, str):  # Handle comma-separated string
                    layers_set = {l.strip() for l in layers_data.split(',') if l.strip()}

                usage_count = 0
                try:
                    usage_count = int(usage_count_raw) if usage_count_raw is not None else 0
                except (ValueError, TypeError):
                    self.logger.debug(f"Invalid usage_count '{usage_count_raw}' for word '{word_str}', defaulting to 0.")

                # Create VocabularyItem and add it (validation happens in _add_item)
                item = VocabularyItem(
                    word=word_str.strip(),
                    categories=[str(cat) for cat in categories if isinstance(cat, str)], # Ensure categories are strings
                    example=example or None,  # Ensure None if empty string
                    tags=[str(tag) for tag in tags if isinstance(tag, str)], # Ensure tags are strings
                    layers=layers_set,  # Pass the processed set
                    usage_count=usage_count,
                    source=source
                )
                original_word_for_check = item.word
                self._add_item(item)
                if original_word_for_check in self.all_words and any(i.word == original_word_for_check for i in self.items):
                    added_count += 1
            else:
                skipped_count += 1
                # Optionally log skipped items if debugging needed
                # self.logger.debug(f"Skipping item (missing/invalid 'word'): {item_dict}")

        self.logger.info(
            f"Parsed {added_count} unique words from structured list "
            f"({len(structured_list)} items processed, {skipped_count} skipped)."
        )

    def _parse_unknown_dict_format(self, data: JsonDict) -> None:
        """Attempts to parse a dictionary with unknown keys, assuming values are word lists."""
        self.logger.warning(
            "Attempting to parse vocabulary from dictionary with unknown structure. "
            "Assuming keys are categories/layers and values are word lists."
        )
        added_count = 0
        total_words_processed = 0
        processed_keys_count = 0

        for key, value in data.items():
            if isinstance(value, list):
                processed_keys_count += 1
                current_key_added_count = 0
                total_words_processed += len(value)
                # Treat key as potential layer or category
                potential_layer_set = {key} if key in self.VALID_LAYERS else set()
                potential_category_list = [key]  # Use key as category

                for word_str in value:
                    if isinstance(word_str, str) and word_str.strip():
                        item = VocabularyItem(
                            word=word_str.strip(),
                            categories=potential_category_list,
                            layers=potential_layer_set  # Let _add_item handle inference/validation
                        )
                        original_word_for_check = item.word
                        self._add_item(item)
                        if original_word_for_check in self.all_words and any(i.word == original_word_for_check for i in self.items):
                            current_key_added_count += 1
                if current_key_added_count > 0:
                    self.logger.debug(f"Parsed {current_key_added_count} words under unknown key '{key}'.")
                    added_count += current_key_added_count
            else:
                self.logger.debug(f"Value associated with key '{key}' in unknown dict format is not a list, skipping.")

        if added_count > 0:
            self.logger.info(
                f"Parsed {added_count} unique words from {processed_keys_count} keys in dictionary with "
                f"unknown structure (Total words processed: {total_words_processed})."
            )
        else:
            self.logger.error("Could not parse any words from the dictionary with unknown structure.")

    def _infer_categories(self, word: str) -> List[str]:
        """Infers potential categories based on keywords (simple heuristic)."""
        # Mapping based on ETI/Gothic concepts
        # Consider making this configurable in NGGSConfig
        category_patterns: Dict[str, List[str]] = {
            "境界性": ["窓", "扉", "境", "鏡", "霧", "間", "辺", "狭間", "閾", "仮面", "境界", "門"],
            "両義性": ["美醜", "聖俗", "悦楽", "苦悩", "魅惑", "恐怖", "愛憎", "二重", "パラドックス"],
            "超越侵犯": ["禁忌", "背徳", "浸食", "侵犯", "越境", "変態", "狂気", "異形", "超越", "呪い", "冒涜"],
            "不確定性": ["幻", "影", "夢", "謎", "迷", "曖昧", "朧", "囁き", "不確定", "秘密", "幻惑", "蜃気楼"],
            "内的変容": ["変容", "崩壊", "消失", "変身", "覚醒", "分裂", "記憶", "忘却", "内的", "精神", "回想"],
            "退廃": ["退廃", "頽廃", "衰退", "廃墟", "墓地", "骸骨", "腐臭", "黴", "朽ちる", "古びた", "荒廃"],
            "神秘": ["神秘", "錬金術", "黒魔術", "聖遺物", "儀式", "詠唱", "タロット", "契約", "魂", "星", "秘儀"],
            "自然": ["月光", "霧", "嵐", "沼地", "森", "茨", "薔薇", "蝙蝠", "鴉", "夜", "雨", "風", "雷鳴"]
        }
        assigned_categories: Set[str] = set()
        word_lower = word.lower()  # Normalize word case for matching

        # Check containment (simple but effective heuristic)
        for category, patterns in category_patterns.items():
            if any(pattern in word_lower for pattern in patterns):
                assigned_categories.add(category)

        # Return sorted list or a default category if none match
        return sorted(list(assigned_categories)) if assigned_categories else ["一般"]  # Default category

    def _infer_layers(self, word: str) -> Set[str]:
        """
        Infers potential layers based on keywords from config (simple heuristic).
        Returns a set of inferred layer names.
        """
        inferred_layers: Set[str] = set()
        word_lower = word.lower()
        # Use keywords defined in NGGSConfig
        layer_keywords = self.config.LAYER_KEYWORDS

        for layer, keywords in layer_keywords.items():
            # Ensure we only consider valid layers defined in the manager
            if layer in self.VALID_LAYERS:
                # Check if any keyword (as whole word or part) is present
                if any(kw in word_lower for kw in keywords):
                    inferred_layers.add(layer)

        # Return the set of inferred layers (could be empty)
        return inferred_layers

# =============================================================================
# Part 6 End: Vocabulary Management (Initialization and Processing)
# =============================================================================
# =============================================================================
# Part 7: Vocabulary Management (Getters and Usage Tracking)
# =============================================================================

class VocabularyManager:  # Re-opening class for clarity, ensure it's one continuous definition
    # --- Methods from Part 6 (__init__, _process_loaded_data, _add_item, _parse_*, _infer_*) ---
    # These methods are assumed to be defined from the previous Part 6 output.
    # For brevity, they are not repeated here but are part of the same class.

    VALID_LAYERS: Final[Set[str]] = {"physical", "sensory", "psychological", "symbolic"}

    def __init__(self, config: NGGSConfig):
        if not isinstance(config, NGGSConfig):
            err_msg = "VocabularyManager requires a valid NGGSConfig instance."
            try:
                logging.getLogger("NGGS-Lite.VocabularyManager").critical(err_msg)
            except Exception:
                print(f"CRITICAL (VocabularyManager): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg)

        self.config = config
        self.logger = logging.getLogger("NGGS-Lite.VocabularyManager") # Use specific logger
        self.items: List[VocabularyItem] = []
        self.layered_vocabulary: Dict[str, List[VocabularyItem]] = {
            layer: [] for layer in self.VALID_LAYERS
        }
        self.all_words: Set[str] = set()
        self.loaded_source_description: str = "Unknown"

        self.logger.info("Initializing VocabularyManager...")
        load_result = load_vocabulary(self.config)

        if load_result.is_ok:
            vocab_data = load_result.unwrap()
            glcai_path_str = str(config.GLCAI_VOCAB_PATH) if config.GLCAI_VOCAB_PATH else ""
            default_vocab_path_str = str(config.DEFAULT_VOCAB_PATH) if config.DEFAULT_VOCAB_PATH else ""

            if glcai_path_str and pathlib.Path(glcai_path_str).exists():
                self.loaded_source_description = f"GLCAI File ({glcai_path_str})"
            elif default_vocab_path_str and pathlib.Path(default_vocab_path_str).exists():
                self.loaded_source_description = f"Default NGGS File ({default_vocab_path_str})"
            else:
                self.loaded_source_description = "Internal Default Vocabulary"
            self._process_loaded_data(vocab_data, self.loaded_source_description)
        else:
            self.logger.error(f"VocabularyManager initialization failed: {load_result.error}")
            self.loaded_source_description = f"Failed to load ({load_result.error})"

        self.logger.info(
            f"VocabularyManager initialized with {len(self.items)} unique items. "
            f"Source: {self.loaded_source_description}"
        )
        for layer, items_in_layer in self.layered_vocabulary.items():
            self.logger.debug(f"  Layer '{layer}': {len(items_in_layer)} items")

    def _process_loaded_data(self, loaded_data: Union[List[str], JsonDict], loaded_from: str) -> None:
        if not loaded_data:
            self.logger.error(f"Vocabulary data is empty or invalid after loading from {loaded_from}.")
            return
        self.logger.info(f"Processing vocabulary data loaded from {loaded_from}...")
        try:
            if isinstance(loaded_data, dict) and any(k in self.VALID_LAYERS for k in loaded_data.keys()):
                self._parse_layered_dict(loaded_data)
            elif isinstance(loaded_data, list) and loaded_data:
                first_item = loaded_data[0]
                if isinstance(first_item, dict): self._parse_structured_list(loaded_data)
                elif isinstance(first_item, str): self._parse_flat_list(loaded_data)
                else: self.logger.warning(f"Unrecognized list item format from {loaded_from}: {type(first_item)}")
            elif isinstance(loaded_data, dict):
                self.logger.warning(f"Vocab data is dict but not layered by keys. Parsing as unknown dict. Source: {loaded_from}")
                self._parse_unknown_dict_format(loaded_data)
            else: self.logger.error(f"Cannot parse vocab from {loaded_from}, unexpected type: {type(loaded_data)}")
        except Exception as e:
            self.logger.error(f"Error processing vocabulary data structure from {loaded_from}: {e}", exc_info=True)

    def _add_item(self, item: VocabularyItem) -> None:
        if not item.word or not isinstance(item.word, str):
            self.logger.debug(f"Skipping invalid VocabularyItem: word missing/not string ({item})")
            return
        word = item.word.strip()
        if not word: self.logger.debug("Skipping invalid VocabularyItem: word empty after strip."); return
        if word in self.all_words: return
        item.word = word
        original_layers = item.layers.copy()
        valid_input_layers = {layer for layer in item.layers if layer in self.VALID_LAYERS}
        if not valid_input_layers:
            inferred_layers = self._infer_layers(word)
            item.layers = {layer for layer in inferred_layers if layer in self.VALID_LAYERS}
            if original_layers:
                invalid_provided = original_layers - self.VALID_LAYERS
                if invalid_provided: self.logger.debug(f"Word '{word}': Invalid layers {invalid_provided}. Inferred: {item.layers}")
        else: item.layers = valid_input_layers
        if not item.layers:
            default_layer = "psychological"
            self.logger.debug(f"Word '{word}' assigned default layer '{default_layer}'.")
            item.layers = {default_layer}
        if not item.categories: item.categories = self._infer_categories(word)
        self.items.append(item); self.all_words.add(word)
        for layer_name in item.layers:
            if layer_name in self.layered_vocabulary: self.layered_vocabulary[layer_name].append(item)
            else: self.logger.error(f"Internal Error: Item '{word}' to unexpected layer '{layer_name}'.")

    def _parse_flat_list(self, vocab_list: List[str]) -> None:
        self.logger.debug(f"Parsing vocabulary from flat list ({len(vocab_list)} words).")
        added = 0
        for word_str in vocab_list:
            if isinstance(word_str, str) and word_str.strip():
                item = VocabularyItem(word=word_str.strip()); original = item.word; self._add_item(item)
                if original in self.all_words and any(i.word == original for i in self.items): added += 1
        self.logger.info(f"Parsed {added} unique words from flat list.")

    def _parse_layered_dict(self, layered_dict: JsonDict) -> None:
        self.logger.debug("Parsing vocabulary from layered dictionary.")
        added = 0; total_processed = 0
        for layer_key, words_in_layer in layered_dict.items():
            if layer_key in self.VALID_LAYERS:
                if isinstance(words_in_layer, list):
                    total_processed += len(words_in_layer)
                    for word_str in words_in_layer:
                        if isinstance(word_str, str) and word_str.strip():
                            item = VocabularyItem(word=word_str.strip(), layers={layer_key}); original = item.word; self._add_item(item)
                            if original in self.all_words and any(i.word == original for i in self.items): added += 1
                else: self.logger.warning(f"Value for layer '{layer_key}' not a list.")
            else: self.logger.warning(f"Ignoring invalid layer key '{layer_key}'.")
        self.logger.info(f"Parsed {added} unique words from {len(layered_dict)} layers (Total processed: {total_processed}).")

    def _parse_structured_list(self, structured_list: List[JsonDict]) -> None:
        self.logger.debug(f"Parsing vocabulary from structured list ({len(structured_list)} items).")
        added = 0; skipped = 0
        for item_dict in structured_list:
            if not isinstance(item_dict, dict): skipped += 1; self.logger.debug(f"Skipping non-dict item: {item_dict}"); continue
            word_str = item_dict.get("word") or item_dict.get("term")
            if isinstance(word_str, str) and word_str.strip():
                categories = item_dict.get("categories", [])
                if not categories and isinstance(item_dict.get("tags"), list): categories = item_dict.get("tags", [])
                tags = item_dict.get("tags", [])
                layers_data = item_dict.get("layers", [])
                example = item_dict.get("example"); usage_raw = item_dict.get("usage_count", 0); source = item_dict.get("source")
                if not isinstance(categories, list): categories = [str(categories)] if categories is not None else []
                if not isinstance(tags, list): tags = [str(tags)] if tags is not None else []
                if not isinstance(example, (str, type(None))): example = str(example) if example is not None else None
                if not isinstance(source, (str, type(None))): source = str(source) if source is not None else None
                layers_set: Set[str] = set()
                if isinstance(layers_data, (list, set)): layers_set = {str(l) for l in layers_data if isinstance(l, str) and l.strip()}
                elif isinstance(layers_data, str): layers_set = {l.strip() for l in layers_data.split(',') if l.strip()}
                usage = 0
                try: usage = int(usage_raw) if usage_raw is not None else 0
                except (ValueError, TypeError): self.logger.debug(f"Invalid usage_count '{usage_raw}' for '{word_str}'.")
                item = VocabularyItem(word=word_str.strip(), categories=[str(c) for c in categories if isinstance(c, str)], example=example or None, tags=[str(t) for t in tags if isinstance(t, str)], layers=layers_set, usage_count=usage, source=source)
                original = item.word; self._add_item(item)
                if original in self.all_words and any(i.word == original for i in self.items): added += 1
            else: skipped += 1
        self.logger.info(f"Parsed {added} unique words from structured list ({len(structured_list)} items, {skipped} skipped).")

    def _parse_unknown_dict_format(self, data: JsonDict) -> None:
        self.logger.warning("Parsing vocab from unknown dict format.")
        added = 0; total_processed = 0; keys_processed = 0
        for key, value in data.items():
            if isinstance(value, list):
                keys_processed += 1; current_key_added = 0; total_processed += len(value)
                potential_layer = {key} if key in self.VALID_LAYERS else set(); potential_cat = [key]
                for word_str in value:
                    if isinstance(word_str, str) and word_str.strip():
                        item = VocabularyItem(word=word_str.strip(), categories=potential_cat, layers=potential_layer)
                        original = item.word; self._add_item(item)
                        if original in self.all_words and any(i.word == original for i in self.items): current_key_added += 1
                if current_key_added > 0: self.logger.debug(f"Parsed {current_key_added} words under key '{key}'."); added += current_key_added
            else: self.logger.debug(f"Value for key '{key}' not a list in unknown dict.")
        if added > 0: self.logger.info(f"Parsed {added} words from {keys_processed} keys in unknown dict (Total processed: {total_processed}).")
        else: self.logger.error("Could not parse words from unknown dict format.")

    def _infer_categories(self, word: str) -> List[str]:
        category_patterns: Dict[str, List[str]] = {
            "境界性": ["窓", "扉", "境", "鏡", "霧", "間", "辺", "狭間", "閾", "仮面", "境界", "門"],
            "両義性": ["美醜", "聖俗", "悦楽", "苦悩", "魅惑", "恐怖", "愛憎", "二重", "パラドックス"],
            "超越侵犯": ["禁忌", "背徳", "浸食", "侵犯", "越境", "変態", "狂気", "異形", "超越", "呪い", "冒涜"],
            "不確定性": ["幻", "影", "夢", "謎", "迷", "曖昧", "朧", "囁き", "不確定", "秘密", "幻惑", "蜃気楼"],
            "内的変容": ["変容", "崩壊", "消失", "変身", "覚醒", "分裂", "記憶", "忘却", "内的", "精神", "回想"],
            "退廃": ["退廃", "頽廃", "衰退", "廃墟", "墓地", "骸骨", "腐臭", "黴", "朽ちる", "古びた", "荒廃"],
            "神秘": ["神秘", "錬金術", "黒魔術", "聖遺物", "儀式", "詠唱", "タロット", "契約", "魂", "星", "秘儀"],
            "自然": ["月光", "霧", "嵐", "沼地", "森", "茨", "薔薇", "蝙蝠", "鴉", "夜", "雨", "風", "雷鳴"]
        }
        assigned: Set[str] = set(); word_lower = word.lower()
        for cat, patterns in category_patterns.items():
            if any(p in word_lower for p in patterns): assigned.add(cat)
        return sorted(list(assigned)) if assigned else ["一般"]

    def _infer_layers(self, word: str) -> Set[str]:
        inferred: Set[str] = set(); word_lower = word.lower()
        layer_keywords = self.config.LAYER_KEYWORDS
        for layer, keywords in layer_keywords.items():
            if layer in self.VALID_LAYERS:
                if any(kw in word_lower for kw in keywords): inferred.add(layer)
        return inferred

    # --- Vocabulary Getters and Usage Tracking Methods (Part 7) ---

    def get_vocabulary_for_prompt(
        self,
        layer: Optional[str] = None,
        category: Optional[str] = None,
        tags: Optional[List[str]] = None,
        style: Optional[str] = None,
        count: int = 20,
        avoid: Optional[List[str]] = None
    ) -> str:
        """
        Gets a comma-separated vocabulary list for prompts, supporting filtering
        by layer, category, tags, style, and avoiding specific words.
        Uses weighted sampling based on usage count.
        """
        if not self.items:
            self.logger.warning("Vocabulary is empty, cannot provide words for prompt.")
            return ""

        candidate_items = list(self.items) # Create a mutable copy for filtering

        # Layer Filter
        if layer:
            if layer in self.VALID_LAYERS:
                candidate_items = [item for item in candidate_items if layer in item.layers]
            else:
                self.logger.warning(f"Invalid layer '{layer}' for filtering, ignoring layer filter.")
        # Category Filter
        if category:
            candidate_items = [item for item in candidate_items if category in item.categories]
        # Tags Filter (Match All)
        if tags:
            tag_set = set(tags)
            candidate_items = [item for item in candidate_items if tag_set.issubset(set(item.tags))]
        # Avoid Filter
        if avoid:
            avoid_set = set(avoid)
            candidate_items = [item for item in candidate_items if item.word not in avoid_set]

        if not candidate_items:
            self.logger.warning(
                f"No candidate vocabulary items found after filtering "
                f"(Layer: {layer}, Cat: {category}, Tags: {tags}, Avoid: {avoid})."
            )
            return ""

        # Calculate Weights
        weights = []
        for item in candidate_items:
            base_weight = 1.0 / (item.usage_count + 1.0) ** 1.5  # Prioritize less used
            style_multiplier = 1.0
            if style and item.tags:
                item_tag_set = set(item.tags)
                if style == "colloquial_high" and "口語度高" in item_tag_set: style_multiplier = 1.5
                elif style == "colloquial_high" and "口語度低" in item_tag_set: style_multiplier = 0.5
                elif style == "colloquial_low" and "口語度低" in item_tag_set: style_multiplier = 1.5
                elif style == "colloquial_low" and "口語度高" in item_tag_set: style_multiplier = 0.5
            weights.append(max(0.01, base_weight * style_multiplier))

        # Weighted Random Sampling
        final_count = min(count, len(candidate_items))
        sampled_items: List[VocabularyItem] = []
        if final_count <= 0: return ""

        try:
            if sum(weights) > 1e-9: # Ensure weights are positive and sum is not zero
                sampled_items = random.choices(candidate_items, weights=weights, k=final_count)
            elif candidate_items: # Fallback if all weights are zero (e.g. all items used heavily)
                self.logger.debug("All candidate items have zero effective weight, using uniform sampling.")
                sampled_items = random.sample(candidate_items, k=final_count)
        except ValueError as e:
            self.logger.warning(f"Error during weighted sampling ({e}). Falling back to simple sampling.")
            if candidate_items: # Ensure there are items to sample from
                try:
                    sampled_items = random.sample(candidate_items, k=min(final_count, len(candidate_items)))
                except ValueError: # Should not happen if k is min(final_count, len)
                    self.logger.error("Fallback sampling failed. Returning first K items.")
                    sampled_items = candidate_items[:final_count]
        except Exception as e:
            self.logger.error(f"Unexpected error during vocabulary sampling: {e}", exc_info=True)
            sampled_items = candidate_items[:final_count] # Last resort

        sampled_words = [item.word for item in sampled_items]
        if sampled_words:
            self.increment_usage(sampled_words)

        return ", ".join(sampled_words)

    def get_vocabulary_by_eti_category(
        self,
        eti_category: str,
        count: int = 5,
        avoid: Optional[List[str]] = None
    ) -> str:
        """Gets vocabulary items matching an ETI category."""
        eti_to_vocab_map: Dict[str, Dict[str, Union[List[str], Set[str]]]] = {
            "境界性": {"categories": ["境界性"], "layers": {"physical", "symbolic"}},
            "両義性": {"categories": ["両義性"], "layers": {"psychological"}},
            "超越侵犯": {"categories": ["超越侵犯"], "layers": {"symbolic", "psychological"}},
            "不確定性": {"categories": ["不確定性"], "layers": {"psychological", "sensory"}},
            "内的変容": {"categories": ["内的変容"], "layers": {"psychological"}}
        }
        target_criteria = eti_to_vocab_map.get(eti_category)
        if not target_criteria:
            self.logger.warning(f"Unknown ETI category for vocab lookup: {eti_category}")
            return ""

        target_categories_list = target_criteria.get("categories", [])
        target_layers_set = target_criteria.get("layers", set())
        target_categories: Set[str] = set(target_categories_list) if isinstance(target_categories_list, list) else set()
        target_layers: Set[str] = target_layers_set if isinstance(target_layers_set, set) else set()

        filtered_items = [
            item for item in self.items
            if (not target_categories or any(cat in target_categories for cat in item.categories)) and \
               (not target_layers or any(layer_name in target_layers for layer_name in item.layers))
        ]
        if avoid:
            avoid_set = set(avoid)
            filtered_items = [item for item in filtered_items if item.word not in avoid_set]

        final_count = min(count, len(filtered_items))
        if final_count <= 0: return ""
        weights = [1.0 / (item.usage_count + 1.0)**1.5 for item in filtered_items]
        sampled_items: List[VocabularyItem] = []
        try:
            if sum(weights) > 1e-9:
                sampled_items = random.choices(filtered_items, weights=weights, k=final_count)
            elif filtered_items:
                sampled_items = random.sample(filtered_items, k=final_count)
        except ValueError: # k > population or other sampling error
            if filtered_items:
                sampled_items = random.sample(filtered_items, k=min(final_count, len(filtered_items)))
        except Exception as e:
            self.logger.error(f"Error sampling for ETI category '{eti_category}': {e}")
            sampled_items = filtered_items[:final_count]

        sampled_words = [item.word for item in sampled_items]
        if sampled_words:
            self.increment_usage(sampled_words)
        return ", ".join(sampled_words)

    def get_vocabulary_by_tags(
        self,
        tags: List[str],
        count: int = 5,
        match_all: bool = True,
        avoid: Optional[List[str]] = None
    ) -> str:
        """Gets vocabulary items matching specific tags."""
        if not tags or not self.items:
            return ""
        tag_set = set(tags)

        if match_all:  # AND condition
            filtered_items = [item for item in self.items if tag_set.issubset(set(item.tags))]
        else:  # OR condition
            filtered_items = [item for item in self.items if any(tag in tag_set for tag in item.tags)]

        if avoid:
            avoid_set = set(avoid)
            filtered_items = [item for item in filtered_items if item.word not in avoid_set]

        final_count = min(count, len(filtered_items))
        if final_count <= 0: return ""
        weights = [1.0 / (item.usage_count + 1.0)**1.5 for item in filtered_items]
        sampled_items: List[VocabularyItem] = []
        try:
            if sum(weights) > 1e-9:
                sampled_items = random.choices(filtered_items, weights=weights, k=final_count)
            elif filtered_items:
                sampled_items = random.sample(filtered_items, k=final_count)
        except ValueError:
            if filtered_items:
                sampled_items = random.sample(filtered_items, k=min(final_count, len(filtered_items)))
        except Exception as e:
            self.logger.error(f"Error sampling for tags '{tags}': {e}")
            sampled_items = filtered_items[:final_count]

        sampled_words = [item.word for item in sampled_items]
        if sampled_words:
            self.increment_usage(sampled_words)
        return ", ".join(sampled_words)

    def increment_usage(self, words: Union[str, List[str]]) -> None:
        """
        Increments the usage count for a specific word or list of words.
        Optimized for list input.
        """
        if isinstance(words, str):
            words_to_increment = {words}
        elif isinstance(words, list):
            words_to_increment = set(words)
        else:
            self.logger.warning(f"Invalid input type for increment_usage: {type(words)}")
            return

        incremented_count = 0
        # Create a mapping for quick lookup if performance is an issue for very large self.items
        # word_to_item_map = {item.word: item for item in self.items} # Consider if needed
        for item in self.items:
            if item.word in words_to_increment:
                item.usage_count += 1
                incremented_count += 1
                # Optimization: remove found words if words_to_increment is small relative to self.items
                # words_to_increment.remove(item.word)
                # if not words_to_increment: break

        if incremented_count > 0:
            self.logger.debug(f"Incremented usage count for {incremented_count} word(s).")
        # Log if some requested words were not found (can be verbose)
        # found_in_items = {item.word for item in self.items}
        # not_found_in_vocab = words_to_increment - found_in_items
        # if not_found_in_vocab:
        #     self.logger.debug(f"Attempted to increment usage for unknown word(s): {not_found_in_vocab}")


    def has_vocabulary(self) -> bool:
        """Checks if the vocabulary list is populated."""
        return bool(self.items)

# =============================================================================
# Part 7 End: Vocabulary Management (Getters and Usage Tracking)
# =============================================================================
# =============================================================================
# Part 8: Base Evaluator Structure and LLM Evaluator (v1.8 Refined)
# =============================================================================

@dataclass
class EvaluationResult:
    """
    Holds structured evaluation results from various evaluators.
    v1.8: Added confidence field. Ensures scores/reasoning are dicts.
    """
    # Use default_factory for mutable types like dict/list
    scores: Dict[str, float] = field(default_factory=dict)  # Key: metric name, Value: score (0-5)
    reasoning: Dict[str, str] = field(default_factory=dict)  # Key: metric name, Value: reasoning text
    analysis: Optional[str] = None  # General analysis text from the evaluator
    # Optional structured data from specific evaluators
    components: JsonDict = field(default_factory=dict)  # Detailed sub-scores (e.g., Subjective)
    distribution: JsonDict = field(default_factory=dict)  # Phase/layer distribution
    # Confidence scores for heuristic-based evaluations (0.0-1.0)
    confidence: Dict[str, float] = field(default_factory=dict)

    def __post_init__(self):
        """Ensure scores, reasoning, components, distribution, and confidence are dictionaries."""
        if not isinstance(self.scores, dict):
            self.scores = {}
        if not isinstance(self.reasoning, dict):
            self.reasoning = {}
        if not isinstance(self.components, dict):
            self.components = {}
        if not isinstance(self.distribution, dict):
            self.distribution = {}
        if not isinstance(self.confidence, dict):
            self.confidence = {}


# Abstract Base Class for all evaluators
class BaseEvaluator(ABC):
    """Abstract base class for all evaluators (v1.8)."""
    def __init__(self, config: NGGSConfig):
        """
        Initializes the base evaluator.

        Args:
            config: The NGGSConfig object.

        Raises:
            ConfigurationError: If config is not a valid NGGSConfig instance.
        """
        if not isinstance(config, NGGSConfig):
            # Use basic print/raise as logger might not be fully configured yet
            err_msg = "BaseEvaluator requires a valid NGGSConfig instance."
            try:
                logging.getLogger("NGGS-Lite.BaseEvaluator").critical(err_msg)
            except Exception:
                print(f"CRITICAL (BaseEvaluator): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg)
        self.config = config
        # Get logger name based on the inheriting class's name
        self.logger = logging.getLogger(f"NGGS-Lite.{self.__class__.__name__}")
        self.logger.debug(f"{self.__class__.__name__} initialized.")

    @abstractmethod
    def evaluate(self, text: str, base_llm_eval: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        Evaluates the given text based on the evaluator's specific criteria.
        Must be implemented by subclasses.

        Args:
            text: The text to evaluate.
            base_llm_eval: Optional EvaluationResult containing results from the
                           primary LLM evaluation. Subclasses can use this for context
                           or to access base LLM scores.

        Returns:
            An EvaluationResult object containing scores, reasoning, and potentially
            other analysis data specific to the evaluator.
        """
        pass  # Subclasses must provide their evaluation logic

    def _get_llm_score(self, base_llm_eval: Optional[EvaluationResult], key: str, default: float = 0.0) -> float:
        """
        Safely retrieves a numeric score from the base LLM evaluation results,
        clamping it to the standard 0.0-5.0 range.

        Args:
            base_llm_eval: The EvaluationResult from the primary LLM evaluator.
            key: The specific score key to retrieve (e.g., "overall_quality").
            default: The default value to return if the score is missing or invalid.

        Returns:
            The score as a float between 0.0 and 5.0, or the default value rounded.
        """
        score_value = default  # Start with default
        if base_llm_eval and isinstance(base_llm_eval.scores, dict):
            raw_score = base_llm_eval.scores.get(key)
            # Check if the retrieved score is a valid number
            if isinstance(raw_score, (int, float)):
                try:
                    # Clamp score to the valid range [0.0, 5.0]
                    score_value = max(0.0, min(5.0, float(raw_score)))
                except (ValueError, TypeError):
                    # This shouldn't happen if isinstance check passed, but for safety
                    self.logger.warning(f"Score for '{key}' was numeric but failed float conversion. Using default.")
                    score_value = default  # Fallback to default on conversion error
            else:
                # Log if score is present but not a number
                if raw_score is not None:
                    self.logger.debug(f"Invalid type for LLM score '{key}': {type(raw_score)}. Using default {default:.1f}.")
        # Return rounded score (typically 2 decimal places)
        return round(score_value, 2)

    def _get_llm_reasoning(self, base_llm_eval: Optional[EvaluationResult], key: str) -> Optional[str]:
        """
        Safely retrieves reasoning text (string) for a specific metric
        from the base LLM evaluation results.

        Args:
            base_llm_eval: The EvaluationResult from the primary LLM evaluator.
            key: The specific reasoning key to retrieve (e.g., "overall_quality").

        Returns:
            The reasoning string, or None if missing or invalid.
        """
        if base_llm_eval and isinstance(base_llm_eval.reasoning, dict):
            reason = base_llm_eval.reasoning.get(key)
            if isinstance(reason, str) and reason.strip():  # Ensure it's a non-empty string
                return reason.strip()
        return None  # Return None if not found or invalid


# --- Primary LLM-based Evaluator ---
class Evaluator(BaseEvaluator):
    """
    Uses the configured LLM to evaluate text based on a structured prompt.
    Handles JSON parsing and basic validation of the LLM response.
    Inherits from BaseEvaluator. (v1.8 Refined Parsing and Error Handling)
    """
    def __init__(self, llm_client: LLMClient, config: NGGSConfig, evaluation_template: str):
        """
        Initializes the LLM-based Evaluator.

        Args:
            llm_client: The configured LLMClient instance.
            config: The NGGSConfig object.
            evaluation_template: The template string for the evaluation prompt.

        Raises:
            ConfigurationError: If llm_client is invalid.
            TemplateError: If the evaluation_template is invalid.
        """
        super().__init__(config)  # Call BaseEvaluator's init
        if not isinstance(llm_client, LLMClient):
            self.logger.critical("Evaluator requires a valid LLMClient instance.")
            raise ConfigurationError("Evaluator requires a valid LLMClient instance.")
        self.llm = llm_client

        # Validate the provided evaluation template during initialization
        # Expecting at least "generated_text" placeholder
        validate_res = validate_template(evaluation_template, expected_keys=["generated_text"])
        if validate_res.is_err:
            template_err = validate_res.error
            self.logger.critical(f"Invalid evaluation_template provided: {template_err}")
            # Include details if available
            if isinstance(template_err, TemplateError) and template_err.details:
                self.logger.critical(f"Template Error Context: {template_err.details}")
            raise template_err  # Halt initialization if template is invalid

        self.evaluation_template = evaluation_template

        # Determine template version (simple check based on known keys)
        # This helps ensure the parser expects the right keys later
        template_lower = evaluation_template.lower()
        if "layer_balance" in template_lower and "emotion_arc_quality" in template_lower:
            self.template_version = "v1.7/v1.8"  # Assuming format is stable
        elif "eti" in template_lower and "subjective_depth" in template_lower:
            self.template_version = "v1.6/Legacy"  # Example check
        else:
            self.template_version = "Unknown"
        self.logger.info(f"Evaluator initialized using evaluation template version: {self.template_version}")
        self.call_count: int = 0

    def evaluate(self, text: str, base_llm_eval: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        Scores the text using an LLM call based on the evaluation template.
        Overrides BaseEvaluator.evaluate. Handles parsing and validation.

        Args:
            text: The text to evaluate.
            base_llm_eval: Not used by this primary evaluator.

        Returns:
            An EvaluationResult containing scores and reasoning from the LLM,
            or default values if the evaluation fails.
        """
        self.call_count += 1
        if not text or not isinstance(text, str) or not text.strip():
            self.logger.warning("Evaluator.evaluate called with empty or invalid text. Returning default scores.")
            scores, reasons = self._get_default_scores_and_reasons()
            return EvaluationResult(scores=scores, reasoning=reasons, analysis="入力テキストが空です。")

        self.logger.debug(f"LLM評価呼び出し {self.call_count} 開始...")

        # --- Build evaluation prompt ---
        try:
            # Use SafeDict to prevent errors if text accidentally contains format-like braces
            eval_prompt = self.evaluation_template.format_map(SafeDict({"generated_text": text}))
        except Exception as e:
            # Catch potential errors during formatting (less likely with SafeDict)
            self.logger.critical(f"評価テンプレートのフォーマットが予期せず失敗しました: {e}", exc_info=True)
            # Return default scores but indicate critical template error
            scores, reasons = self._get_default_scores_and_reasons()
            err_analysis = f"致命的エラー: 評価テンプレートのフォーマットエラー: {e}"
            return EvaluationResult(scores=scores, reasoning=reasons, analysis=err_analysis)

        # --- Call LLM for evaluation ---
        # Use configured evaluation temperature
        llm_result: Result[str, LLMError] = self.llm.generate(
            eval_prompt,
            temperature=self.config.EVALUATION_TEMPERATURE
        )

        if llm_result.is_err:
            # LLM call itself failed (network, API error after retries, etc.)
            eval_error = llm_result.error
            self.logger.error(f"LLM評価呼び出し失敗: {eval_error}")
            scores, reasons = self._get_default_scores_and_reasons()
            # Include error details in analysis string
            analysis = f"LLM評価API呼び出し失敗: {eval_error}"
            if isinstance(eval_error, NGGSError):  # Add context if available
                analysis += f" Context: {eval_error.get_context()}"
            return EvaluationResult(scores=scores, reasoning=reasons, analysis=analysis)

        evaluation_response_text = llm_result.unwrap() # .unwrap() will raise if is_err
        if not evaluation_response_text or not evaluation_response_text.strip(): # Should be caught by LLMResponse.to_result
            self.logger.error("LLMが空の評価応答を返しました。")
            scores, reasons = self._get_default_scores_and_reasons()
            return EvaluationResult(scores=scores, reasoning=reasons, analysis="LLMが空の応答を返しました。")

        # --- Parse the LLM response ---
        # _parse_evaluation_response handles JSON extraction and validation
        parse_result = self._parse_evaluation_response(evaluation_response_text)

        if parse_result.is_err:
            parse_error = parse_result.error
            self.logger.error(f"LLM評価応答の解析失敗: {parse_error}")
            scores, reasons = self._get_default_scores_and_reasons()
            # Include parsing error details in analysis
            analysis = f"応答解析失敗: {parse_error}."
            if isinstance(parse_error, NGGSError): # Check if it's NGGSError for get_context
                analysis += f" Context: {parse_error.get_context()}"
            # Add raw response preview for debugging if parsing failed
            analysis += f" 元の応答抜粋: {truncate_text(evaluation_response_text, 150)}"
            return EvaluationResult(scores=scores, reasoning=reasons, analysis=analysis)
        else:
            # Successfully parsed and validated evaluation data
            validated_data = parse_result.unwrap()
            self.logger.info("LLM評価と応答解析成功。")
            # Return the validated scores and reasoning in EvaluationResult
            return EvaluationResult(
                scores=validated_data.get("scores", {}),  # Should be populated
                reasoning=validated_data.get("reasoning", {})  # Should be populated
                # Analysis text is not generated by this evaluator directly
                # Confidence is not set by this evaluator
            )

    def _get_default_scores_and_reasons(self) -> Tuple[Dict[str, float], Dict[str, str]]:
        """Provides default scores and reasons when evaluation fails."""
        default_reason = "評価失敗またはLLM応答解析不可のためデフォルト値を使用"
        # Define expected keys based on v1.8 template (should align with v1.7)
        expected_keys = [
            "gothic_atmosphere", "stylistic_gravity", "indirect_emotion",
            "vocabulary_richness", "overall_quality", "eti",
            "subjective_depth", "phase_transition", "colloquial_gothic_blend",
            "layer_balance", "emotion_arc_quality"
        ]
        # Use a consistent default score (e.g., 2.5 or 3.0) reflecting average/poor
        default_score_value = 2.5
        default_scores = {key: default_score_value for key in expected_keys}
        default_reasons = {key: default_reason for key in expected_keys}
        return default_scores, default_reasons

    def _parse_evaluation_response(self, response_text: str) -> Result[JsonDict, JsonParsingError]:
        """
        Parses the LLM's evaluation response, expecting JSON.
        Handles various potential formats (raw JSON, markdown block).
        Validates the structure after parsing. (v1.8 Refined)

        Returns:
            Result containing a dictionary with 'scores' and 'reasoning' (Ok),
            or a JsonParsingError (Err).
        """
        self.logger.debug(f"評価応答からのJSON解析試行 (長さ {len(response_text)})...")
        json_str_candidate: Optional[str] = None
        parsed_data: Optional[JsonDict] = None
        parsing_method: str = "N/A"
        error_details: List[str] = []  # Collect parsing errors

        stripped_response = response_text.strip()

        # 1. Try parsing the entire response first (clean API response)
        if stripped_response.startswith('{') and stripped_response.endswith('}'):
            parsing_method = "応答全体をJSONとして解析"
            try:
                parsed_data = json.loads(stripped_response)
                json_str_candidate = stripped_response
            except json.JSONDecodeError as e:
                error_details.append(f"メソッド '{parsing_method}' 失敗: {e}")
                parsed_data = None  # Ensure reset on failure

        # 2. Markdown code block (allow optional 'json' tag)
        if parsed_data is None:
            # Use DOTALL to match across newlines inside the block
            # Make language tag optional and flexible (json, JSON, or nothing)
            md_match = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", stripped_response, re.IGNORECASE | re.DOTALL)
            if md_match:
                json_str_candidate = md_match.group(1).strip()
                parsing_method = "Markdownコードブロックから抽出"
                try:
                    parsed_data = json.loads(json_str_candidate)
                except json.JSONDecodeError as e:
                    error_details.append(f"メソッド '{parsing_method}' 失敗: {e}")
                    parsed_data = None
                    json_str_candidate = None # Reset if parse failed

        # 3. Strict scores/reasoning structure pattern (if specific format expected)
        # Useful if LLM adds extra text around the core JSON object.
        if parsed_data is None:
            # Regex looks for the outer braces containing scores and reasoning keys
            # This regex is quite specific; broader matching might be needed if LLM output varies more.
            outer_json_match = re.search(
                r'\{\s*"scores"\s*:\s*\{[\s\S]*?\}\s*,\s*"reasoning"\s*:\s*\{[\s\S]*?\}\s*\}',
                stripped_response, re.DOTALL
            )
            if outer_json_match:
                json_str_candidate = outer_json_match.group(0).strip()
                parsing_method = "外部JSONオブジェクト(scores/reasoning)を抽出"
                try:
                    parsed_data = json.loads(json_str_candidate)
                except json.JSONDecodeError as e:
                    error_details.append(f"メソッド '{parsing_method}' 失敗: {e}")
                    parsed_data = None
                    json_str_candidate = None

        # 4. Substring between first '{' and last '}' as a last resort
        if parsed_data is None:
            start = stripped_response.find('{')
            end = stripped_response.rfind('}')
            if start != -1 and end > start:
                json_str_candidate = stripped_response[start: end + 1]
                parsing_method = "最初と最後の'{' '}'間の部分文字列を抽出"
                try:
                    parsed_data = json.loads(json_str_candidate)
                except json.JSONDecodeError as e:
                    error_details.append(f"メソッド '{parsing_method}' 失敗: {e}")
                    # Optional: Try JSON repair library if installed
                    # try:
                    #     import json_repair
                    #     repaired_json = json_repair.repair_json(json_str_candidate)
                    #     parsed_data = json.loads(repaired_json)
                    #     json_str_candidate = repaired_json # Update with repaired string
                    #     self.logger.info("Repaired JSON structure successfully.")
                    # except ImportError: pass # json_repair not available
                    # except Exception as repair_e: error_details.append(f"JSON repair failed: {repair_e}")
                    parsed_data = None
                    json_str_candidate = None

        # --- Final Check and Validation ---
        if parsed_data is None:
            err = JsonParsingError(
                "LLM応答から有効なJSON構造を見つけられませんでした。",
                details={"errors": error_details, "response_preview": truncate_text(stripped_response, 150)}
            )
            return Result.fail(err)

        self.logger.info(f"JSON解析成功 (使用メソッド: '{parsing_method}')。構造検証中...")
        # Validate the parsed data structure and normalize scores
        validation_result = self._validate_and_structure_parsed_data(parsed_data)

        if validation_result.is_err:
            # Validation failed, return the specific EvaluationError
            err_obj = validation_result.error
            if isinstance(err_obj, EvaluationError) and json_str_candidate: # Check if it's EvaluationError
                # Add extracted JSON string to error details for debugging
                err_obj.details["extracted_json_string"] = truncate_text(json_str_candidate, 300)
            return Result.fail(err_obj) # type: ignore
        else:
            # Return the validated dictionary containing 'scores' and 'reasoning'
            return Result.ok(validation_result.unwrap())

    def _validate_and_structure_parsed_data(self, parsed_data: Any) -> Result[JsonDict, EvaluationError]:
        """Validates the parsed JSON data structure and normalizes scores/reasons."""
        if not isinstance(parsed_data, dict):
            return Result.fail(EvaluationError("解析された評価データが辞書形式ではありません。", details={"parsed_type": type(parsed_data)}))

        # Check for required top-level keys
        if "scores" not in parsed_data or not isinstance(parsed_data["scores"], dict):
            return Result.fail(EvaluationError("解析データに'scores'辞書が含まれていないか、型が不正です。", details={"keys": list(parsed_data.keys())}))
        if "reasoning" not in parsed_data or not isinstance(parsed_data["reasoning"], dict):
            # Allow reasoning to be missing but log a warning and use empty dict
            self.logger.warning("解析データに'reasoning'辞書が含まれていません。理由なしで続行します。")
            parsed_data["reasoning"] = {}  # Ensure reasoning exists as dict

        validated_scores: Dict[str, float] = {}
        validated_reasons: Dict[str, str] = {}
        validation_warnings: List[str] = []
        # Get expected keys from the default structure
        expected_score_keys_map, _ = self._get_default_scores_and_reasons()

        raw_scores = parsed_data["scores"]
        raw_reasons = parsed_data["reasoning"]

        # Validate scores for expected keys
        for key in expected_score_keys_map:
            score_val = raw_scores.get(key)
            reason_val = raw_reasons.get(key, "(理由提供なし)")  # Default if missing

            # --- Score Validation ---
            final_score = expected_score_keys_map[key]  # Use the default score for this key as fallback
            if score_val is None:
                validation_warnings.append(f"スコア'{key}'が欠落。デフォルト値 {final_score:.1f} を使用。")
            else:
                try:
                    score_num = float(score_val)
                    # Clamp score to the valid range [0.0, 5.0]
                    clamped_score = max(0.0, min(5.0, score_num))
                    if abs(clamped_score - score_num) > 1e-5: # Check if clamping occurred
                        validation_warnings.append(f"スコア'{key}' ({score_num}) が範囲外 [0.0, 5.0]。 {clamped_score:.1f} に調整。")
                    final_score = clamped_score
                except (ValueError, TypeError):
                    validation_warnings.append(f"スコア'{key}'の形式が無効: '{score_val}'。デフォルト値 {final_score:.1f} を使用。")

            validated_scores[key] = round(final_score, 2)  # Store validated score, rounded

            # --- Reason Validation ---
            if not isinstance(reason_val, str):
                validation_warnings.append(f"理由'{key}'が文字列ではありません (型: {type(reason_val)})。変換します。")
                try:
                    reason_val = str(reason_val)  # Attempt conversion
                except Exception:
                    reason_val = "(理由変換不可)"  # Fallback if conversion fails
            validated_reasons[key] = reason_val.strip() or "(理由提供なし)"  # Handle empty strings

        # Log warnings if any occurred during validation
        if validation_warnings:
            self.logger.warning("評価JSON検証での警告:")
            for warning in validation_warnings:
                self.logger.warning(f"  - {warning}")

        # Check for extra keys in scores (optional warning)
        extra_score_keys = set(raw_scores.keys()) - set(expected_score_keys_map.keys())
        if extra_score_keys:
            self.logger.warning(f"scores辞書に予期しないキーが含まれています: {extra_score_keys}")
            # Optionally add extra scores if they look valid? For v1.8, ignore them.

        # Allow extra keys in reasoning without warning

        return Result.ok({"scores": validated_scores, "reasoning": validated_reasons})

# =============================================================================
# Part 8 End: Base Evaluator Structure and LLM Evaluator
# =============================================================================
# =============================================================================
# Part 9: Evaluator Components (v1.8) - Specific Evaluators (ETI, RI)
# =============================================================================
# NOTE: This part depends on NarrativeFlowFramework being defined,
# which was planned for Part 4 (latter half) / Task 3.1 in the AI Handover Plan.
# Assuming NarrativeFlowFramework is defined above this point.

# Forward declaration for type hinting if NarrativeFlowFramework is in a later part
# For a single file, it should be defined before this part.
if 'NarrativeFlowFramework' not in globals():
    # Define a placeholder if it's not found, to allow type hinting.
    # This will cause runtime errors if not replaced by the actual class.
    # However, the AI Handover Plan specified defining it in Task 3.1 (before Part 9).
    # We proceed assuming it's defined.
    # class NarrativeFlowFramework: pass # This line is only for linting if it's truly missing
    pass


class ExtendedETIEvaluator(BaseEvaluator):
    """
    Evaluates text based on Extended ETI metrics (Boundary, Ambivalence, etc.)
    plus Phase Transition and Subjectivity heuristics. (v1.8 Refined)
    Focuses on validating/tuning existing heuristics.
    """
    def __init__(self, config: NGGSConfig, narrative_flow: 'NarrativeFlowFramework'):
        """
        Initializes the Extended ETI Evaluator.

        Args:
            config: The NGGSConfig object.
            narrative_flow: An instance of NarrativeFlowFramework for phase analysis.
                            Type hint uses string forward reference if defined later,
                            but direct type if defined earlier.

        Raises:
            ConfigurationError: If narrative_flow is not provided or invalid.
        """
        super().__init__(config)  # Call BaseEvaluator's init
        # Check if the provided narrative_flow object is of the expected type
        # A more robust check would be isinstance(narrative_flow, NarrativeFlowFramework)
        # but this depends on the actual class definition.
        if not hasattr(narrative_flow, 'analyze_phase_distribution') or \
           not hasattr(narrative_flow, 'analyze_layer_distribution'): # Added layer check
            self.logger.error("ExtendedETIEvaluator requires a valid NarrativeFlowFramework instance "
                              "with 'analyze_phase_distribution' and 'analyze_layer_distribution' methods.")
            raise ConfigurationError(
                "ExtendedETIEvaluator requires a valid NarrativeFlowFramework instance."
            )
        self.narrative_flow = narrative_flow
        # Use weights from config
        self.weights = self.config.EXTENDED_ETI_WEIGHTS.copy()
        # Validate weights sum approximately to 1.0 if needed during init
        weight_sum = sum(self.weights.values())
        if abs(weight_sum - 1.0) > 0.01: # Allow small floating point inaccuracies
            self.logger.warning(
                f"Extended ETI weights do not sum to 1.0 (Sum: {weight_sum:.3f}). "
                f"Scores might not be perfectly normalized to the 0-5 scale based on weights alone."
            )
            # Optionally normalize here if desired:
            # if weight_sum > 1e-6:
            #     self.weights = {k: v / weight_sum for k, v in self.weights.items()}
        self.logger.info("ExtendedETIEvaluator initialized with v1.8 config weights.")

    def evaluate(self, text: str, base_llm_eval: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        Calculates ETI score based on heuristics and base LLM scores.
        v1.8: Uses refined heuristics, integrates phase distribution.
        """
        if not text or not isinstance(text, str) or not text.strip():
            self.logger.warning("ExtendedETIEvaluator received empty or invalid text.")
            return EvaluationResult(analysis="入力テキストが空です。", scores={"eti_total_calculated": 0.0})

        # --- Component Score Calculation ---
        # Get scores from base LLM evaluation using helper
        # Note: Mappings are illustrative and should be refined based on prompt/model
        scores: Dict[str, float] = {
            # Map LLM evaluation results to ETI components
            "境界性": self._get_llm_score(base_llm_eval, "gothic_atmosphere", default=2.5),
            "両義性": self._get_llm_score(base_llm_eval, "indirect_emotion", default=2.5),
            "超越侵犯": self._get_llm_score(base_llm_eval, "stylistic_gravity", default=2.5),
            "不確定性": self._get_llm_score(base_llm_eval, "eti", default=2.5),  # Use direct ETI if available
            "内的変容": self._get_llm_score(base_llm_eval, "emotion_arc_quality", default=2.5),
            # Heuristic calculations (v1.8 focus: validation/tuning of these)
            "位相移行": self._evaluate_phase_transitions_heuristic(text),  # v1.8 heuristic
            "主観性": self._evaluate_subjectivity_heuristic(text)  # v1.8 heuristic
        }

        # --- Calculate Weighted Total ETI ---
        weighted_sum = 0.0
        total_applied_weight = 0.0 # Sum of weights for components that had a score
        missing_keys: List[str] = []

        # Iterate through weights defined in config to ensure all are considered
        for key, weight in self.weights.items():
            score = scores.get(key)
            if score is not None and isinstance(score, (int, float)):
                # Ensure score is within 0-5 range before weighting
                clamped_score = max(0.0, min(5.0, score))
                weighted_sum += clamped_score * weight
                total_applied_weight += weight
                # Update the scores dict with the clamped value used for calculation
                scores[key] = clamped_score # Store the (potentially clamped) score used
            else:
                missing_keys.append(key)
                self.logger.warning(f"ETI計算: キー '{key}' のスコアが見つからないか無効です。デフォルト(2.5)で計算に含めます。")
                # Assign a default score (e.g., 2.5) for missing components for weighted sum
                default_component_score = 2.5
                scores[key] = default_component_score # Store default for completeness
                weighted_sum += default_component_score * weight
                total_applied_weight += weight


        # Normalize score (handle division by zero if no weights applied)
        if total_applied_weight <= 1e-6:  # Use a small epsilon
            self.logger.error("ETI計算の合計適用重みがゼロまたは負です。正規化できません。")
            normalized_total = 0.0
        else:
            # Normalize by the sum of weights of components that actually contributed
            normalized_total = weighted_sum / total_applied_weight

        # Clamp final score after normalization
        final_eti_total = max(0.0, min(5.0, normalized_total))

        # --- Prepare Results ---
        # Add calculated total score to the scores dictionary
        scores["eti_total_calculated"] = round(final_eti_total, 2)
        # Optionally add direct LLM ETI score for reference if available
        llm_eti_score = self._get_llm_score(base_llm_eval, "eti", default=-1.0)  # Use -1 default to check if found
        if llm_eti_score >= 0: # Check if it was found and valid
            scores["eti_from_llm"] = round(llm_eti_score, 2)

        # Generate analysis text based on component scores
        analysis_text = self._generate_eti_analysis(scores)

        # Estimate phase distribution using NarrativeFlowFramework
        phase_dist: JsonDict = {}
        phase_analysis_confidence = 0.0  # Default confidence
        if self.narrative_flow:
            try:
                # Assuming analyze_phase_distribution returns a dict or can be cast/processed into one
                phase_data = self.narrative_flow.analyze_phase_distribution(text)
                if isinstance(phase_data, dict):
                    phase_dist = phase_data
                    phase_analysis_confidence = 1.0 if phase_dist else 0.5 # Simple confidence
                else:
                    self.logger.warning(f"Phase distribution analysis returned non-dict: {type(phase_data)}")
                    analysis_text += " (位相分布分析結果形式不正)"

            except Exception as e:
                self.logger.error(f"ETI評価中の位相分布分析でエラー: {e}", exc_info=True)
                analysis_text += " (位相分布分析失敗)"
        else:
            # This case should be prevented by __init__ check
            analysis_text += " (NarrativeFlowFramework利用不可)"

        # Round all component scores for the final result
        rounded_scores = {k: round(v, 2) for k, v in scores.items() if isinstance(v, (int, float))}

        # Store confidence for heuristic parts (these values are estimates)
        confidence_scores = {
            "位相移行": 0.65,  # Confidence in the heuristic calculation
            "主観性": 0.65,  # Confidence in the heuristic calculation
            "phase_distribution_analysis": round(phase_analysis_confidence, 2)
        }
        # Confidence for LLM-derived scores depends on LLM eval confidence (not tracked here)

        return EvaluationResult(
            scores=rounded_scores,
            reasoning={},  # ETI evaluator doesn't generate detailed reasoning per component
            analysis=analysis_text,
            distribution={"phase": phase_dist},  # Store phase distribution if calculated
            confidence=confidence_scores
        )

    def _evaluate_phase_transitions_heuristic(self, text: str) -> float:
        """
        Heuristically evaluates phase transition naturalness (v1.8 heuristic).
        Focuses on transition frequency and diversity as proxy for naturalness.
        Needs validation/tuning against benchmark data.
        """
        # (Implementation uses slightly refined logic based on v1.7)
        # Note: More sophisticated analysis (e.g., Markov models, context) is v1.9+
        serif_pattern = r'[「『](?:(?![」』]).)*?[」』]'
        mono_pattern = r'[（](?:(?![）]).)*?[）]'  # Thoughts in parentheses
        # Split by sentence-ending punctuation followed by whitespace/newline
        sentences = re.split(r'(?<=[。！？?!\.])\s+', text.strip())
        sentences = [s for s in sentences if s and s.strip()]
        if not sentences or len(sentences) < 2:
            return 3.0  # Neutral score if too short to judge transitions

        transitions = 0
        last_phase = "other"  # Start with a default non-specific phase
        phase_sequence: List[str] = []
        phase_counts: Dict[str, int] = {"serif": 0, "monologue": 0, "other": 0}

        for sentence in sentences:
            current_phase = "other"  # Default to 'other' (narration/live_report)
            # Simple classification based on punctuation
            is_serif = re.search(serif_pattern, sentence)
            is_mono = re.search(mono_pattern, sentence)
            if is_serif:
                current_phase = "serif"
            elif is_mono:
                current_phase = "monologue"

            phase_counts[current_phase] = phase_counts.get(current_phase, 0) + 1
            phase_sequence.append(current_phase)

            # Count transition if phase changes
            if current_phase != last_phase:
                transitions += 1
            last_phase = current_phase

        # Calculate diversity (number of phase types present)
        diversity = len([p for p, count in phase_counts.items() if count > 0])
        num_sentences = len(sentences)
        # Transition frequency (transitions per sentence)
        transition_frequency = transitions / (num_sentences - 1) if num_sentences > 1 else 0

        # --- Scoring Logic (Tunable) ---
        # 1. Diversity Score: Reward having more phase types present
        # Max score 2.5 for diversity (e.g., 0.8 per type up to 3 types)
        diversity_score = min(2.5, diversity * 0.85)

        # 2. Frequency Score: Penalize extremes (too few or too many transitions)
        # Ideal frequency around 0.25-0.5 (1 transition every 2-4 sentences)
        ideal_freq = 0.35
        freq_deviation = abs(transition_frequency - ideal_freq)
        # Max score 2.5, penalty increases with deviation
        freq_score = max(0.0, 2.5 - (freq_deviation * 4.0))

        # Combine scores (equal weight for diversity and frequency)
        # Add a small base score (e.g., 0.0) as diversity/freq cover the range
        base_score = 0.0 + diversity_score + freq_score
        score = max(0.0, min(5.0, base_score))  # Clamp final score 0-5

        return round(score, 2)

    def _evaluate_subjectivity_heuristic(self, text: str) -> float:
        """Heuristically evaluates subjectivity depth (v1.8 simple heuristic)."""
        # Uses keywords from config
        first_person = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        internal_words = self.config.SUBJECTIVE_INNER_KEYWORDS
        fp_count = sum(text.count(p) for p in first_person)
        iw_count = sum(text.count(w) for w in internal_words)

        # Normalize by text length (per 1000 characters)
        text_len_norm = max(0.1, len(text) / 1000.0) # Avoid division by zero for very short texts

        # Calculate frequency scores (adjust multipliers for 0-5 scale)
        # Target FP freq: ~5-15/k. Score peaks at 10/k, max contribution 2.5
        fp_freq = fp_count / text_len_norm
        fp_score_comp = max(0.0, min(2.5, 2.5 - abs(fp_freq - 10.0) * 0.25))

        # Target IW freq: ~20-40/k. Score peaks at 30/k, max contribution 2.5
        iw_freq = iw_count / text_len_norm
        iw_score_comp = max(0.0, min(2.5, 2.5 - abs(iw_freq - 30.0) * 0.125))

        # Combine components (equal weight for this heuristic)
        # No base score needed as components cover 0-5 range
        score = fp_score_comp + iw_score_comp

        return round(max(0.0, min(5.0, score)), 2) # Clamp to 0.0-5.0 range and round

    def _generate_eti_analysis(self, scores: Dict[str, float]) -> str:
        """Generates a brief textual analysis based on ETI component scores."""
        analysis_parts = []
        # Filter out total/LLM scores before sorting, consider only defined weights
        component_scores_tuples = [
            (k, v) for k, v in scores.items()
            if k in self.weights and isinstance(v, (int, float))
        ]
        if not component_scores_tuples:
            # Get total score if available
            eti_total_val = scores.get("eti_total_calculated", scores.get("eti_from_llm"))
            eti_total_str = f"{eti_total_val:.1f}" if eti_total_val is not None else "N/A"
            return f"ETI分析データなし。 ETI総合: {eti_total_str}"

        sorted_scores_tuples = sorted(component_scores_tuples, key=lambda item: item[1])

        weakest = sorted_scores_tuples[0] if sorted_scores_tuples else None
        strongest = sorted_scores_tuples[-1] if sorted_scores_tuples else None

        # Identify top strengths and weaknesses
        if strongest and strongest[1] >= 4.0:
            analysis_parts.append(f"強み: {get_metric_display_name(strongest[0])}({strongest[1]:.1f})")
        if weakest and weakest[1] < 3.0:  # Use a lower threshold for weakness
            analysis_parts.append(f"改善点: {get_metric_display_name(weakest[0])}({weakest[1]:.1f})")
        elif len(sorted_scores_tuples) > 1 and sorted_scores_tuples[1][1] < 3.5:  # Mention second weakest if slightly low
            second_weakest = sorted_scores_tuples[1]
            analysis_parts.append(f"要確認: {get_metric_display_name(second_weakest[0])}({second_weakest[1]:.1f})")

        # Add overall score info
        eti_total = scores.get("eti_total_calculated", scores.get("eti_from_llm", 0.0))
        analysis_parts.append(f"ETI総合: {eti_total:.1f}")

        return " ".join(analysis_parts) if analysis_parts else "ETI要素は概ねバランスが取れています。"


class RIEvaluator(BaseEvaluator):
    """
    Evaluates text based on Readability Index (RI) metrics. (v1.8 Refined)
    Focuses on clarity, rhythm, flow, load, and resonance using heuristics.
    v1.8: Tuned heuristic calculations and scoring curves.
    """
    def __init__(self, config: NGGSConfig):
        """Initializes the RI Evaluator."""
        super().__init__(config)
        # RI Weights (Adjusted slightly for v1.8 tuning)
        self.weights: Dict[str, float] = {
            "clarity": 0.30,            # Structural clarity, sentence length
            "visualRhythm": 0.20,       # Paragraphing, line breaks
            "emotionalFlow": 0.15,      # Consistency via LLM scores + keyword check
            "cognitiveLoad": 0.20,      # Sentence complexity, Kanji ratio, commas
            "interpretiveResonance": 0.15 # LLM scores (vocab, atmosphere, ETI) + symbolic density
        }
        # Validate weights sum approximately to 1.0 if needed
        weight_sum = sum(self.weights.values())
        if abs(weight_sum - 1.0) > 0.01:
            self.logger.warning(f"RI weights do not sum to 1.0 (Sum: {weight_sum:.3f})")
        self.logger.info("RIEvaluator initialized with v1.8 logic.")

    def evaluate(self, text: str, base_llm_eval: Optional[EvaluationResult] = None) -> EvaluationResult:
        """Calculates RI score based on text analysis and base LLM scores."""
        if not text or not isinstance(text, str) or not text.strip():
            self.logger.warning("RIEvaluator received empty or invalid text.")
            return EvaluationResult(analysis="入力テキストが空です。", scores={"ri_total_calculated": 0.0})

        # Calculate individual RI component scores using helper methods
        # Ensure helpers return clamped 0-5 scores
        scores: Dict[str, float] = {
            "clarity": self._calc_clarity_score(text, base_llm_eval),
            "visualRhythm": self._calc_rhythm_score(text),
            "emotionalFlow": self._calc_emotion_flow_score(text, base_llm_eval),
            "cognitiveLoad": self._calc_cognitive_load_score(text),
            "interpretiveResonance": self._calc_resonance_score(text, base_llm_eval)
        }

        # Calculate weighted total RI
        weighted_sum = 0.0
        total_applied_weight = 0.0
        for key, weight in self.weights.items():
            score = scores.get(key)  # Already clamped between 0-5 in calc methods
            if score is not None and isinstance(score, (int, float)):
                weighted_sum += score * weight
                total_applied_weight += weight
            else:
                self.logger.warning(f"RI計算: キー '{key}' のスコアが見つからないか無効です。デフォルト(2.5)を使用します。")
                # Assign default for missing component
                default_component_score = 2.5
                scores[key] = default_component_score # Store for completeness
                weighted_sum += default_component_score * weight
                total_applied_weight += weight

        if total_applied_weight <= 1e-6:
            self.logger.error("RI計算の合計適用重みがゼロまたは負です。")
            normalized_total = 0.0
        else:
            normalized_total = weighted_sum / total_applied_weight

        final_ri_total = max(0.0, min(5.0, normalized_total))  # Clamp final [0, 5]

        # Add total score to the scores dict
        scores["ri_total_calculated"] = round(final_ri_total, 2)

        # Generate analysis text
        analysis_text = self._generate_ri_analysis(scores)

        # Round component scores for consistency in the final result
        rounded_scores = {k: round(v, 2) for k, v in scores.items() if isinstance(v, (int, float))}

        # Define confidence (heuristics are moderately reliable, adjust per component)
        confidence_scores = {
            "clarity": 0.7, "visualRhythm": 0.75, "emotionalFlow": 0.6,
            "cognitiveLoad": 0.7, "interpretiveResonance": 0.65
        }
        # Overall confidence can be average or weighted average
        overall_confidence = sum(
            confidence_scores.get(k, 0.5) * w for k, w in self.weights.items()
            if k in confidence_scores # Only consider keys present in confidence_scores
        ) / max(0.01, sum(w for k, w in self.weights.items() if k in confidence_scores))

        confidence_scores["ri_total_calculated"] = round(overall_confidence, 2)

        return EvaluationResult(
            scores=rounded_scores,
            reasoning={},  # RI evaluator primarily uses heuristics
            analysis=analysis_text,
            confidence=confidence_scores
        )

    # --- RI Calculation Helper Methods (v1.8 Refined) ---
    def _get_sentences(self, text: str) -> List[str]:
        """Splits text into sentences (improved splitting)."""
        if not text:
            return []
        # Split using lookbehind for sentence-ending punctuation followed by space or newline or end
        sentences = re.split(r'(?<=[。！？?!\.])(?:\s|\n|$)+', text.strip())
        # Filter out empty strings that might result from splitting
        return [s.strip() for s in sentences if s and s.strip()]

    def _get_paragraphs(self, text: str) -> List[str]:
        """Splits text into paragraphs based on one or more empty lines."""
        if not text:
            return []
        # Split by one or more newlines (allowing optional whitespace between them)
        paragraphs = re.split(r'\n\s*\n+', text.strip())
        return [p.strip() for p in paragraphs if p and p.strip()]

    def _calc_clarity_score(self, text: str, base_llm_eval: Optional[EvaluationResult]) -> float:
        """Calculates structural clarity score (v1.8). Combines LLM + heuristics."""
        # 1. Get LLM stylistic gravity score (less weight now)
        style_score = self._get_llm_score(base_llm_eval, "stylistic_gravity", default=3.0)

        # 2. Sentence Length Analysis
        sentences = self._get_sentences(text)
        if not sentences:
            # Base score on style if no sentences, maybe lower default
            return round(max(0.0, min(5.0, style_score * 0.5)), 2) # Clamped 0-5

        lengths = [len(s) for s in sentences]
        avg_len = sum(lengths) / len(lengths) if lengths else 0

        # Penalty for very long average length (more sensitive curve)
        # Score = 5.0 - Penalty. Penalty starts > 45 chars, max penalty 4.0 at 100+ chars
        length_penalty = 0.0
        if avg_len > 100:
            length_penalty = 4.0
        elif avg_len > 45:
            length_penalty = (avg_len - 45) / (100 - 45) * 4.0
        sentence_length_score = 5.0 - length_penalty

        # Sentence Length Variance (CV: coefficient of variation)
        variance_score = 3.0  # Start at average
        if len(lengths) >= 2 and 'statistics' in sys.modules:
            try:
                mean_len = statistics.mean(lengths)
                stdev_len = statistics.stdev(lengths)  # Use sample stdev
                cv = stdev_len / mean_len if mean_len > 1e-6 else 0
                # Target CV range: 0.3 to 0.7 (Moderate variability is good)
                if 0.3 <= cv <= 0.7:
                    # Bonus within the ideal range
                    variance_score = min(5.0, 3.5 + (cv - 0.3) / 0.4 * 1.5)
                elif cv < 0.3:
                    # Penalize low variance
                    variance_score = max(1.0, 3.5 - (0.3 - cv) * 5.0)
                else:  # cv > 0.7
                    # Penalize high variance
                    variance_score = max(1.0, 3.5 - (cv - 0.7) * 3.0)
            except statistics.StatisticsError: # Not enough data points for stdev
                self.logger.debug("Clarity: Not enough data for sentence length variance calculation.")
            except Exception as e:
                self.logger.warning(f"Clarity: Sentence variance calculation error: {e}")

        # Combine scores (adjust weights v1.8: Length 60%, Variance 30%, Style 10%)
        clarity_score = (sentence_length_score * 0.60) + (variance_score * 0.30) + (style_score * 0.10)
        return round(max(0.0, min(5.0, clarity_score)), 2)  # Ensure clamped 0-5

    def _calc_rhythm_score(self, text: str) -> float:
        """Calculates visual rhythm score based on paragraphing and line breaks (v1.8)."""
        paragraphs = self._get_paragraphs(text)
        if not paragraphs or len(paragraphs) < 2:  # Need at least 2 paras for meaningful rhythm
            return 2.0  # Lower default score if poor paragraph structure

        # 1. Paragraph Length Analysis
        para_lengths = [len(p) for p in paragraphs]
        avg_para_len = sum(para_lengths) / len(paragraphs) if paragraphs else 0

        # Penalty for extreme paragraph lengths (target 100-600 chars?)
        # Score = 5.0 - Penalty
        para_len_penalty = 0.0
        if avg_para_len > 600:
            para_len_penalty = min(3.0, (avg_para_len - 600) * 0.005)
        elif avg_para_len < 100:
            para_len_penalty = min(2.5, (100 - avg_para_len) * 0.03)
        para_length_score = 5.0 - para_len_penalty

        # 2. Paragraph Length Variance (CV - similar to sentence variance)
        para_variance_score = 3.0
        if len(para_lengths) >= 2 and 'statistics' in sys.modules:
            try:
                mean_para = statistics.mean(para_lengths)
                stdev_para = statistics.stdev(para_lengths)
                cv_para = stdev_para / mean_para if mean_para > 1e-6 else 0
                # Target CV range: 0.4 to 0.8 (Allow more variance than sentences)
                if 0.4 <= cv_para <= 0.8:
                    para_variance_score = min(5.0, 3.5 + (cv_para - 0.4) / 0.4 * 1.5)
                elif cv_para < 0.4:
                    para_variance_score = max(1.0, 3.5 - (0.4 - cv_para) * 4.0)
                else:  # cv_para > 0.8
                    para_variance_score = max(1.0, 3.5 - (cv_para - 0.8) * 2.5)
            except statistics.StatisticsError:
                self.logger.debug("Rhythm: Not enough data for paragraph length variance calculation.")
            except Exception as e:
                self.logger.warning(f"Rhythm: Paragraph variance calculation error: {e}")

        # Combine scores (adjust weights v1.8: Length 55%, Variance 45%)
        rhythm_score = (para_length_score * 0.55) + (para_variance_score * 0.45)
        return round(max(0.0, min(5.0, rhythm_score)), 2)  # Clamp 0-5

    def _calc_emotion_flow_score(self, text: str, base_llm_eval: Optional[EvaluationResult]) -> float:
        """Calculates emotional flow score using LLM scores and basic heuristic (v1.8)."""
        # Primarily rely on LLM evaluation of these aspects
        indirect_emotion_score = self._get_llm_score(base_llm_eval, "indirect_emotion", default=2.5)
        emotion_arc_score = self._get_llm_score(base_llm_eval, "emotion_arc_quality", default=2.5)

        # Simple check for very low emotional keyword presence as a penalty factor
        emotion_keywords = self.config.SUBJECTIVE_INNER_KEYWORDS
        hits = sum(text.lower().count(kw.lower()) for kw in emotion_keywords)
        text_len_norm = max(0.1, len(text) / 1000.0)
        # Penalty if density is very low (e.g., < 5 hits/1k chars)
        low_density_penalty = max(0.0, min(1.0, (5.0 - (hits / text_len_norm)) * 0.1)) if text_len_norm > 0 else 0.0

        # Combine scores (Heavy weight on LLM, slight penalty for low keyword density)
        llm_weight = 0.90  # Weight for combined LLM scores
        base_llm_avg = (indirect_emotion_score + emotion_arc_score) / 2.0
        # Apply penalty factor
        flow_score = base_llm_avg * (1.0 - (low_density_penalty * (1.0 - llm_weight)))

        return round(max(0.0, min(5.0, flow_score)), 2)  # Clamp 0-5

    def _calc_cognitive_load_score(self, text: str) -> float:
        """Calculates cognitive load score based on complexity heuristics (v1.8)."""
        sentences = self._get_sentences(text)
        if not sentences:
            return 3.0  # Neutral if no sentences

        num_sentences = len(sentences)
        total_chars = sum(len(s) for s in sentences)
        if total_chars == 0:
            return 3.0  # Avoid division by zero

        # 1. Average Sentence Length Score (Penalize > 45 chars)
        avg_sent_len = total_chars / num_sentences if num_sentences > 0 else 0
        sent_len_score = max(0.0, min(5.0, 5.0 - max(0, avg_sent_len - 45.0) * 0.08))  # Slightly gentler penalty slope

        # 2. Word Complexity (proxy: Kanji ratio) - adjusted target
        kanji_count = 0
        meaningful_char_count = 0
        try:
            if 'unicodedata' in sys.modules:
                for char_val in text:
                    # P: Punctuation, S: Symbol, Z: Separator, C: Other (Control, Format, etc.)
                    if not unicodedata.category(char_val)[0] in ('P', 'S', 'Z', 'C'):
                        meaningful_char_count += 1
                        # Basic check for CJK Unified Ideographs block and extensions
                        if ('\u4e00' <= char_val <= '\u9fff' or  # CJK Unified Ideographs
                            '\u3400' <= char_val <= '\u4dbf' or  # CJK Extension A
                            '\uF900' <= char_val <= '\uFAFF'): # CJK Compatibility Ideographs
                            kanji_count += 1
            else:  # Fallback if unicodedata not available
                self.logger.warning("unicodedata module not found for Kanji ratio calculation. Using simpler method.")
                meaningful_chars = [c for c in text if not c.isspace() and not c in "。、！？「」『』（）"] # Basic filter
                meaningful_char_count = len(meaningful_chars)
                kanji_count = sum(1 for c in meaningful_chars if '\u4e00' <= c <= '\u9fff')
        except Exception as e:
            self.logger.warning(f"Cognitive Load: Character analysis failed: {e}")
            kanji_count = 0 # Default if analysis fails
            meaningful_char_count = len(text) # Use total length as fallback denominator

        kanji_ratio = kanji_count / meaningful_char_count if meaningful_char_count > 0 else 0.0
        # Target ratio 0.30-0.45. Score peaks at 0.375.
        complexity_score = max(0.0, min(5.0, 5.0 - abs(kanji_ratio - 0.375) * 15.0))  # Stronger penalty factor

        # 3. Comma Frequency Score (Target 1.5 commas/sentence)
        comma_count = text.count('、') + text.count(',')
        comma_freq = comma_count / num_sentences if num_sentences > 0 else 0
        comma_score = max(1.0, min(5.0, 4.5 - abs(comma_freq - 1.5) * 1.2))  # Peak at 4.5, gentler penalty

        # Combine scores (Weights: Sentence Length 50%, Complexity 30%, Commas 20%)
        load_score = (sent_len_score * 0.50) + (complexity_score * 0.30) + (comma_score * 0.20)
        return round(max(0.0, min(5.0, load_score)), 2)  # Clamp 0-5

    def _calc_resonance_score(self, text: str, base_llm_eval: Optional[EvaluationResult]) -> float:
        """Calculates interpretive resonance score using LLM scores + heuristics (v1.8)."""
        # Use LLM scores (give more weight to ETI and Atmosphere)
        vocab_score = self._get_llm_score(base_llm_eval, "vocabulary_richness", default=2.5)
        atmosphere_score = self._get_llm_score(base_llm_eval, "gothic_atmosphere", default=2.5)
        eti_score = self._get_llm_score(base_llm_eval, "eti", default=2.5)  # Direct ETI score

        # Symbolic keyword density heuristic (less weight)
        symbolic_keywords = self.config.LAYER_KEYWORDS.get("symbolic", [])
        symbolic_hits = sum(text.lower().count(kw.lower()) for kw in symbolic_keywords)
        text_len_norm = max(0.1, len(text) / 1000.0)
        # Score density: Target ~5 symbolic hits/1k chars, max contribution 1.5 points to base 2.5
        symbolic_density_score_comp = min(2.5, max(-1.5, (symbolic_hits / text_len_norm - 5.0) * 0.2))
        symbolic_score = 2.5 + symbolic_density_score_comp

        # Combine (Weights: Atmosphere 35%, ETI 35%, Symbolic Heuristic 15%, Vocab 15%)
        resonance_score = (atmosphere_score * 0.35) + (eti_score * 0.35) + \
                          (symbolic_score * 0.15) + (vocab_score * 0.15)
        return round(max(0.0, min(5.0, resonance_score)), 2)  # Clamp 0-5

    def _generate_ri_analysis(self, scores: Dict[str, float]) -> str:
        """Generates a brief textual analysis based on RI scores."""
        analysis_parts = []
        # Filter out total score before sorting, use defined weights keys
        component_scores_tuples = [(k, v) for k, v in scores.items() if k in self.weights and isinstance(v, (int, float))]
        if not component_scores_tuples:
            ri_total_val = scores.get("ri_total_calculated", "N/A")
            ri_total_str = f"{ri_total_val:.1f}" if isinstance(ri_total_val, float) else ri_total_val
            return f"RI分析データなし。 RI総合: {ri_total_str}"

        sorted_scores_tuples = sorted(component_scores_tuples, key=lambda item: item[1])

        weakest = sorted_scores_tuples[0] if sorted_scores_tuples else None
        strongest = sorted_scores_tuples[-1] if sorted_scores_tuples else None

        # Identify top strength and weakness
        if strongest and strongest[1] >= 4.0:
            analysis_parts.append(f"読みやすさ強み: {get_metric_display_name(strongest[0])}({strongest[1]:.1f})")
        if weakest and weakest[1] < 3.0:  # Threshold for weakness
            analysis_parts.append(f"読みやすさ改善点: {get_metric_display_name(weakest[0])}({weakest[1]:.1f})")

        # Add overall score info
        ri_total = scores.get("ri_total_calculated", 0.0)
        analysis_parts.append(f"RI総合: {ri_total:.1f}")

        return " ".join(analysis_parts) if analysis_parts else "読みやすさは概ね良好です。"

# =============================================================================
# Part 9 End: Specific Evaluator Implementations (ETI, RI)
# =============================================================================
# =============================================================================
# Part 10: Evaluator Components (v1.8) - NarrativeFlowFramework & SubjectiveEvaluator
# =============================================================================

class NarrativeFlowFramework:
    """
    Analyzes narrative phase and layer distributions, and generates flow templates.
    v1.8: Core component for understanding and guiding narrative structure.
    """
    def __init__(self, vocab_manager: VocabularyManager, config: NGGSConfig):
        """
        Initializes the NarrativeFlowFramework.

        Args:
            vocab_manager: An instance of VocabularyManager.
            config: The NGGSConfig object.
        """
        self.vocab_manager = vocab_manager
        self.config = config
        self.logger = logging.getLogger("NGGS-Lite.NarrativeFlowFramework")
        self.logger.info("NarrativeFlowFramework initialized.")

        # Define patterns for phase detection (can be refined)
        self.phase_patterns = {
            "serif": r"[「『](?:(?![」』]).)*?[」』]",  # Dialogue
            "monologue": r'[（](?:(?![）]).)*?[）]',  # Thoughts in parentheses
            # "serif_prime" might need more complex contextual detection,
            # for now, it's treated like "serif" or needs specific markers.
        }
        # Phases that are not explicitly matched by patterns are considered 'narration' or 'live_report'
        # For simplicity, we'll group them as 'narration' for now in distribution.
        # 'live_report' would require more semantic understanding.

    def analyze_phase_distribution(self, text: str) -> JsonDict:
        """
        Analyzes the text to determine the distribution of narrative phases.
        Phases: "serif", "monologue", "narration" (includes live_report, serif_prime for now).

        Args:
            text: The text to analyze.

        Returns:
            A dictionary with phase names as keys and their ratios (0.0-1.0) as values.
            Example: {"serif": 0.25, "monologue": 0.4, "narration": 0.35}
        """
        if not text or not isinstance(text, str) or not text.strip():
            self.logger.warning("analyze_phase_distribution: Input text is empty.")
            return {}

        total_length = len(text)
        if total_length == 0:
            return {}

        phase_lengths: Dict[str, int] = {
            "serif": 0,
            "monologue": 0,
            "narration": 0, # Default for text not matching other patterns
            # "live_report": 0, # Requires more advanced detection
            # "serif_prime": 0 # Requires more advanced detection
        }

        # Find all serif and monologue segments first
        serif_matches = list(re.finditer(self.phase_patterns["serif"], text))
        monologue_matches = list(re.finditer(self.phase_patterns["monologue"], text))

        # Combine and sort matches by start position to handle overlaps correctly
        # For simplicity, we assume non-overlapping or prioritize (e.g., serif over monologue if nested)
        # A more robust approach would be a proper tokenizer or state machine.
        # Here, we'll sum lengths, which might double-count if nesting occurs in a simple way.
        # A better approach for length-based distribution:
        # Create a mask of the text, marking characters by phase.

        for match in serif_matches:
            phase_lengths["serif"] += len(match.group(0))
        for match in monologue_matches:
            # Avoid double counting if a monologue is inside a serif (unlikely with Japanese punctuation)
            # A simple check: if this monologue is already counted as part of a serif, skip.
            # This is still a heuristic.
            is_nested = False
            for s_match in serif_matches:
                if s_match.start() <= match.start() and s_match.end() >= match.end():
                    is_nested = True
                    break
            if not is_nested:
                phase_lengths["monologue"] += len(match.group(0))

        # Calculate narration length
        # This is a simplification: total text length minus identified phase lengths.
        # More accurate would be to segment text and classify each segment.
        identified_phases_length = phase_lengths["serif"] + phase_lengths["monologue"]
        phase_lengths["narration"] = max(0, total_length - identified_phases_length)


        # Normalize to ratios
        distribution: JsonDict = {}
        for phase, length in phase_lengths.items():
            if phase in self.config.PHASE_BALANCE_TARGETS: # Only include target phases
                 distribution[phase] = round(length / total_length, 3) if total_length > 0 else 0.0

        # Ensure all target phases are present in the output, even if with 0 ratio
        for target_phase in self.config.PHASE_BALANCE_TARGETS:
            if target_phase not in distribution:
                distribution[target_phase] = 0.0
        
        # Adjust serif_prime if it's a target (currently, it's not detected separately)
        if "serif_prime" in self.config.PHASE_BALANCE_TARGETS and "serif_prime" not in distribution:
            distribution["serif_prime"] = 0.0 # Or potentially a fraction of "serif" if logic allows

        self.logger.debug(f"Phase distribution: {distribution}")
        return distribution


    def analyze_layer_distribution(self, text: str) -> JsonDict:
        """
        Analyzes text for the four-layer model (physical, sensory, psychological, symbolic)
        based on keyword occurrences.

        Args:
            text: The text to analyze.

        Returns:
            A dictionary with layer names as keys and their estimated ratios as values.
            Example: {"physical": 0.3, "sensory": 0.25, ...}
        """
        if not text or not isinstance(text, str) or not text.strip():
            self.logger.warning("analyze_layer_distribution: Input text is empty.")
            return {}

        layer_counts: Dict[str, int] = {layer: 0 for layer in self.VALID_LAYERS}
        text_lower = text.lower()
        total_keywords_hit = 0

        for layer, keywords in self.config.LAYER_KEYWORDS.items():
            if layer in self.VALID_LAYERS:
                for keyword in keywords:
                    # Count occurrences of each keyword
                    # Using simple count; more advanced would be NLP-based entity/concept recognition
                    hits = text_lower.count(keyword.lower())
                    if hits > 0:
                        layer_counts[layer] += hits
                        total_keywords_hit += hits
        
        distribution: JsonDict = {}
        if total_keywords_hit > 0:
            for layer, count in layer_counts.items():
                distribution[layer] = round(count / total_keywords_hit, 3)
        else: # No keywords found, distribute evenly or return zeros
            # Returning zeros if no keywords are hit might be more informative
            # than assuming an even distribution.
            for layer in self.VALID_LAYERS:
                distribution[layer] = 0.0
        
        self.logger.debug(f"Layer distribution based on keywords: {distribution}")
        return distribution

    def generate_flow_template(
        self,
        theme: str,
        emotion_arc: Optional[str],
        perspective: str
    ) -> str:
        """
        Generates a narrative flow prompt section based on theme, emotion, and perspective.

        Args:
            theme: The overarching theme of the narrative (e.g., "記憶回帰型").
            emotion_arc: The desired emotional progression (e.g., "違和感->恐怖->啓示").
            perspective: The narrative perspective (e.g., "subjective_first_person").

        Returns:
            A string to be inserted into the main generation prompt.
        """
        flow_instructions: List[str] = []
        flow_instructions.append("### 物語構成・流れに関する指示:")

        if theme == "記憶回帰型":
            flow_instructions.append(
                "- 物語は現在の場面から始まり、過去の重要な記憶や出来事へと回帰し、"
                "最終的に現在の視点に戻り、記憶を通じて得られた気づきや変化が示唆されるように構成してください。"
            )
            flow_instructions.append(
                "- 過去のシーンは、現在の状況や主人公の心理状態と関連付けられるように描写してください。"
            )
        elif theme == "境界侵犯型": # Example theme
            flow_instructions.append(
                "- 主人公が日常と非日常、あるいは現実と異界の境界に徐々に引き込まれていく過程を描写してください。"
            )
            flow_instructions.append(
                "- 境界を越える瞬間とその結果生じる変容や認識の変化をクライマックスとしてください。"
            )
        else: # Generic theme instruction
            flow_instructions.append(
                f"- テーマ「{theme}」に沿って、物語に明確な起承転結、あるいは序破急の構造を持たせてください。"
            )

        if emotion_arc:
            arc_steps = [step.strip() for step in emotion_arc.split("->")]
            if len(arc_steps) > 1:
                flow_instructions.append(
                    f"- 感情の弧「{emotion_arc}」に従い、物語を通じて主人公の感情が段階的に変化していく様子を描写してください。"
                )
                flow_instructions.append(
                    f"  - 開始時の感情: {arc_steps[0]}"
                )
                if len(arc_steps) > 2:
                    middle_emotions = ", ".join(arc_steps[1:-1])
                    flow_instructions.append(
                        f"  - 中間での感情の推移: {middle_emotions}"
                    )
                flow_instructions.append(
                    f"  - 終結時の感情: {arc_steps[-1]}"
                )
        else:
            flow_instructions.append(
                "- 物語全体を通じて、一貫した感情のトーンを保つか、あるいは自然な感情の変化を描写してください。"
            )

        if perspective == "subjective_first_person":
            flow_instructions.append(
                "- 物語の展開は、主人公（私）の主観的な体験、知覚、内省を通じて語られるようにしてください。"
            )
        elif perspective == "perspective_shift":
            flow_instructions.append(
                "- 物語の途中で効果的に視点人物を切り替えるか、あるいは同一人物の異なる時間軸からの視点を混在させることで、"
                "多角的な描写や物語の深みを狙ってください。視点変更の際は、読者が混乱しないよう明確に示してください。"
            )
        
        flow_instructions.append(
            "- クライマックスや重要な転換点では、描写の密度を高め、読者の感情移入を促してください。"
        )

        return "\n".join(flow_instructions)


class SubjectiveEvaluator(BaseEvaluator):
    """
    Evaluates the quality and depth of subjective narration. (v1.8 Refined)
    Uses more detailed heuristics and component scoring based on the v1.8 plan.
    Focuses on validating/tuning existing heuristics.
    """
    def __init__(self, config: NGGSConfig):
        """Initializes the Subjective Evaluator."""
        super().__init__(config)
        # Keywords are accessed via self.config when needed
        self.logger.info("SubjectiveEvaluator initialized with v1.8 logic.")

    def evaluate(self, text: str, base_llm_eval: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        Evaluates subjectivity based on multiple heuristics.
        v1.8: Focuses on refining component scoring based on v1.7 heuristics.
        """
        analysis_metrics: JsonDict = {}  # Store raw metrics used for analysis text
        components: Dict[str, float] = {}  # Store component scores (0-5)
        confidence: Dict[str, float] = {}  # Store confidence scores for heuristics (0-1)

        text_length = len(text)
        if text_length < 10:  # Need some text to evaluate meaningfully
            self.logger.warning("Subjective evaluation skipped: Input text too short.")
            # Return default result for empty/short text
            default_scores = {"subjective_score": 1.0}  # Lower default for empty text
            return EvaluationResult(
                scores=default_scores,
                analysis="入力テキストが短すぎるため主観性評価スキップ。",
                components={"first_person_score": 0.0, "inner_expression_score": 0.0,
                            "monologue_quality_score": 0.0, "consistency_score": 0.0},
                confidence={"subjective_score": 0.0}
            )

        # Normalize length for frequency calculations (per 1000 chars)
        text_len_norm = max(0.1, text_length / 1000.0)

        # --- 1. First-Person Pronoun Usage ---
        fp_score, fp_analysis = self._evaluate_first_person(text, text_len_norm)
        components['first_person_score'] = round(fp_score, 2)
        analysis_metrics.update(fp_analysis)
        confidence['first_person_score'] = 0.8  # High confidence for pronoun count

        # --- 2. Internal Expression Frequency & Diversity ---
        inner_score, inner_analysis = self._evaluate_internal_expression(text, text_len_norm)
        components['inner_expression_score'] = round(inner_score, 2)
        analysis_metrics.update(inner_analysis)
        confidence['inner_expression_score'] = 0.7  # Moderate confidence for keyword matching

        # --- 3. Monologue Quality (Heuristic) ---
        mono_score, mono_analysis = self._evaluate_monologue_quality(text)
        components['monologue_quality_score'] = round(mono_score, 2)
        analysis_metrics.update(mono_analysis)
        confidence['monologue_quality_score'] = 0.6  # Lower confidence for quality heuristic

        # --- 4. Perspective Consistency (Simple Check) ---
        consistency_score, consistency_analysis = self._evaluate_consistency(text)
        components['consistency_score'] = round(consistency_score, 2)
        analysis_metrics.update(consistency_analysis)
        confidence['consistency_score'] = 0.75  # Moderate-high confidence

        # --- Final Subjectivity Score (Weighted Average) ---
        # Weights prioritize inner expression and consistency (Tunable)
        subjectivity_weights: Dict[str, float] = {
            'first_person_score': 0.15,  # Reduced weight for simple frequency
            'inner_expression_score': 0.35,
            'monologue_quality_score': 0.25,  # Increased weight for quality
            'consistency_score': 0.25
        }
        weighted_sum = 0.0
        total_weight = 0.0
        component_confidences: List[float] = []  # Collect confidences for weighted average

        for key, weight in subjectivity_weights.items():
            score = components.get(key)
            comp_confidence = confidence.get(key, 0.5)  # Default confidence if missing
            if score is not None:
                # Ensure score is clamped 0-5 before weighting
                clamped_score = max(0.0, min(5.0, score))
                weighted_sum += clamped_score * weight
                total_weight += weight
                component_confidences.append(comp_confidence * weight)  # Weight the confidence
                components[key] = clamped_score  # Store clamped score
            else:
                self.logger.warning(f"主観性評価: コンポーネント '{key}' のスコアがありません。")
                # Add default score (e.g., 2.5) and weight for normalization
                default_component_score = 2.5
                components[key] = default_component_score
                weighted_sum += default_component_score * weight
                total_weight += weight
                component_confidences.append(0.5 * weight)  # Use average confidence for default

        if total_weight <= 1e-6:
            self.logger.error("主観性評価の合計重みがゼロです。")
            final_score = 0.0
            overall_confidence = 0.0
        else:
            final_score = weighted_sum / total_weight
            overall_confidence = sum(component_confidences) / total_weight if total_weight > 1e-6 else 0.0


        final_score = max(0.0, min(5.0, final_score))  # Clamp final score [0, 5]

        # --- Generate Analysis Text ---
        analysis_parts = [
            f"一人称頻度({components.get('first_person_score', 0.0):.1f})",
            f"内的表現({components.get('inner_expression_score', 0.0):.1f},"
            f"多様性{analysis_metrics.get('inner_expression_diversity', 0.0):.1f})", # Ensure key exists
            f"独白({components.get('monologue_quality_score', 0.0):.1f},"
            f"数{analysis_metrics.get('monologue_count', 0)})", # Ensure key exists
            f"視点一貫性({components.get('consistency_score', 0.0):.1f})"
        ]
        if analysis_metrics.get('consistency_warning'):
            analysis_parts.append(f"警告: {analysis_metrics['consistency_warning']}")
        analysis_text = f"主観性評価: {', '.join(analysis_parts)}."

        # Basic reasoning based on overall score
        reasoning = {
            "subjective_score": f"総合的な主観描写の質 (要素スコア参照)。最終スコア: {final_score:.1f}"
        }

        final_scores_dict = {"subjective_score": round(final_score, 2)}
        final_confidence_dict = confidence.copy() # Start with component confidences
        final_confidence_dict["subjective_score"] = round(overall_confidence, 2)

        return EvaluationResult(
            scores=final_scores_dict,  # Return the main score
            reasoning=reasoning,
            analysis=analysis_text,
            components=components,  # Store component scores for detail
            confidence=final_confidence_dict  # Store confidence scores
        )

    # --- Subjectivity Helper Methods (v1.8 Heuristics) ---
    def _evaluate_first_person(self, text: str, text_len_norm: float) -> Tuple[float, JsonDict]:
        """Evaluates first-person pronoun usage frequency and distribution."""
        fp_pronouns = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        # Use word boundaries for potentially more accurate counting
        fp_pattern = r'\b(' + '|'.join(re.escape(p) for p in fp_pronouns) + r')\b'
        fp_count = 0
        fp_matches: List[Any] = [] # To store re.Match objects
        try:
            fp_matches = list(re.finditer(fp_pattern, text))
            fp_count = len(fp_matches)
        except re.error as e:
            self.logger.warning(f"First-person pronoun regex error: {e}. Falling back to simple count.")
            fp_count = sum(text.count(p) for p in fp_pronouns)  # Simple count fallback
        
        fp_freq = fp_count / text_len_norm if text_len_norm > 0 else 0.0

        # --- Distribution Penalty (Simplified) ---
        # Penalize if all occurrences are clustered in one half of the text
        fp_dist_penalty = 0.0
        text_length = len(text)
        if fp_count >= 4 and text_length > 0 and fp_matches: # Need enough points for basic dist check
            fp_positions = sorted([m.start() / text_length for m in fp_matches])
            # Check if all points are in first or second half
            if all(p < 0.5 for p in fp_positions) or all(p >= 0.5 for p in fp_positions):
                fp_dist_penalty = 1.5  # Apply significant penalty for clustering
            # Optional: More sophisticated check (e.g., gaps) could be added here

        # --- Scoring based on frequency ---
        # Target frequency range: ~5-15 per 1k chars. Score peaks around 10/k.
        target_freq = 10.0
        ideal_score_at_target = 4.0  # Max score component from frequency
        # Score drops quadratically away from target, max penalty brings score down
        freq_penalty = ((fp_freq - target_freq) / max(1.0, target_freq))**2 * 3.0  # Quadratic penalty, scaled, avoid div by zero
        freq_score_comp = max(0.0, ideal_score_at_target - freq_penalty)

        # Combine frequency score with distribution penalty
        score = max(0.0, min(5.0, freq_score_comp + 1.0 - fp_dist_penalty))  # Base 1.0

        analysis = {
            'first_person_freq': round(fp_freq, 1),
            'first_person_count': fp_count,
            'first_person_dist_penalty': round(fp_dist_penalty, 2)
        }
        return score, analysis

    def _evaluate_internal_expression(self, text: str, text_len_norm: float) -> Tuple[float, JsonDict]:
        """Evaluates internal expression frequency and diversity."""
        inner_keywords = self.config.SUBJECTIVE_INNER_KEYWORDS
        # Use case-insensitive matching for keywords
        text_lower = text.lower()
        inner_hits: Dict[str, int] = {kw: text_lower.count(kw.lower()) for kw in inner_keywords} # Ensure kw is lowercased for count
        inner_count = sum(inner_hits.values())
        inner_freq = inner_count / text_len_norm if text_len_norm > 0 else 0.0

        # Diversity: Ratio of unique keywords used
        used_expressions_count = len([kw for kw, count in inner_hits.items() if count > 0])
        total_keywords = len(inner_keywords)
        diversity_ratio = used_expressions_count / total_keywords if total_keywords > 0 else 0.0
        # Scale diversity ratio to 0-5 score (linear mapping)
        diversity_score = diversity_ratio * 5.0

        # Frequency score component: Target ~20-45/k chars? Peak around 30/k.
        target_freq = 30.0
        # Score max 3.0 from frequency, penalize deviation
        freq_score_comp = max(0.0, min(3.0, 3.0 - abs(inner_freq - target_freq) * 0.1))

        # Combine (Weights: Diversity 60%, Frequency 40%)
        score = max(0.0, min(5.0, (diversity_score * 0.6) + (freq_score_comp * 0.4)))

        analysis = {
            'inner_expression_freq': round(inner_freq, 1),
            'inner_expression_diversity': round(diversity_ratio, 2),
            'inner_expressions_used': used_expressions_count,
            'inner_expression_total_hits': inner_count
        }
        return score, analysis

    def _evaluate_monologue_quality(self, text: str) -> Tuple[float, JsonDict]:
        """Heuristically evaluates monologue quality based on length and content markers."""
        # Regex for thoughts in Japanese parentheses （）
        mono_pattern = r'[（](?:(?![）]).)+?[）]'  # Ensure non-empty content
        # Regex for sentences likely indicating reflection/thought
        introspection_pattern = r'[^。！？「『（\)）]{15,}(?:だろうか|かもしれない|気がする|ように思う|なのか|とは|理由は|なぜだろう)[。！？?]'
        monologues: List[str] = []
        introspections: List[str] = []
        try:
            monologues = re.findall(mono_pattern, text)
            introspections = re.findall(introspection_pattern, text)
        except re.error as e:
            self.logger.warning(f"Monologue/Introspection regex error: {e}")

        all_monologues = monologues + introspections
        monologue_count = len(all_monologues)
        analysis: JsonDict = {'monologue_count': monologue_count, 'avg_monologue_length': 0.0}
        if monologue_count == 0:
            return 1.5, analysis  # Low score if no internal thought detected

        # Evaluate quality based on average length and content markers
        mono_lengths = [len(m) for m in all_monologues]
        avg_mono_len = sum(mono_lengths) / monologue_count if monologue_count > 0 else 0.0
        analysis['avg_monologue_length'] = round(avg_mono_len, 1)
        mono_quality_base = 2.0  # Start slightly below average

        # Ideal length bonus (e.g., 25-90 characters)
        if 25 < avg_mono_len < 90:
            mono_quality_base += min(1.5, (avg_mono_len - 25) / (90 - 25) * 1.5)  # Linear bonus in range
        else:
            # Penalize deviation from the ideal range
            deviation = min(abs(avg_mono_len - 25), abs(avg_mono_len - 90)) if avg_mono_len > 0 else 25 # Default deviation if avg_mono_len is 0
            mono_quality_base -= min(1.0, deviation * 0.02)  # Penalty factor

        # Bonus for internal keywords within monologues/introspections
        internal_keywords = self.config.SUBJECTIVE_INNER_KEYWORDS
        internal_in_mono_count = sum(any(kw.lower() in m.lower() for kw in internal_keywords) for m in all_monologues)
        internal_ratio = internal_in_mono_count / monologue_count if monologue_count > 0 else 0.0
        mono_quality_base += min(1.0, internal_ratio * 1.0)  # Bonus up to 1.0 based on ratio

        # Bonus for questions/uncertainty markers
        uncertainty_markers = ['?', '？', 'だろうか', 'なのか', 'かもしれない', 'なぜ']
        uncertainty_in_mono_count = sum(any(um in m for um in uncertainty_markers) for m in all_monologues)
        uncertainty_ratio = uncertainty_in_mono_count / monologue_count if monologue_count > 0 else 0.0
        mono_quality_base += min(0.5, uncertainty_ratio * 0.5)  # Smaller bonus for questions

        score = max(0.0, min(5.0, mono_quality_base))
        return score, analysis

    def _evaluate_consistency(self, text: str) -> Tuple[float, JsonDict]:
        """Evaluates perspective consistency (simple first vs third person pronoun check)."""
        fp_pronouns = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        tp_pronouns = self.config.SUBJECTIVE_THIRD_PERSON_PRONOUNS
        # Use word boundaries and escape potentially special regex characters
        fp_pattern = r'\b(?:' + '|'.join(re.escape(p) for p in fp_pronouns) + r')\b'
        tp_pattern = r'\b(?:' + '|'.join(re.escape(p) for p in tp_pronouns) + r')\b'
        fp_count = 0
        tp_count = 0
        try:
            fp_count = len(re.findall(fp_pattern, text))
            tp_count = len(re.findall(tp_pattern, text))
        except re.error as e:
            self.logger.warning(f"Pronoun regex error during consistency check: {e}")

        score = 5.0  # Assume consistent by default
        warning = None
        if fp_count > 0 and tp_count > 0:
            # Penalize mixing unless mode allows (v1.8 assumes strict consistency needed)
            score = 1.0  # Significant penalty for mixed perspective
            warning = "一人称と三人称の代名詞が混在しています。"
        elif fp_count == 0 and tp_count == 0:
            # No clear perspective - neutral score, maybe slight penalty
            score = 3.5
            # warning = "明確な一人称または三人称の代名詞が見つかりません。" # Optional warning
        # Else (only first OR only third found) -> score remains 5.0

        analysis: JsonDict = {
            'first_person_count': fp_count,
            'third_person_count': tp_count,
        }
        if warning:
            analysis['consistency_warning'] = warning

        return score, analysis

# =============================================================================
# Part 10 End: Evaluator Components (SubjectiveEvaluator & NarrativeFlowFramework)
# =============================================================================
# =============================================================================
# Part 11: TextProcessor Initialization and Core Evaluation Helper Methods
# =============================================================================

# Ensure all necessary classes defined in previous parts are available here.
# For example: NGGSConfig, LLMClient, EvaluationResult, BaseEvaluator, Evaluator,
# ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator, NarrativeFlowFramework,
# VocabularyManager, VocabularyItem, Result, NGGSError, ConfigurationError, etc.

logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")

class TextProcessor:
    """
    Orchestrates the text processing workflow: generation, evaluation, and improvement.
    v1.8 integrates Gemini, refined evaluators, GLCAI vocab intake, enhanced feedback loops,
    NDGS basic integration parser concept, and improved error handling/reporting.
    Focuses on quality improvement through iterative refinement guided by multiple metrics.
    """
    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,  # Base LLM Evaluator
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        # Optional: Inject NDGSIntegration parser if needed
        ndgs_parser: Optional[Any] = None  # Use 'Any' for now, define class later if implemented
    ):
        """
        Initializes the TextProcessor.

        Args:
            config: NGGSConfig object.
            llm_client: Initialized LLMClient.
            evaluator: Initialized base Evaluator (for LLM calls).
            vocab_manager: Initialized VocabularyManager.
            narrative_flow: Initialized NarrativeFlowFramework.
            eti_evaluator: Initialized ExtendedETIEvaluator.
            ri_evaluator: Initialized RIEvaluator.
            subjective_evaluator: Initialized SubjectiveEvaluator.
            generation_template: Template string for initial generation.
            improvement_template: Template string for improvement instruction generation.
            ndgs_parser: Optional object for parsing NDGS input data.

        Raises:
            ConfigurationError: If any essential component is missing or invalid.
            TemplateError: If templates are invalid.
        """
        self.logger = logger_text_processor  # Use dedicated processor logger

        # --- Validate and Store Core Components ---
        if not isinstance(config, NGGSConfig):
            raise ConfigurationError("TextProcessor requires a valid NGGSConfig instance.")
        if not isinstance(llm_client, LLMClient):
            raise ConfigurationError("TextProcessor requires a valid LLMClient instance.")
        if not isinstance(evaluator, Evaluator):
            raise ConfigurationError("TextProcessor requires a valid base Evaluator instance.")
        if not isinstance(vocab_manager, VocabularyManager):
            raise ConfigurationError("TextProcessor requires a valid VocabularyManager instance.")
        if not isinstance(narrative_flow, NarrativeFlowFramework):
            raise ConfigurationError("TextProcessor requires a valid NarrativeFlowFramework instance.")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator):
            raise ConfigurationError("TextProcessor requires a valid ExtendedETIEvaluator instance.")
        if not isinstance(ri_evaluator, RIEvaluator):
            raise ConfigurationError("TextProcessor requires a valid RIEvaluator instance.")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator):
            raise ConfigurationError("TextProcessor requires a valid SubjectiveEvaluator instance.")
        # NDGS parser validation is optional, depends on its expected interface
        if ndgs_parser is not None and not hasattr(ndgs_parser, 'parse'): # Basic check
            self.logger.warning("Provided ndgs_parser object may lack a 'parse' method if it's intended for direct use.")
            # Depending on how ndgs_parser is used, this might be a ConfigurationError

        self.config = config
        self.llm = llm_client
        self.evaluator = evaluator
        self.vocab_manager = vocab_manager
        self.narrative_flow = narrative_flow
        self.eti_evaluator = eti_evaluator
        self.ri_evaluator = ri_evaluator
        self.subjective_evaluator = subjective_evaluator
        self.ndgs_parser = ndgs_parser

        # --- Validate and Store Templates ---
        # Basic validation: Check if string and not empty, and has expected placeholders
        gen_template_res = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_res.is_err:
            self.logger.critical(f"Invalid generation_template: {gen_template_res.error}")
            raise gen_template_res.error  # Raise TemplateError
        self.generation_template = generation_template

        imp_template_res = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_res.is_err:
            self.logger.critical(f"Invalid improvement_template: {imp_template_res.error}")
            raise imp_template_res.error  # Raise TemplateError
        self.improvement_template = improvement_template

        self.logger.info(
            f"TextProcessor initialized (v{config.VERSION}). Engine: {self.llm.current_engine}, "
            f"Max Loops: {config.DEFAULT_MAX_LOOPS}, Threshold: {config.DEFAULT_IMPROVEMENT_THRESHOLD:.1f}"
        )

    def _get_default_aggregated_scores(self) -> JsonDict:
        """
        Returns a dictionary with default values for all aggregated score keys.
        Used when an evaluation step fails critically.
        """
        default_score = 0.0  # Or a more neutral 2.5, depending on desired behavior
        return {
            "overall_quality": default_score,
            "gothic_atmosphere": default_score,
            "stylistic_gravity": default_score,
            "indirect_emotion": default_score,
            "vocabulary_richness": default_score,
            "eti": default_score, # Represents eti_total_calculated
            "subjective_depth": default_score, # From LLM eval
            "phase_transition": default_score, # From LLM eval
            "colloquial_gothic_blend": default_score,
            "layer_balance": default_score, # From LLM eval
            "emotion_arc_quality": default_score,
            "ri": default_score, # Represents ri_total_calculated
            "subjective": default_score, # Represents subjective_score from SubjectiveEvaluator
            "phase": default_score, # Represents phase_score (derived)
            "layer": default_score, # Represents layer_balance_score (derived)
            "emotion": default_score, # Represents emotion_arc_score (derived)
            "colloquial": default_score # Represents colloquial_score (derived)
        }

    def _perform_initial_generation(
        self, initial_text: str, target_length: int, perspective_mode: str,
        phase_focus: str, colloquial_level: str, emotion_arc: Optional[str],
        narrative_flow_prompt: Optional[str]
    ) -> Result[str, LLMError]:
        """
        Handles the initial text generation call using the main _generate_text helper.
        (Placeholder for now, full implementation in a later Part focusing on generation logic)
        """
        self.logger.info("初期テキスト生成を実行中...")
        # This will call the core text generation method.
        # For Part 11, we focus on evaluation helpers. Generation logic will be in TextProcessor.process.
        # For now, return a placeholder or call a simplified _generate_text if it's defined.
        # Assuming _generate_text will be implemented later.
        # return self._generate_text(
        #     input_text=initial_text,
        #     target_length=target_length,
        #     perspective_mode=perspective_mode,
        #     phase_focus=phase_focus,
        #     colloquial_level=colloquial_level,
        #     emotion_arc=emotion_arc,
        #     narrative_flow=narrative_flow_prompt,
        #     improvement_instructions=None,
        #     temperature=None
        # )
        self.logger.warning("_perform_initial_generation is a placeholder in Part 11.")
        # Return a dummy success for evaluation flow testing
        return Result.ok(f"Placeholder initial generation based on: {truncate_text(initial_text, 50)}")


    def _perform_full_evaluation(
        self, text: str, phase_focus: str, colloquial_level: str, emotion_arc: Optional[str]
    ) -> Result[JsonDict, EvaluationError]:
        """
        Performs all evaluations (LLM, ETI, RI, Subjective, Distributions, Derived)
        and aggregates results into a single dictionary.

        Args:
            text: The text to evaluate.
            phase_focus: Phase focus parameter (used for derived scoring).
            colloquial_level: Colloquial level parameter (used for derived scoring).
            emotion_arc: Emotion arc parameter (used for derived scoring).

        Returns:
            Result object containing a dictionary with all evaluation data (Ok)
            or an EvaluationError if a critical evaluation step fails (Err).
        """
        self.logger.info(f"完全評価を実行中 (テキスト長: {len(text)})...")
        full_results: JsonDict = {
            "llm_eval_result": None, # Will store EvaluationResult object
            "eti_result": None,      # Will store EvaluationResult object
            "ri_result": None,       # Will store EvaluationResult object
            "subjective_result": None,# Will store EvaluationResult object
            "phase_distribution": {},
            "layer_distribution": {},
            "phase_score": 0.0,
            "layer_balance_score": 0.0,
            "emotion_arc_score": 0.0,
            "colloquial_score": 0.0,
            "aggregated_scores": {}, # Combined scores from all evaluators
            "errors": [],            # List of recoverable error messages during evaluation
            "llm_eval_success": False # Flag indicating if base LLM eval produced scores
        }
        # This will hold the scores that are aggregated for decision making and reporting
        aggregated_scores: Dict[str, float] = {}
        llm_eval_result_obj: Optional[EvaluationResult] = None

        # 1. Base LLM Evaluation (Most critical)
        llm_eval_result_obj = self._run_llm_eval(text, full_results, aggregated_scores)
        full_results["llm_eval_success"] = (
            llm_eval_result_obj is not None and
            isinstance(llm_eval_result_obj.scores, dict) and
            "overall_quality" in llm_eval_result_obj.scores and
            isinstance(llm_eval_result_obj.scores["overall_quality"], (int, float))
        )
        base_eval_context = llm_eval_result_obj if full_results["llm_eval_success"] else None

        # 2. Specific Heuristic Evaluators
        self._run_eti_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_ri_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_subjective_eval(text, base_eval_context, full_results, aggregated_scores)

        # 3. Distribution Analysis
        phase_dist, layer_dist = self._run_distribution_analysis(text, full_results)

        # 4. Calculate Derived Scores
        self._calculate_and_store_derived_scores(
            text, phase_dist, layer_dist, phase_focus, colloquial_level, emotion_arc,
            full_results, aggregated_scores
        )

        full_results["aggregated_scores"] = {
            k: round(v, 2) for k, v in aggregated_scores.items()
            if isinstance(v, (int, float))
        }

        if full_results["errors"]:
            self.logger.warning(f"完全評価完了、ただし {len(full_results['errors'])} 件の回復可能エラーが発生しました。")
        else:
            self.logger.info("完全評価成功。")
        return Result.ok(full_results)

    def _run_llm_eval(self, text: str, full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> Optional[EvaluationResult]:
        """Runs the base LLM evaluator, updates results_dict and agg_scores."""
        self.logger.debug("基本LLM評価を実行中...")
        default_s, _ = self.evaluator._get_default_scores_and_reasons() # pylint: disable=protected-access
        eval_result_obj: Optional[EvaluationResult] = None
        try:
            eval_result_obj = self.evaluator.evaluate(text)
            full_results_dict["llm_eval_result"] = eval_result_obj

            if eval_result_obj.scores and "overall_quality" in eval_result_obj.scores:
                # Ensure all scores are valid floats before adding to aggregated_scores
                valid_scores = {
                    k: float(v) for k, v in eval_result_obj.scores.items()
                    if isinstance(v, (int, float))
                }
                agg_scores.update(valid_scores)
                self.logger.info("基本LLM評価成功。")
            else:
                msg = eval_result_obj.analysis or "基本LLM評価が有効なスコアを生成しませんでした。"
                full_results_dict["errors"].append(f"LLM Eval: {msg}")
                self.logger.error(f"基本LLM評価失敗または無効データ: {msg}")
                agg_scores.update(default_s) # Populate with defaults
        except Exception as e:
            crit_err_msg = f"基本LLM評価中の致命的エラー: {e}"
            eval_error = EvaluationError(crit_err_msg) # Wrap it
            self.logger.critical(str(eval_error), exc_info=True)
            full_results_dict["errors"].append(str(eval_error))
            agg_scores.update(default_s)
            eval_result_obj = EvaluationResult(analysis=f"LLM評価エラー: {e}", scores=default_s.copy())
            full_results_dict["llm_eval_result"] = eval_result_obj
        return eval_result_obj


    def _run_eti_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("ETI評価を実行中...")
        default_score_val = 2.5
        try:
            eti_res = self.eti_evaluator.evaluate(text, base_eval_ctx)
            full_results_dict["eti_result"] = eti_res
            main_score = eti_res.scores.get("eti_total_calculated")
            if isinstance(main_score, (int, float)):
                agg_scores["eti"] = float(main_score)
            else:
                agg_scores["eti"] = default_score_val
                warn_msg = "ETI評価が数値スコア 'eti_total_calculated' を返しませんでした。"
                self.logger.warning(warn_msg)
                full_results_dict["errors"].append(f"ETI Eval: {warn_msg}")
        except Exception as e:
            msg = f"ETI評価失敗: {e}"
            self.logger.error(msg, exc_info=True)
            full_results_dict["errors"].append(msg)
            full_results_dict["eti_result"] = EvaluationResult(analysis=f"ETIエラー: {e}", scores={"eti_total_calculated": default_score_val})
            agg_scores["eti"] = default_score_val

    def _run_ri_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("RI評価を実行中...")
        default_score_val = 2.5
        try:
            ri_res = self.ri_evaluator.evaluate(text, base_eval_ctx)
            full_results_dict["ri_result"] = ri_res
            main_score = ri_res.scores.get("ri_total_calculated")
            if isinstance(main_score, (int, float)):
                agg_scores["ri"] = float(main_score)
            else:
                agg_scores["ri"] = default_score_val
                warn_msg = "RI評価が数値スコア 'ri_total_calculated' を返しませんでした。"
                self.logger.warning(warn_msg)
                full_results_dict["errors"].append(f"RI Eval: {warn_msg}")
        except Exception as e:
            msg = f"RI評価失敗: {e}"
            self.logger.error(msg, exc_info=True)
            full_results_dict["errors"].append(msg)
            full_results_dict["ri_result"] = EvaluationResult(analysis=f"RIエラー: {e}", scores={"ri_total_calculated": default_score_val})
            agg_scores["ri"] = default_score_val

    def _run_subjective_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("主観性評価を実行中...")
        default_score_val = 2.5
        try:
            subj_res = self.subjective_evaluator.evaluate(text, base_eval_ctx)
            full_results_dict["subjective_result"] = subj_res
            main_score = subj_res.scores.get("subjective_score")
            if isinstance(main_score, (int, float)):
                agg_scores["subjective"] = float(main_score)
            else:
                agg_scores["subjective"] = default_score_val
                warn_msg = "主観性評価が数値スコア 'subjective_score' を返しませんでした。"
                self.logger.warning(warn_msg)
                full_results_dict["errors"].append(f"Subjective Eval: {warn_msg}")
        except Exception as e:
            msg = f"主観性評価失敗: {e}"
            self.logger.error(msg, exc_info=True)
            full_results_dict["errors"].append(msg)
            full_results_dict["subjective_result"] = EvaluationResult(analysis=f"主観性エラー: {e}", scores={"subjective_score": default_score_val})
            agg_scores["subjective"] = default_score_val

    def _run_distribution_analysis(self, text: str, full_results_dict: JsonDict) -> Tuple[JsonDict, JsonDict]:
        """Runs phase and layer distribution analysis using NarrativeFlowFramework."""
        self.logger.debug("分布分析 (位相・層) を実行中...")
        phase_dist: JsonDict = {}
        layer_dist: JsonDict = {}
        if self.narrative_flow:
            try:
                phase_dist = self.narrative_flow.analyze_phase_distribution(text)
                full_results_dict["phase_distribution"] = phase_dist
                self.logger.debug(f"位相分布: {phase_dist}")
            except Exception as e:
                msg = f"位相分布分析失敗: {e}"
                self.logger.error(msg, exc_info=True)
                full_results_dict["errors"].append(msg)
                full_results_dict["phase_distribution"] = {}
            try:
                layer_dist = self.narrative_flow.analyze_layer_distribution(text)
                full_results_dict["layer_distribution"] = layer_dist
                self.logger.debug(f"層分布: {layer_dist}")
            except Exception as e:
                msg = f"層分布分析失敗: {e}"
                self.logger.error(msg, exc_info=True)
                full_results_dict["errors"].append(msg)
                full_results_dict["layer_distribution"] = {}
        else:
            msg = "NarrativeFlowFrameworkが利用不可のため分布分析をスキップ。"
            self.logger.warning(msg)
            full_results_dict["errors"].append(msg)
            full_results_dict["phase_distribution"] = {}
            full_results_dict["layer_distribution"] = {}
        return phase_dist, layer_dist

    def _calculate_and_store_derived_scores(
        self, text: str, phase_dist: JsonDict, layer_dist: JsonDict, phase_focus: str,
        colloquial_level: str, emotion_arc: Optional[str],
        full_results_dict: JsonDict, agg_scores: Dict[str, float]
    ) -> None:
        """Calculates derived scores (Phase, Layer, Emotion, Colloquial) based on analysis."""
        self.logger.debug("派生スコア (位相、層、感情、口語) を計算中...")

        def _safe_calculate(func: Callable[..., float], *args: Any, score_name: str) -> float:
            default_score_val = 2.5
            try:
                score = func(*args)
                if not isinstance(score, (int, float)):
                    self.logger.warning(f"{score_name} calculation returned non-numeric: {type(score)}. Defaulting.")
                    full_results_dict["errors"].append(f"{score_name} Score Error: Invalid type {type(score)}")
                    return default_score_val
                return max(0.0, min(5.0, float(score))) # Clamp
            except Exception as e:
                msg = f"{score_name} スコアリング失敗: {e}"
                self.logger.warning(msg, exc_info=False)
                full_results_dict["errors"].append(msg)
                return default_score_val

        phase_score = _safe_calculate(self._calculate_phase_score, phase_dist, phase_focus, score_name="Phase")
        layer_score = _safe_calculate(self._calculate_layer_balance_score, layer_dist, score_name="Layer Balance")
        emotion_score = _safe_calculate(self._calculate_emotion_arc_score, text, emotion_arc, score_name="Emotion Arc")
        colloquial_score = _safe_calculate(self._calculate_colloquial_score, text, colloquial_level, score_name="Colloquial")

        full_results_dict["phase_score"] = round(phase_score, 2)
        full_results_dict["layer_balance_score"] = round(layer_score, 2)
        full_results_dict["emotion_arc_score"] = round(emotion_score, 2)
        full_results_dict["colloquial_score"] = round(colloquial_score, 2)
        agg_scores["phase"] = full_results_dict["phase_score"]
        agg_scores["layer"] = full_results_dict["layer_balance_score"]
        agg_scores["emotion"] = full_results_dict["emotion_arc_score"]
        agg_scores["colloquial"] = full_results_dict["colloquial_score"]
        self.logger.debug(
            f"派生スコア: Phase={phase_score:.1f}, Layer={layer_score:.1f}, "
            f"Emotion={emotion_score:.1f}, Colloquial={colloquial_score:.1f}"
        )

    def _calculate_phase_score(self, phase_distribution: JsonDict, phase_focus: str) -> float:
        """Calculates phase score based on distribution balance and focus."""
        if not phase_distribution: return 2.0
        target_dist = self.config.PHASE_BALANCE_TARGETS
        tolerance = self.config.PHASE_DEVIATION_TOLERANCE
        focus_target_ratio = self.config.PHASE_SCORE_FOCUS_TARGET_RATIO
        penalty_factor = self.config.PHASE_SCORE_BALANCE_PENALTY_FACTOR
        diversity_bonus = self.config.PHASE_SCORE_DIVERSITY_BONUS

        total_deviation = 0.0; valid_phases = 0
        for phase, target_ratio in target_dist.items():
            if phase in phase_distribution:
                deviation = abs(phase_distribution.get(phase, 0.0) - target_ratio)
                penalty = max(0, deviation - tolerance) * penalty_factor
                total_deviation += penalty
                valid_phases +=1

        max_possible_penalty = penalty_factor * valid_phases if valid_phases > 0 else 1.0
        balance_score_comp = max(0.0, 5.0 - (total_deviation / max(0.1, max_possible_penalty) * 5.0))

        present_phases = len([p for p, r in phase_distribution.items() if isinstance(r, (int,float)) and r > 0.01])
        diversity_score_comp = min(5.0, 1.0 + present_phases * diversity_bonus)

        focus_score_comp = 3.0
        if phase_focus != "balanced" and phase_focus in phase_distribution:
            actual_ratio = phase_distribution.get(phase_focus, 0.0)
            focus_score_comp = min(5.0, max(1.0, 3.0 + (actual_ratio - focus_target_ratio) * 5.0))

        if phase_focus != "balanced":
            final_score = (focus_score_comp * 0.5) + (diversity_score_comp * 0.3) + (balance_score_comp * 0.2)
        else:
            final_score = (balance_score_comp * 0.45) + (diversity_score_comp * 0.45) + (focus_score_comp * 0.1)
        return max(0.0, min(5.0, final_score))

    def _calculate_layer_balance_score(self, layer_distribution: JsonDict) -> float:
        """Calculates layer balance score based on distribution."""
        if not layer_distribution: return 2.0
        target_dist = self.config.LAYER_BALANCE_TARGETS
        tolerance = self.config.LAYER_DEVIATION_TOLERANCE
        penalty_factor = self.config.LAYER_BALANCE_PENALTY_FACTOR

        total_deviation_penalty = 0.0; valid_layers = 0
        for layer, target_ratio in target_dist.items():
            if layer in layer_distribution:
                valid_layers +=1
                actual_ratio = layer_distribution.get(layer, 0.0)
                deviation = abs(actual_ratio - target_ratio)
                penalty = max(0, deviation - tolerance) * penalty_factor
                total_deviation_penalty += penalty
        
        max_possible_penalty = penalty_factor * valid_layers if valid_layers > 0 else 1.0
        normalized_penalty = (total_deviation_penalty / max(0.1, max_possible_penalty)) * 5.0
        score = 5.0 - normalized_penalty
        return max(0.0, min(5.0, score))

    def _calculate_emotion_arc_score(self, text: str, emotion_arc_target: Optional[str]) -> float:
        """Calculates emotion arc score (v1.8 placeholder/heuristic)."""
        # For v1.8, this remains a placeholder or very basic heuristic.
        # True emotion arc tracking requires more sophisticated NLP.
        if emotion_arc_target:
            self.logger.debug(f"Emotion arc scoring: Target '{emotion_arc_target}' provided. Placeholder score 3.0.")
            # Could do a very simple check if start/end keywords appear, but likely unreliable.
            return 3.0
        else:
            self.logger.debug("Emotion arc scoring: No target specified. Placeholder score 3.0.")
            return 3.0

    def _calculate_colloquial_score(self, text: str, colloquial_target: str) -> float:
        """Calculates colloquial/gothic blend score (v1.8 placeholder/heuristic)."""
        self.logger.debug(f"Colloquial score: Target '{colloquial_target}'. Placeholder score 3.0.")
        # This is a complex linguistic task. For v1.8, a simple heuristic or placeholder.
        # Example: if target is 'high' and text contains many archaic forms, penalize.
        # If target is 'low' and text is very modern, penalize.
        # For now, returning a neutral score.
        score = 3.0
        # A very basic heuristic (can be expanded significantly)
        text_lower = text.lower()
        informal_markers = ["だよね", "ってか", "まじ", "ウケる", "みたいな"] # Example Japanese informal markers
        formal_markers = ["である。", "なり。", "御座います", "申し上げます", "〜奉る"] # Example formal/archaic

        informal_hits = sum(marker in text_lower for marker in informal_markers)
        formal_hits = sum(marker in text for marker in formal_markers) # Some formal markers are case-sensitive

        if colloquial_target == "high":
            if formal_hits > informal_hits: score -= 1.0 # Too formal
            elif informal_hits == 0 : score -= 0.5 # Not colloquial enough
        elif colloquial_target == "low":
            if informal_hits > formal_hits : score -= 1.0 # Too informal
            elif formal_hits == 0 : score -= 0.5 # Not formal enough
        elif colloquial_target == "medium":
            # Penalize if too skewed either way
            if informal_hits > formal_hits * 2 and formal_hits < 1 : score -= 0.5
            if formal_hits > informal_hits * 2 and informal_hits < 1 : score -= 0.5
        
        return max(1.0, min(4.0, score)) # Keep score within a reasonable range for a heuristic


    # process method and other helpers will be in subsequent parts
    # ... (Placeholder for process method and other TextProcessor logic)

# =============================================================================
# Part 11 End: TextProcessor Initialization and Core Evaluation Helper Methods
# =============================================================================
# =============================================================================
# Part 12: TextProcessor Main Logic (Process Method Skeleton & Loop 0)
# =============================================================================

class TextProcessor:  # Re-opening class for clarity
    # --- __init__ and core evaluation helpers from Part 11 ---
    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        ndgs_parser: Optional[Any] = None
    ):
        self.logger = logging.getLogger("NGGS-Lite.TextProcessor") # Ensure logger is specific
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TextProcessor requires a valid NGGSConfig instance.")
        # ... (rest of validations from Part 11)
        self.config = config
        self.llm = llm_client
        self.evaluator = evaluator
        self.vocab_manager = vocab_manager
        self.narrative_flow = narrative_flow
        self.eti_evaluator = eti_evaluator
        self.ri_evaluator = ri_evaluator
        self.subjective_evaluator = subjective_evaluator
        self.ndgs_parser = ndgs_parser

        gen_template_res = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_res.is_err:
            self.logger.critical(f"Invalid generation_template: {gen_template_res.error}")
            raise gen_template_res.error
        self.generation_template = generation_template

        imp_template_res = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_res.is_err:
            self.logger.critical(f"Invalid improvement_template: {imp_template_res.error}")
            raise imp_template_res.error
        self.improvement_template = improvement_template

        self.logger.info(
            f"TextProcessor initialized (v{config.VERSION}). Engine: {self.llm.current_engine}, "
            f"Max Loops: {config.DEFAULT_MAX_LOOPS}, Threshold: {config.DEFAULT_IMPROVEMENT_THRESHOLD:.1f}"
        )

    def _get_default_aggregated_scores(self) -> JsonDict:
        default_score = 0.0
        return {
            "overall_quality": default_score, "gothic_atmosphere": default_score,
            "stylistic_gravity": default_score, "indirect_emotion": default_score,
            "vocabulary_richness": default_score, "eti": default_score,
            "subjective_depth": default_score, "phase_transition": default_score,
            "colloquial_gothic_blend": default_score, "layer_balance": default_score,
            "emotion_arc_quality": default_score, "ri": default_score,
            "subjective": default_score, "phase": default_score, "layer": default_score,
            "emotion": default_score, "colloquial": default_score
        }

    # --- Evaluation Helper Methods from Part 11 ---
    def _perform_full_evaluation(
        self, text: str, phase_focus: str, colloquial_level: str, emotion_arc: Optional[str]
    ) -> Result[JsonDict, EvaluationError]:
        self.logger.info(f"完全評価を実行中 (テキスト長: {len(text)})...")
        full_results: JsonDict = {
            "llm_eval_result": None, "eti_result": None, "ri_result": None, "subjective_result": None,
            "phase_distribution": {}, "layer_distribution": {},
            "phase_score": 0.0, "layer_balance_score": 0.0, "emotion_arc_score": 0.0, "colloquial_score": 0.0,
            "aggregated_scores": {}, "errors": [], "llm_eval_success": False
        }
        aggregated_scores: Dict[str, float] = {}
        llm_eval_result_obj: Optional[EvaluationResult] = None

        llm_eval_result_obj = self._run_llm_eval(text, full_results, aggregated_scores)
        full_results["llm_eval_success"] = (
            llm_eval_result_obj is not None and
            isinstance(llm_eval_result_obj.scores, dict) and
            "overall_quality" in llm_eval_result_obj.scores and
            isinstance(llm_eval_result_obj.scores["overall_quality"], (int, float))
        )
        base_eval_context = llm_eval_result_obj if full_results["llm_eval_success"] else None

        self._run_eti_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_ri_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_subjective_eval(text, base_eval_context, full_results, aggregated_scores)
        phase_dist, layer_dist = self._run_distribution_analysis(text, full_results)
        self._calculate_and_store_derived_scores(
            text, phase_dist, layer_dist, phase_focus, colloquial_level, emotion_arc,
            full_results, aggregated_scores
        )
        full_results["aggregated_scores"] = {
            k: round(v, 2) for k, v in aggregated_scores.items() if isinstance(v, (int, float))
        }
        if full_results["errors"]: self.logger.warning(f"完全評価完了、ただし {len(full_results['errors'])} 件の回復可能エラーが発生。")
        else: self.logger.info("完全評価成功。")
        return Result.ok(full_results)

    def _run_llm_eval(self, text: str, full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> Optional[EvaluationResult]:
        self.logger.debug("基本LLM評価を実行中...")
        default_s, _ = self.evaluator._get_default_scores_and_reasons() # pylint: disable=protected-access
        eval_result_obj: Optional[EvaluationResult] = None
        try:
            eval_result_obj = self.evaluator.evaluate(text)
            full_results_dict["llm_eval_result"] = eval_result_obj
            if eval_result_obj.scores and "overall_quality" in eval_result_obj.scores:
                valid_scores = {k: float(v) for k,v in eval_result_obj.scores.items() if isinstance(v, (int, float))}
                agg_scores.update(valid_scores)
                self.logger.info("基本LLM評価成功。")
            else:
                msg = eval_result_obj.analysis or "基本LLM評価が有効なスコアを生成しませんでした。"
                full_results_dict["errors"].append(f"LLM Eval: {msg}")
                self.logger.error(f"基本LLM評価失敗または無効データ: {msg}")
                agg_scores.update(default_s)
        except Exception as e:
            crit_err_msg = f"基本LLM評価中の致命的エラー: {e}"
            eval_error = EvaluationError(crit_err_msg); self.logger.critical(str(eval_error), exc_info=True)
            full_results_dict["errors"].append(str(eval_error)); agg_scores.update(default_s)
            eval_result_obj = EvaluationResult(analysis=f"LLM評価エラー: {e}", scores=default_s.copy())
            full_results_dict["llm_eval_result"] = eval_result_obj
        return eval_result_obj

    def _run_eti_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("ETI評価を実行中..."); default_score_val = 2.5
        try:
            eti_res = self.eti_evaluator.evaluate(text, base_eval_ctx); full_results_dict["eti_result"] = eti_res
            main_score = eti_res.scores.get("eti_total_calculated")
            if isinstance(main_score, (int,float)): agg_scores["eti"] = float(main_score)
            else: agg_scores["eti"] = default_score_val; warn_msg="ETI評価が数値スコア 'eti_total_calculated' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"ETI Eval: {warn_msg}")
        except Exception as e:
            msg=f"ETI評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["eti_result"] = EvaluationResult(analysis=f"ETIエラー: {e}", scores={"eti_total_calculated": default_score_val}); agg_scores["eti"] = default_score_val

    def _run_ri_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("RI評価を実行中..."); default_score_val = 2.5
        try:
            ri_res = self.ri_evaluator.evaluate(text, base_eval_ctx); full_results_dict["ri_result"] = ri_res
            main_score = ri_res.scores.get("ri_total_calculated")
            if isinstance(main_score, (int,float)): agg_scores["ri"] = float(main_score)
            else: agg_scores["ri"] = default_score_val; warn_msg="RI評価が数値スコア 'ri_total_calculated' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"RI Eval: {warn_msg}")
        except Exception as e:
            msg=f"RI評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["ri_result"] = EvaluationResult(analysis=f"RIエラー: {e}", scores={"ri_total_calculated": default_score_val}); agg_scores["ri"] = default_score_val

    def _run_subjective_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("主観性評価を実行中..."); default_score_val = 2.5
        try:
            subj_res = self.subjective_evaluator.evaluate(text, base_eval_ctx); full_results_dict["subjective_result"] = subj_res
            main_score = subj_res.scores.get("subjective_score")
            if isinstance(main_score, (int,float)): agg_scores["subjective"] = float(main_score)
            else: agg_scores["subjective"] = default_score_val; warn_msg="主観性評価が数値スコア 'subjective_score' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"Subjective Eval: {warn_msg}")
        except Exception as e:
            msg=f"主観性評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["subjective_result"] = EvaluationResult(analysis=f"主観性エラー: {e}", scores={"subjective_score": default_score_val}); agg_scores["subjective"] = default_score_val

    def _run_distribution_analysis(self, text: str, full_results_dict: JsonDict) -> Tuple[JsonDict, JsonDict]:
        self.logger.debug("分布分析 (位相・層) を実行中..."); phase_dist: JsonDict = {}; layer_dist: JsonDict = {}
        if self.narrative_flow:
            try:
                phase_dist = self.narrative_flow.analyze_phase_distribution(text); full_results_dict["phase_distribution"] = phase_dist; self.logger.debug(f"位相分布: {phase_dist}")
            except Exception as e: msg=f"位相分布分析失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg); full_results_dict["phase_distribution"] = {}
            try:
                layer_dist = self.narrative_flow.analyze_layer_distribution(text); full_results_dict["layer_distribution"] = layer_dist; self.logger.debug(f"層分布: {layer_dist}")
            except Exception as e: msg=f"層分布分析失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg); full_results_dict["layer_distribution"] = {}
        else:
            msg="NarrativeFlowFrameworkが利用不可のため分布分析をスキップ。"; self.logger.warning(msg); full_results_dict["errors"].append(msg)
            full_results_dict["phase_distribution"] = {}; full_results_dict["layer_distribution"] = {}
        return phase_dist, layer_dist

    def _calculate_and_store_derived_scores(
        self, text: str, phase_dist: JsonDict, layer_dist: JsonDict, phase_focus: str,
        colloquial_level: str, emotion_arc: Optional[str],
        full_results_dict: JsonDict, agg_scores: Dict[str, float]
    ) -> None:
        self.logger.debug("派生スコア (位相、層、感情、口語) を計算中...")
        def _safe_calculate(func: Callable[..., float], *args: Any, score_name: str) -> float:
            default_score_val = 2.5
            try:
                score = func(*args)
                if not isinstance(score, (int,float)): self.logger.warning(f"{score_name} calculation non-numeric: {type(score)}. Defaulting."); full_results_dict["errors"].append(f"{score_name} Score Error: Invalid type {type(score)}"); return default_score_val
                return max(0.0, min(5.0, float(score)))
            except Exception as e: msg=f"{score_name} スコアリング失敗: {e}"; self.logger.warning(msg, exc_info=False); full_results_dict["errors"].append(msg); return default_score_val
        phase_score = _safe_calculate(self._calculate_phase_score, phase_dist, phase_focus, score_name="Phase")
        layer_score = _safe_calculate(self._calculate_layer_balance_score, layer_dist, score_name="Layer Balance")
        emotion_score = _safe_calculate(self._calculate_emotion_arc_score, text, emotion_arc, score_name="Emotion Arc")
        colloquial_score = _safe_calculate(self._calculate_colloquial_score, text, colloquial_level, score_name="Colloquial")
        full_results_dict["phase_score"] = round(phase_score,2); full_results_dict["layer_balance_score"] = round(layer_score,2)
        full_results_dict["emotion_arc_score"] = round(emotion_score,2); full_results_dict["colloquial_score"] = round(colloquial_score,2)
        agg_scores["phase"] = full_results_dict["phase_score"]; agg_scores["layer"] = full_results_dict["layer_balance_score"]
        agg_scores["emotion"] = full_results_dict["emotion_arc_score"]; agg_scores["colloquial"] = full_results_dict["colloquial_score"]
        self.logger.debug(f"派生スコア: Phase={phase_score:.1f}, Layer={layer_score:.1f}, Emotion={emotion_score:.1f}, Colloquial={colloquial_score:.1f}")

    def _calculate_phase_score(self, phase_distribution: JsonDict, phase_focus: str) -> float:
        if not phase_distribution: return 2.0
        target_dist=self.config.PHASE_BALANCE_TARGETS; tolerance=self.config.PHASE_DEVIATION_TOLERANCE
        focus_target_ratio=self.config.PHASE_SCORE_FOCUS_TARGET_RATIO; penalty_factor=self.config.PHASE_SCORE_BALANCE_PENALTY_FACTOR
        diversity_bonus=self.config.PHASE_SCORE_DIVERSITY_BONUS
        total_dev=0.0; valid_ph=0
        for ph,t_ratio in target_dist.items():
            if ph in phase_distribution:
                dev=abs(phase_distribution.get(ph,0.0)-t_ratio); pen=max(0,dev-tolerance)*penalty_factor; total_dev+=pen; valid_ph+=1
        max_pen=penalty_factor*valid_ph if valid_ph>0 else 1.0; bal_score=max(0.0,5.0-(total_dev/max(0.1,max_pen)*5.0))
        present_ph=len([p for p,r in phase_distribution.items() if isinstance(r,(int,float)) and r>0.01]); div_score=min(5.0,1.0+present_ph*diversity_bonus)
        foc_score=3.0
        if phase_focus!="balanced" and phase_focus in phase_distribution:
            act_ratio=phase_distribution.get(phase_focus,0.0); foc_score=min(5.0,max(1.0,3.0+(act_ratio-focus_target_ratio)*5.0))
        if phase_focus!="balanced": final_s=(foc_score*0.5)+(div_score*0.3)+(bal_score*0.2)
        else: final_s=(bal_score*0.45)+(div_score*0.45)+(foc_score*0.1)
        return max(0.0,min(5.0,final_s))

    def _calculate_layer_balance_score(self, layer_distribution: JsonDict) -> float:
        if not layer_distribution: return 2.0
        target_dist=self.config.LAYER_BALANCE_TARGETS; tolerance=self.config.LAYER_DEVIATION_TOLERANCE
        penalty_factor=self.config.LAYER_BALANCE_PENALTY_FACTOR
        total_dev_pen=0.0; valid_lay=0
        for lay,t_ratio in target_dist.items():
            if lay in layer_distribution:
                valid_lay+=1; act_ratio=layer_distribution.get(lay,0.0); dev=abs(act_ratio-t_ratio)
                pen=max(0,dev-tolerance)*penalty_factor; total_dev_pen+=pen
        max_poss_pen=penalty_factor*valid_lay if valid_lay>0 else 1.0; norm_pen=(total_dev_pen/max(0.1,max_poss_pen))*5.0
        score=5.0-norm_pen; return max(0.0,min(5.0,score))

    def _calculate_emotion_arc_score(self, text: str, emotion_arc_target: Optional[str]) -> float:
        if emotion_arc_target: self.logger.debug(f"Emotion arc: Target '{emotion_arc_target}'. Placeholder 3.0."); return 3.0
        else: self.logger.debug("Emotion arc: No target. Placeholder 3.0."); return 3.0

    def _calculate_colloquial_score(self, text: str, colloquial_target: str) -> float:
        self.logger.debug(f"Colloquial score: Target '{colloquial_target}'. Placeholder 3.0."); score=3.0
        text_l=text.lower(); informal_m=["だよね","ってか","まじ","ウケる","みたいな"]; formal_m=["である。","なり。","御座います","申し上げます","〜奉る"]
        informal_h=sum(m in text_l for m in informal_m); formal_h=sum(m in text for m in formal_m)
        if colloquial_target=="high":
            if formal_h>informal_h: score-=1.0
            elif informal_h==0: score-=0.5
        elif colloquial_target=="low":
            if informal_h>formal_h: score-=1.0
            elif formal_h==0: score-=0.5
        elif colloquial_target=="medium":
            if informal_h>formal_h*2 and formal_h<1:score-=0.5
            if formal_h>informal_h*2 and informal_h<1:score-=0.5
        return max(1.0,min(4.0,score))
    
    # --- Stub for _generate_text, to be fully implemented later ---
    def _generate_text(
        self, input_text: str, target_length: int, perspective_mode: str,
        phase_focus: str, colloquial_level: str, emotion_arc: Optional[str],
        narrative_flow: Optional[str], improvement_instructions: Optional[str],
        temperature: Optional[float]
    ) -> Result[str, LLMError]:
        """
        Core text generation method. Formats prompt and calls LLM.
        (Full implementation in a later Part, this is a functional stub for Loop 0)
        """
        self.logger.info(f"Core text generation called (stub in Part 12). Type: {'Improvement' if improvement_instructions else 'Initial'}")
        
        # Simplified prompt formatting for the stub
        prompt_content_parts = [f"# Input Context:\n{input_text}"]
        if narrative_flow:
            prompt_content_parts.append(f"\n# Narrative Flow Guidance:\n{narrative_flow}")
        if improvement_instructions:
            prompt_content_parts.append(f"\n# Improvement Instructions:\n{improvement_instructions}")
        
        final_prompt = "\n\n".join(prompt_content_parts)
        final_prompt += f"\n\n# Generation Task:\nGenerate text (approx. {target_length} chars) "
        final_prompt += f"with perspective '{perspective_mode}', phase focus '{phase_focus}', "
        final_prompt += f"colloquial level '{colloquial_level}'."
        if emotion_arc:
            final_prompt += f" Follow emotion arc: '{emotion_arc}'."

        # Use a default temperature if none is provided (e.g., for initial generation)
        temp_to_use = temperature if temperature is not None else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)

        # Call LLM (actual call)
        llm_result = self.llm.generate(final_prompt, temperature=temp_to_use)

        if llm_result.is_ok:
            generated_text = llm_result.unwrap()
            # Simulate basic length control for the stub
            if len(generated_text) > target_length * 1.5: # If too long, truncate
                generated_text = generated_text[:int(target_length * 1.2)]
            elif len(generated_text) < target_length * 0.5 and not improvement_instructions: # If too short and initial
                 generated_text += "\n(追加のテキストが生成されました...これはスタブからの補足です)" * (target_length // 100)
            self.logger.debug(f"Stub _generate_text produced text of length {len(generated_text)}")
            return Result.ok(generated_text)
        else:
            self.logger.error(f"Stub _generate_text failed: {llm_result.error}")
            return Result.fail(llm_result.error) # Propagate LLMError

    # --- _perform_initial_generation from Part 11, now calling the stubbed _generate_text ---
    def _perform_initial_generation(
        self, initial_text: str, target_length: int, perspective_mode: str,
        phase_focus: str, colloquial_level: str, emotion_arc: Optional[str],
        narrative_flow_prompt: Optional[str]
    ) -> Result[str, LLMError]:
        self.logger.info("初期テキスト生成を実行中...")
        return self._generate_text(
            input_text=initial_text,
            target_length=target_length,
            perspective_mode=perspective_mode,
            phase_focus=phase_focus,
            colloquial_level=colloquial_level,
            emotion_arc=emotion_arc,
            narrative_flow=narrative_flow_prompt,
            improvement_instructions=None, # None for initial generation
            temperature=None # Use default generation temperature
        )

    # --- Main process method ---
    def process(
        self,
        initial_text: str,
        target_length: Optional[int] = None,
        perspective_mode: str = "subjective_first_person",
        phase_focus: str = "balanced",
        colloquial_level: str = "medium",
        emotion_arc: Optional[str] = None,
        max_loops_override: Optional[int] = None,
        threshold_override: Optional[float] = None,
        narrative_flow_prompt: Optional[str] = None,
        skip_initial_generation: bool = False,
        ndgs_input_data: Optional[JsonDict] = None
    ) -> Result[JsonDict, NGGSError]:
        """
        Processes text through generation and iterative improvement loops (v1.8).
        """
        # --- Parameter Validation and Setup ---
        max_loops = max_loops_override if max_loops_override is not None else self.config.DEFAULT_MAX_LOOPS
        improvement_threshold = threshold_override if threshold_override is not None else self.config.DEFAULT_IMPROVEMENT_THRESHOLD
        final_target_length = target_length if target_length is not None else self.config.DEFAULT_TARGET_LENGTH

        if not isinstance(initial_text, str) and not (skip_initial_generation and ndgs_input_data):
             # If initial_text is not a string, AND we are not skipping generation with NDGS data, it's an error.
             # If ndgs_input_data is present, initial_text might be derived from it later.
             if not ndgs_input_data:
                 return Result.fail(ConfigurationError("初期テキストは文字列である必要があり、提供されていません。"))
        
        max_loops = max(0, min(max_loops, 10)) # Allow up to 10 loops, 0 for eval only
        improvement_threshold = max(0.0, min(5.0, improvement_threshold))

        self.logger.info(
            f"テキスト処理開始。最大ループ:{max_loops},目標長:{final_target_length},改善閾値:{improvement_threshold:.1f},"
            f"視点:{perspective_mode},位相焦点:{phase_focus},口語レベル:{colloquial_level},"
            f"感情弧:{emotion_arc or '指定なし'},初期生成スキップ:{skip_initial_generation},"
            f"NDGS入力:{'あり' if ndgs_input_data else 'なし'}"
        )

        # --- Initialize results dictionary ---
        results_dict: JsonDict = {
            "job_id": self.config.generate_job_id(),
            "start_time": datetime.now(timezone.utc).isoformat(),
            "original_text_provided": initial_text if isinstance(initial_text, str) else "(NDGSデータから抽出予定)",
            "parameters": {
                "target_length": final_target_length, "perspective_mode": perspective_mode,
                "phase_focus": phase_focus, "colloquial_level": colloquial_level,
                "emotion_arc": emotion_arc, "max_loops": max_loops,
                "improvement_threshold": improvement_threshold,
                "skip_initial_generation": skip_initial_generation,
                "ndgs_input_provided": bool(ndgs_input_data),
                "llm_engine": self.llm.current_engine,
                "llm_model": self.llm._get_model_name()
            },
            "versions": [], "final_text": "", "best_loop_index": -1,
            "final_scores": {}, "distributions": {"phase": {}, "layer": {}},
            "html_report": "", "status": "Processing", "errors": []
        }

        # --- Process Optional NDGS Input ---
        current_initial_text = initial_text if isinstance(initial_text, str) else ""
        if ndgs_input_data and self.ndgs_parser:
            self.logger.info("NDGS入力データを解析中...")
            try:
                # NDGSIntegration.parse is expected to return a dict that might override parameters
                # and provide 'initial_text' if it wasn't given directly.
                # For now, assume it might return a dict with an 'initial_text' key.
                parsed_ndgs_context = self.ndgs_parser.parse(ndgs_input_data) # type: ignore
                if parsed_ndgs_context.is_ok:
                    ndgs_content = parsed_ndgs_context.unwrap()
                    current_initial_text = ndgs_content.get("initial_text", current_initial_text)
                    # Potentially override other parameters like perspective_mode, etc.
                    perspective_mode = ndgs_content.get("perspective_mode", perspective_mode)
                    # ... add other parameter overrides from ndgs_content if schema supports ...
                    results_dict["original_text_provided"] = current_initial_text # Update with text from NDGS if used
                    self.logger.info("NDGSデータ解析完了。パラメータが更新された可能性があります。")
                else:
                    err_msg = f"NDGS入力データの解析失敗: {parsed_ndgs_context.error}"
                    self.logger.error(err_msg)
                    results_dict["errors"].append(err_msg) # Log error but try to continue
            except Exception as e:
                err_msg = f"NDGS入力データの処理中に予期せぬエラー: {e}"
                self.logger.error(err_msg, exc_info=True)
                results_dict["errors"].append(err_msg)
        
        if not current_initial_text and not skip_initial_generation:
            return Result.fail(ConfigurationError("処理可能な初期テキストが見つかりませんでした (NDGSデータからも抽出不可)。"))


        # --- Initialize Loop Variables ---
        # current_text_for_loop is the text being worked on in the current iteration
        current_text_for_loop = current_initial_text if skip_initial_generation else ""
        best_text = current_text_for_loop
        best_full_eval_data: JsonDict = {} # Stores the full_eval_data of the best version
        processing_error_occurred_this_loop = False # Flag for errors within a single loop

        # --- Generate Narrative Flow Prompt (if needed) ---
        final_narrative_flow_prompt = narrative_flow_prompt # Use provided, or generate
        if final_narrative_flow_prompt is None and self.narrative_flow:
            try:
                theme = "記憶回帰型" # Default theme, could be parameterized
                final_narrative_flow_prompt = self.narrative_flow.generate_flow_template(
                    theme=theme, emotion_arc=emotion_arc, perspective=perspective_mode
                )
                self.logger.debug(f"生成された物語構成プロンプト: {truncate_text(final_narrative_flow_prompt, 100)}")
            except Exception as e:
                err_msg = f"物語構成プロンプトの生成失敗: {e}"
                self.logger.error(err_msg, exc_info=True)
                results_dict["errors"].append(err_msg)
                final_narrative_flow_prompt = None # Continue without it

        # --- Main Processing Loop ---
        try:
            # === LOOP 0: INITIAL GENERATION or EVALUATION ===
            loop_idx = 0
            version_data: JsonDict = {"loop": loop_idx, "status": "Pending"}
            results_dict["versions"].append(version_data)
            self.logger.info(f"--- 開始 ループ {loop_idx} ({'初期評価' if skip_initial_generation else '初期生成'}) ---")
            start_loop_time = time.monotonic()
            processing_error_occurred_this_loop = False

            if skip_initial_generation:
                self.logger.info("初期生成スキップ、提供されたテキストを評価します。")
                current_text_for_loop = current_initial_text
                version_data["text_source"] = "provided_initial_text"
                version_data["text"] = current_text_for_loop # Store text evaluated
            else:
                # Perform initial generation
                version_data["text_source"] = "initial_generation"
                generation_result = self._perform_initial_generation(
                    initial_text=current_initial_text, # Context for generation
                    target_length=final_target_length,
                    perspective_mode=perspective_mode,
                    phase_focus=phase_focus,
                    colloquial_level=colloquial_level,
                    emotion_arc=emotion_arc,
                    narrative_flow_prompt=final_narrative_flow_prompt
                )
                if generation_result.is_err:
                    if not self._handle_error(generation_result.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                        self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（生成）")
                        return Result.fail(generation_result.error) # This indicates a critical, unrecoverable error
                    processing_error_occurred_this_loop = True
                    current_text_for_loop = "" # No text to evaluate
                    version_data["status"] = "失敗 (生成)"
                else:
                    current_text_for_loop = generation_result.unwrap()
                    version_data["text"] = current_text_for_loop

            # Evaluate (either provided text or newly generated text)
            if not processing_error_occurred_this_loop: # Only evaluate if generation (if any) succeeded
                eval_res = self._perform_full_evaluation(
                    text=current_text_for_loop,
                    phase_focus=phase_focus,
                    colloquial_level=colloquial_level,
                    emotion_arc=emotion_arc
                )
                if eval_res.is_err: # This implies _perform_full_evaluation itself had a critical internal error
                    if not self._handle_error(eval_res.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                        self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（評価）")
                        return Result.fail(eval_res.error)
                    processing_error_occurred_this_loop = True
                    version_data["status"] = "失敗 (評価フレームワークエラー)"
                else:
                    full_eval_data_loop0 = eval_res.unwrap()
                    version_data.update(full_eval_data_loop0) # Store all eval data for this version
                    if full_eval_data_loop0.get("llm_eval_success", False):
                        best_text = current_text_for_loop
                        best_full_eval_data = full_eval_data_loop0
                        results_dict["best_loop_index"] = loop_idx
                        version_data["status"] = "成功"
                    else: # LLM eval part within _perform_full_evaluation failed
                        processing_error_occurred_this_loop = True
                        version_data["status"] = "失敗 (LLM評価)"
            
            # If status is still pending (e.g. gen failed, eval skipped), update it
            if version_data["status"] == "Pending":
                version_data["status"] = "失敗 (不定)" # Generic failure if not set by specific step
            if processing_error_occurred_this_loop and "失敗" not in version_data["status"]:
                 version_data["status"] = "完了（回復可能エラーあり）"


            loop_duration = time.monotonic() - start_loop_time
            self.logger.info(f"--- 完了 ループ {loop_idx} ({version_data.get('status', '?')}) - {loop_duration:.2f} 秒 ---")

            # --- Check if loop should terminate after initial step ---
            current_best_score = best_full_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0)
            
            # Terminate if:
            # 1. Max loops is 0 (eval only mode)
            # 2. A critical (non-recoverable) error occurred during this loop (indicated by status)
            # 3. Score threshold met *and* it was the first loop (best_loop_index is 0)
            #    and MIN_FEEDBACK_LOOPS requirement is met. (If MIN_FEEDBACK_LOOPS is 0 or 1 for initial check)
            min_loops_for_threshold_check = self.config.MIN_FEEDBACK_LOOPS if self.config.MIN_FEEDBACK_LOOPS > 0 else 1

            should_terminate_early = (
                max_loops == 0 or
                ("失敗" in version_data.get("status", "") and not version_data.get("status", "").startswith("失敗 (LLM評価)")) or # Critical failure in loop 0
                (current_best_score >= improvement_threshold and results_dict["best_loop_index"] == 0 and loop_idx +1 >= min_loops_for_threshold_check )
            )

            if should_terminate_early:
                final_status_msg = version_data.get("status", "完了")
                if results_dict["best_loop_index"] == -1 and "失敗" not in final_status_msg : final_status_msg = "失敗 (有効な結果なし)"
                elif "失敗" in final_status_msg: final_status_msg = "エラーで終了" # Already contains failure reason
                elif current_best_score >= improvement_threshold and max_loops > 0 : final_status_msg = "完了 (閾値達成)"
                elif max_loops == 0 : final_status_msg = "完了 (初期評価のみ)"

                self.logger.info(f"ループ0完了後、処理終了。Status: {final_status_msg}")
                self._finalize_results(results_dict, best_text, best_full_eval_data, final_status_msg)
                return Result.ok(results_dict)

            # --- Improvement Loops (Skeleton for now, detailed in Part 13) ---
            for improvement_loop_idx in range(1, max_loops + 1):
                loop_idx = improvement_loop_idx # Actual loop number for versions array
                version_data = {"loop": loop_idx, "status": "Pending"}
                results_dict["versions"].append(version_data)
                self.logger.info(f"--- 開始 ループ {loop_idx} (改善) ---")
                start_loop_time = time.monotonic()
                processing_error_occurred_this_loop = False

                current_best_score = best_full_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0)
                if not best_full_eval_data or not best_text: # Check if there's a valid base for improvement
                    version_data["status"] = "スキップ (有効な前結果なし)"
                    self.logger.warning(f"ループ {loop_idx}: スキップ。前ループの有効なテキスト/評価がありません。改善ループを停止します。")
                    results_dict["errors"].append(f"Loop {loop_idx}: Skipped due to no valid prior result.")
                    break

                if current_best_score >= improvement_threshold and loop_idx >= min_loops_for_threshold_check:
                    version_data["status"] = f"スキップ (閾値達成: {current_best_score:.2f})"
                    self.logger.info(f"ループ {loop_idx}: 早期終了。スコア {current_best_score:.2f} >= 閾値 {improvement_threshold:.2f}")
                    break

                # --- Placeholder for _perform_improvement_loop ---
                # loop_result = self._perform_improvement_loop(...)
                # For Part 12, we'll simulate a loop pass without actual improvement.
                self.logger.info(f"ループ {loop_idx}: _perform_improvement_loop 呼び出し (Part 13で実装予定)")
                version_data["text"] = f"Placeholder text from improvement loop {loop_idx}"
                # Simulate an evaluation of this placeholder text
                placeholder_eval_res = self._perform_full_evaluation(
                    text=version_data["text"],
                    phase_focus=phase_focus, colloquial_level=colloquial_level, emotion_arc=emotion_arc
                )
                if placeholder_eval_res.is_err:
                     if not self._handle_error(placeholder_eval_res.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                        self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（改善ループ評価）")
                        return Result.fail(placeholder_eval_res.error)
                     processing_error_occurred_this_loop = True
                else:
                    new_eval_data = placeholder_eval_res.unwrap()
                    version_data.update(new_eval_data)
                    # Logic to update best_text and best_full_eval_data would go here
                    # based on new_eval_data, but that's for Part 13.
                    if new_eval_data.get("llm_eval_success", False):
                        version_data["status"] = "成功 (プレースホルダ)"
                    else:
                        processing_error_occurred_this_loop = True
                        version_data["status"] = "失敗 (LLM評価 - プレースホルダ)"
                
                if version_data["status"] == "Pending": version_data["status"] = "失敗 (不定 - プレースホルダ)"

                loop_duration = time.monotonic() - start_loop_time
                self.logger.info(f"--- 完了 ループ {loop_idx} ({version_data.get('status', '?')} - プレースホルダ) - {loop_duration:.2f} 秒 ---")
                
                if processing_error_occurred_this_loop and "失敗" in version_data.get("status", ""):
                    self.logger.warning(f"ループ {loop_idx} はエラーで終了しました。改善ループを停止します。")
                    break


            # === Loop End / Finalization ===
            final_status_msg = "完了"
            if results_dict["errors"] : final_status_msg = "エラーあり完了"
            if results_dict["best_loop_index"] == -1: final_status_msg = "失敗 (有効な結果なし)"
            elif current_best_score >= improvement_threshold and results_dict["best_loop_index"] >=0 : final_status_msg = "完了 (閾値達成)"
            elif loop_idx >= max_loops : final_status_msg = "完了 (最大ループ到達)" # loop_idx will be last attempted loop

            self.logger.info(f"全ループ処理完了。 Status: {final_status_msg}")
            self._finalize_results(results_dict, best_text, best_full_eval_data, final_status_msg)
            return Result.ok(results_dict)

        except Exception as e:
            # Catch unexpected critical errors during the process flow
            error_type_name = type(e).__name__
            critical_err_msg = f"TextProcessor.processで予期せぬクリティカルエラー: {error_type_name} - {e}"
            self.logger.critical(critical_err_msg, exc_info=True)
            results_dict["status"] = "致命的エラー"
            results_dict["errors"].append(critical_err_msg)
            self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー")
            return Result.fail(NGGSError(critical_err_msg, details={"exception_type": error_type_name}))

    def _handle_error(
        self,
        error: Exception,
        loop_idx: int,
        results_dict: JsonDict,
        version_data: JsonDict,
        is_fatal_loop: bool = False # If true, this loop cannot continue
    ) -> bool:
        """
        Handles errors, logs them, updates version_data and results_dict.
        Returns True if processing can potentially continue (e.g., for recoverable errors
        or loop-level failures that don't stop the whole batch), False if it's fatal.
        """
        error_type = type(error)
        # Determine severity: if is_fatal_loop, treat as LOOP unless already FATAL.
        # Otherwise, use the map.
        severity: ErrorSeverity
        if is_fatal_loop:
            base_severity = ERROR_SEVERITY_MAP.get(error_type, ErrorSeverity.LOOP) # Default to LOOP for loop-fatal
            severity = ErrorSeverity.FATAL if base_severity == ErrorSeverity.FATAL else ErrorSeverity.LOOP
        else:
            severity = ERROR_SEVERITY_MAP.get(error_type, ErrorSeverity.FATAL)


        error_msg = f"Loop {loop_idx}: [{severity.name}] {error_type.__name__} - {str(error)}"
        log_func = self.logger.critical if severity == ErrorSeverity.FATAL else \
                   self.logger.error if severity == ErrorSeverity.LOOP else \
                   self.logger.warning

        # Determine if full traceback is needed
        log_exc_info = severity == ErrorSeverity.FATAL
        is_safety_block = isinstance(error, ValueError) and ("コンテンツ生成ブロック" in str(error) or "SAFETY" in str(error).upper())
        if is_safety_block: log_exc_info = False # Don't need full trace for safety blocks

        log_func(error_msg, exc_info=log_exc_info)

        version_data["status"] = f"失敗 ({severity.name})"
        version_data["error_details"] = f"[{severity.name}] {error_type.__name__}: {str(error)}"
        if isinstance(error, NGGSError) and error.get_context():
            version_data["error_details"] += f" Context: {error.get_context()}"
        
        # Add concise error summary to global list if not already present
        global_err_summary = f"Loop {loop_idx}: [{severity.name}] {error_type.__name__}"
        if global_err_summary not in results_dict["errors"]:
            results_dict["errors"].append(global_err_summary)
        
        if severity == ErrorSeverity.FATAL:
            return False # Cannot continue
        return True # Can continue (LOOP or RECOVERABLE)

    def _finalize_results(
        self,
        results_dict: JsonDict,
        best_text: str,
        best_full_eval_data: JsonDict, # This is the full eval dict for the best version
        final_status: str
    ) -> None:
        """Finalizes the results dictionary with best data and generates report (stub)."""
        self.logger.info(f"結果を最終化中... Status: {final_status}")
        results_dict["final_text"] = best_text if best_text else "(有効な生成テキストなし)"
        results_dict["best_loop_index"] = best_full_eval_data.get("loop", results_dict.get("best_loop_index", -1))
        
        agg_scores = best_full_eval_data.get("aggregated_scores", {})
        results_dict["final_scores"] = {
            k: round(v, 2) for k, v in agg_scores.items() if isinstance(v, (int, float))
        }
        
        phase_dist = best_full_eval_data.get("phase_distribution")
        layer_dist = best_full_eval_data.get("layer_distribution")
        results_dict["distributions"] = {
            "phase": phase_dist if isinstance(phase_dist, dict) else {},
            "layer": layer_dist if isinstance(layer_dist, dict) else {}
        }
        
        results_dict["completion_time"] = datetime.now(timezone.utc).isoformat()
        
        # Set final status, but don't overwrite a more severe "致命的エラー" status
        if "致命的エラー" not in results_dict.get("status", ""):
            results_dict["status"] = final_status
        
        self.logger.info(f"_finalize_results stub called. HTML report generation will be in a later part.")
        # HTML report generation will be called here in a later part:
        # results_dict["html_report"] = self._generate_html_report(results_dict)

# =============================================================================
# Part 12 End: TextProcessor Main Logic (Process Method Skeleton & Loop 0)
# =============================================================================
# =============================================================================
# Part 13: TextProcessor Improvement Loop and Instruction Generation
# =============================================================================

class TextProcessor:  # Re-opening class for clarity
    # --- __init__ and other methods from Part 11 & 12 ---
    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        ndgs_parser: Optional[Any] = None
    ):
        self.logger = logging.getLogger("NGGS-Lite.TextProcessor")
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TextProcessor requires a valid NGGSConfig instance.")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TextProcessor requires a valid LLMClient instance.")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TextProcessor requires a valid base Evaluator instance.")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TextProcessor requires a valid VocabularyManager instance.")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TextProcessor requires a valid NarrativeFlowFramework instance.")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TextProcessor requires a valid ExtendedETIEvaluator instance.")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TextProcessor requires a valid RIEvaluator instance.")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TextProcessor requires a valid SubjectiveEvaluator instance.")
        if ndgs_parser is not None and not hasattr(ndgs_parser, 'parse'):
            self.logger.warning("Provided ndgs_parser object may lack a 'parse' method.")

        self.config = config
        self.llm = llm_client
        self.evaluator = evaluator
        self.vocab_manager = vocab_manager
        self.narrative_flow = narrative_flow
        self.eti_evaluator = eti_evaluator
        self.ri_evaluator = ri_evaluator
        self.subjective_evaluator = subjective_evaluator
        self.ndgs_parser = ndgs_parser

        gen_template_res = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_res.is_err:
            self.logger.critical(f"Invalid generation_template: {gen_template_res.error}")
            raise gen_template_res.error
        self.generation_template = generation_template

        imp_template_res = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_res.is_err:
            self.logger.critical(f"Invalid improvement_template: {imp_template_res.error}")
            raise imp_template_res.error
        self.improvement_template = improvement_template

        self.logger.info(
            f"TextProcessor initialized (v{config.VERSION}). Engine: {self.llm.current_engine}, "
            f"Max Loops: {config.DEFAULT_MAX_LOOPS}, Threshold: {config.DEFAULT_IMPROVEMENT_THRESHOLD:.1f}"
        )

    def _get_default_aggregated_scores(self) -> JsonDict:
        default_score = 0.0
        return {
            "overall_quality": default_score, "gothic_atmosphere": default_score,
            "stylistic_gravity": default_score, "indirect_emotion": default_score,
            "vocabulary_richness": default_score, "eti": default_score,
            "subjective_depth": default_score, "phase_transition": default_score,
            "colloquial_gothic_blend": default_score, "layer_balance": default_score,
            "emotion_arc_quality": default_score, "ri": default_score,
            "subjective": default_score, "phase": default_score, "layer": default_score,
            "emotion": default_score, "colloquial": default_score
        }

    def _perform_full_evaluation(
        self, text: str, phase_focus: str, colloquial_level: str, emotion_arc: Optional[str]
    ) -> Result[JsonDict, EvaluationError]:
        self.logger.info(f"完全評価を実行中 (テキスト長: {len(text)})...")
        full_results: JsonDict = {
            "llm_eval_result": None, "eti_result": None, "ri_result": None, "subjective_result": None,
            "phase_distribution": {}, "layer_distribution": {},
            "phase_score": 0.0, "layer_balance_score": 0.0, "emotion_arc_score": 0.0, "colloquial_score": 0.0,
            "aggregated_scores": {}, "errors": [], "llm_eval_success": False
        }
        aggregated_scores: Dict[str, float] = {}
        llm_eval_result_obj: Optional[EvaluationResult] = None

        llm_eval_result_obj = self._run_llm_eval(text, full_results, aggregated_scores)
        full_results["llm_eval_success"] = (
            llm_eval_result_obj is not None and
            isinstance(llm_eval_result_obj.scores, dict) and
            "overall_quality" in llm_eval_result_obj.scores and
            isinstance(llm_eval_result_obj.scores["overall_quality"], (int, float))
        )
        base_eval_context = llm_eval_result_obj if full_results["llm_eval_success"] else None

        self._run_eti_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_ri_eval(text, base_eval_context, full_results, aggregated_scores)
        self._run_subjective_eval(text, base_eval_context, full_results, aggregated_scores)
        phase_dist, layer_dist = self._run_distribution_analysis(text, full_results)
        self._calculate_and_store_derived_scores(
            text, phase_dist, layer_dist, phase_focus, colloquial_level, emotion_arc,
            full_results, aggregated_scores
        )
        full_results["aggregated_scores"] = {
            k: round(v, 2) for k, v in aggregated_scores.items() if isinstance(v, (int, float))
        }
        if full_results["errors"]: self.logger.warning(f"完全評価完了、ただし {len(full_results['errors'])} 件の回復可能エラーが発生。")
        else: self.logger.info("完全評価成功。")
        return Result.ok(full_results)

    def _run_llm_eval(self, text: str, full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> Optional[EvaluationResult]:
        self.logger.debug("基本LLM評価を実行中...")
        default_s, _ = self.evaluator._get_default_scores_and_reasons() # pylint: disable=protected-access
        eval_result_obj: Optional[EvaluationResult] = None
        try:
            eval_result_obj = self.evaluator.evaluate(text)
            full_results_dict["llm_eval_result"] = eval_result_obj
            if eval_result_obj.scores and "overall_quality" in eval_result_obj.scores:
                valid_scores = {k: float(v) for k,v in eval_result_obj.scores.items() if isinstance(v, (int, float))}
                agg_scores.update(valid_scores)
                self.logger.info("基本LLM評価成功。")
            else:
                msg = eval_result_obj.analysis or "基本LLM評価が有効なスコアを生成しませんでした。"
                full_results_dict["errors"].append(f"LLM Eval: {msg}")
                self.logger.error(f"基本LLM評価失敗または無効データ: {msg}")
                agg_scores.update(default_s)
        except Exception as e:
            crit_err_msg = f"基本LLM評価中の致命的エラー: {e}"
            eval_error = EvaluationError(crit_err_msg); self.logger.critical(str(eval_error), exc_info=True)
            full_results_dict["errors"].append(str(eval_error)); agg_scores.update(default_s)
            eval_result_obj = EvaluationResult(analysis=f"LLM評価エラー: {e}", scores=default_s.copy())
            full_results_dict["llm_eval_result"] = eval_result_obj
        return eval_result_obj

    def _run_eti_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("ETI評価を実行中..."); default_score_val = 2.5
        try:
            eti_res = self.eti_evaluator.evaluate(text, base_eval_ctx); full_results_dict["eti_result"] = eti_res
            main_score = eti_res.scores.get("eti_total_calculated")
            if isinstance(main_score, (int,float)): agg_scores["eti"] = float(main_score)
            else: agg_scores["eti"] = default_score_val; warn_msg="ETI評価が数値スコア 'eti_total_calculated' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"ETI Eval: {warn_msg}")
        except Exception as e:
            msg=f"ETI評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["eti_result"] = EvaluationResult(analysis=f"ETIエラー: {e}", scores={"eti_total_calculated": default_score_val}); agg_scores["eti"] = default_score_val

    def _run_ri_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("RI評価を実行中..."); default_score_val = 2.5
        try:
            ri_res = self.ri_evaluator.evaluate(text, base_eval_ctx); full_results_dict["ri_result"] = ri_res
            main_score = ri_res.scores.get("ri_total_calculated")
            if isinstance(main_score, (int,float)): agg_scores["ri"] = float(main_score)
            else: agg_scores["ri"] = default_score_val; warn_msg="RI評価が数値スコア 'ri_total_calculated' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"RI Eval: {warn_msg}")
        except Exception as e:
            msg=f"RI評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["ri_result"] = EvaluationResult(analysis=f"RIエラー: {e}", scores={"ri_total_calculated": default_score_val}); agg_scores["ri"] = default_score_val

    def _run_subjective_eval(self, text: str, base_eval_ctx: Optional[EvaluationResult], full_results_dict: JsonDict, agg_scores: Dict[str, float]) -> None:
        self.logger.debug("主観性評価を実行中..."); default_score_val = 2.5
        try:
            subj_res = self.subjective_evaluator.evaluate(text, base_eval_ctx); full_results_dict["subjective_result"] = subj_res
            main_score = subj_res.scores.get("subjective_score")
            if isinstance(main_score, (int,float)): agg_scores["subjective"] = float(main_score)
            else: agg_scores["subjective"] = default_score_val; warn_msg="主観性評価が数値スコア 'subjective_score' を返しませんでした。"; self.logger.warning(warn_msg); full_results_dict["errors"].append(f"Subjective Eval: {warn_msg}")
        except Exception as e:
            msg=f"主観性評価失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg)
            full_results_dict["subjective_result"] = EvaluationResult(analysis=f"主観性エラー: {e}", scores={"subjective_score": default_score_val}); agg_scores["subjective"] = default_score_val

    def _run_distribution_analysis(self, text: str, full_results_dict: JsonDict) -> Tuple[JsonDict, JsonDict]:
        self.logger.debug("分布分析 (位相・層) を実行中..."); phase_dist: JsonDict = {}; layer_dist: JsonDict = {}
        if self.narrative_flow:
            try:
                phase_dist = self.narrative_flow.analyze_phase_distribution(text); full_results_dict["phase_distribution"] = phase_dist; self.logger.debug(f"位相分布: {phase_dist}")
            except Exception as e: msg=f"位相分布分析失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg); full_results_dict["phase_distribution"] = {}
            try:
                layer_dist = self.narrative_flow.analyze_layer_distribution(text); full_results_dict["layer_distribution"] = layer_dist; self.logger.debug(f"層分布: {layer_dist}")
            except Exception as e: msg=f"層分布分析失敗: {e}"; self.logger.error(msg, exc_info=True); full_results_dict["errors"].append(msg); full_results_dict["layer_distribution"] = {}
        else:
            msg="NarrativeFlowFrameworkが利用不可のため分布分析をスキップ。"; self.logger.warning(msg); full_results_dict["errors"].append(msg)
            full_results_dict["phase_distribution"] = {}; full_results_dict["layer_distribution"] = {}
        return phase_dist, layer_dist

    def _calculate_and_store_derived_scores(
        self, text: str, phase_dist: JsonDict, layer_dist: JsonDict, phase_focus: str,
        colloquial_level: str, emotion_arc: Optional[str],
        full_results_dict: JsonDict, agg_scores: Dict[str, float]
    ) -> None:
        self.logger.debug("派生スコア (位相、層、感情、口語) を計算中...")
        def _safe_calculate(func: Callable[..., float], *args: Any, score_name: str) -> float:
            default_score_val = 2.5
            try:
                score = func(*args)
                if not isinstance(score, (int,float)): self.logger.warning(f"{score_name} calculation non-numeric: {type(score)}. Defaulting."); full_results_dict["errors"].append(f"{score_name} Score Error: Invalid type {type(score)}"); return default_score_val
                return max(0.0, min(5.0, float(score)))
            except Exception as e: msg=f"{score_name} スコアリング失敗: {e}"; self.logger.warning(msg, exc_info=False); full_results_dict["errors"].append(msg); return default_score_val
        phase_score = _safe_calculate(self._calculate_phase_score, phase_dist, phase_focus, score_name="Phase")
        layer_score = _safe_calculate(self._calculate_layer_balance_score, layer_dist, score_name="Layer Balance")
        emotion_score = _safe_calculate(self._calculate_emotion_arc_score, text, emotion_arc, score_name="Emotion Arc")
        colloquial_score = _safe_calculate(self._calculate_colloquial_score, text, colloquial_level, score_name="Colloquial")
        full_results_dict["phase_score"] = round(phase_score,2); full_results_dict["layer_balance_score"] = round(layer_score,2)
        full_results_dict["emotion_arc_score"] = round(emotion_score,2); full_results_dict["colloquial_score"] = round(colloquial_score,2)
        agg_scores["phase"] = full_results_dict["phase_score"]; agg_scores["layer"] = full_results_dict["layer_balance_score"]
        agg_scores["emotion"] = full_results_dict["emotion_arc_score"]; agg_scores["colloquial"] = full_results_dict["colloquial_score"]
        self.logger.debug(f"派生スコア: Phase={phase_score:.1f}, Layer={layer_score:.1f}, Emotion={emotion_score:.1f}, Colloquial={colloquial_score:.1f}")

    def _calculate_phase_score(self, phase_distribution: JsonDict, phase_focus: str) -> float:
        if not phase_distribution: return 2.0
        target_dist=self.config.PHASE_BALANCE_TARGETS; tolerance=self.config.PHASE_DEVIATION_TOLERANCE
        focus_target_ratio=self.config.PHASE_SCORE_FOCUS_TARGET_RATIO; penalty_factor=self.config.PHASE_SCORE_BALANCE_PENALTY_FACTOR
        diversity_bonus=self.config.PHASE_SCORE_DIVERSITY_BONUS
        total_dev=0.0; valid_ph=0
        for ph,t_ratio in target_dist.items():
            if ph in phase_distribution:
                dev=abs(phase_distribution.get(ph,0.0)-t_ratio); pen=max(0,dev-tolerance)*penalty_factor; total_dev+=pen; valid_ph+=1
        max_pen=penalty_factor*valid_ph if valid_ph>0 else 1.0; bal_score=max(0.0,5.0-(total_dev/max(0.1,max_pen)*5.0))
        present_ph=len([p for p,r in phase_distribution.items() if isinstance(r,(int,float)) and r>0.01]); div_score=min(5.0,1.0+present_ph*diversity_bonus)
        foc_score=3.0
        if phase_focus!="balanced" and phase_focus in phase_distribution:
            act_ratio=phase_distribution.get(phase_focus,0.0); foc_score=min(5.0,max(1.0,3.0+(act_ratio-focus_target_ratio)*5.0))
        if phase_focus!="balanced": final_s=(foc_score*0.5)+(div_score*0.3)+(bal_score*0.2)
        else: final_s=(bal_score*0.45)+(div_score*0.45)+(foc_score*0.1)
        return max(0.0,min(5.0,final_s))

    def _calculate_layer_balance_score(self, layer_distribution: JsonDict) -> float:
        if not layer_distribution: return 2.0
        target_dist=self.config.LAYER_BALANCE_TARGETS; tolerance=self.config.LAYER_DEVIATION_TOLERANCE
        penalty_factor=self.config.LAYER_BALANCE_PENALTY_FACTOR
        total_dev_pen=0.0; valid_lay=0
        for lay,t_ratio in target_dist.items():
            if lay in layer_distribution:
                valid_lay+=1; act_ratio=layer_distribution.get(lay,0.0); dev=abs(act_ratio-t_ratio)
                pen=max(0,dev-tolerance)*penalty_factor; total_dev_pen+=pen
        max_poss_pen=penalty_factor*valid_lay if valid_lay>0 else 1.0; norm_pen=(total_dev_pen/max(0.1,max_poss_pen))*5.0
        score=5.0-norm_pen; return max(0.0,min(5.0,score))

    def _calculate_emotion_arc_score(self, text: str, emotion_arc_target: Optional[str]) -> float:
        if emotion_arc_target: self.logger.debug(f"Emotion arc: Target '{emotion_arc_target}'. Placeholder 3.0."); return 3.0
        else: self.logger.debug("Emotion arc: No target. Placeholder 3.0."); return 3.0

    def _calculate_colloquial_score(self, text: str, colloquial_target: str) -> float:
        self.logger.debug(f"Colloquial score: Target '{colloquial_target}'. Placeholder 3.0."); score=3.0
        text_l=text.lower(); informal_m=["だよね","ってか","まじ","ウケる","みたいな"]; formal_m=["である。","なり。","御座います","申し上げます","〜奉る"]
        informal_h=sum(m in text_l for m in informal_m); formal_h=sum(m in text for m in formal_m)
        if colloquial_target=="high":
            if formal_h>informal_h: score-=1.0
            elif informal_h==0: score-=0.5
        elif colloquial_target=="low":
            if informal_h>formal_h: score-=1.0
            elif formal_h==0: score-=0.5
        elif colloquial_target=="medium":
            if informal_h>formal_h*2 and formal_h<1:score-=0.5
            if formal_h>informal_h*2 and informal_h<1:score-=0.5
        return max(1.0,min(4.0,score))
    
    def _generate_text(
        self, input_text: str, target_length: int, perspective_mode: str,
        phase_focus: str, colloquial_level: str, emotion_arc: Optional[str],
        narrative_flow: Optional[str], improvement_instructions: Optional[str],
        temperature: Optional[float]
    ) -> Result[str, LLMError]:
        self.logger.info(f"Core text generation called. Type: {'Improvement' if improvement_instructions else 'Initial'}")
        
        prompt_params = {
            "input_text": input_text,
            "target_length": target_length,
            "perspective_mode": perspective_mode,
            "phase_focus": phase_focus,
            "colloquial_level": colloquial_level,
            "emotion_arc": emotion_arc or "指定なし",
            "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=25), # General vocab
            "narrative_flow_section": narrative_flow or "（物語構成の指定なし）",
            "improvement_section": improvement_instructions or "（特定の改善指示なし）" # For improvement template
        }
        
        current_template: str
        if improvement_instructions:
            current_template = self.improvement_template
            # Add evaluation context to prompt_params for improvement template
            # This requires access to last_eval_data, which isn't directly passed here.
            # This method might need refactoring if it's to handle both initial and improvement.
            # For now, assume improvement_template can handle missing eval details if needed.
            # Or, _perform_improvement_loop should format the full improvement prompt.
            # Let's assume this method is primarily for initial generation for now,
            # and improvement prompt formatting is handled elsewhere or this method is expanded.
            # For the current structure, we'll use a simplified approach.
            # The improvement_template expects 'original_text' and 'evaluation_results_json' etc.
            # This method is becoming complex if it tries to do both.
            # Decision: _generate_text is generic. Prompt formatting happens *before* calling it.
            # So, `improvement_instructions` here IS the fully formatted instruction part.
            # The `self.generation_template` or `self.improvement_template` should be chosen *before* calling this.

            # Let's refine: This method should receive the *final* prompt.
            # The calling method (_perform_initial_generation or _perform_improvement_loop)
            # will be responsible for formatting the correct template.
            # So, `input_text` here is actually the fully formatted prompt.
            final_prompt = input_text # Assuming input_text is the fully formatted prompt
        else: # Initial generation
            try:
                final_prompt = self.generation_template.format_map(SafeDict(prompt_params))
            except KeyError as e:
                self.logger.error(f"Generation template formatting error, missing key: {e}")
                return Result.fail(TemplateError(f"Generation template error: missing key {e}"))


        temp_to_use = temperature if temperature is not None else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)
        llm_result = self.llm.generate(final_prompt, temperature=temp_to_use)

        if llm_result.is_ok:
            generated_text = llm_result.unwrap()
            # Basic length control (can be refined)
            # if len(generated_text) > target_length * 1.5:
            #     generated_text = generated_text[:int(target_length * 1.2)]
            # elif len(generated_text) < target_length * 0.5 and not improvement_instructions:
            #     generated_text += "\n(追加のテキスト...スタブ補足)" * (target_length // 100)
            self.logger.debug(f"_generate_text produced text of length {len(generated_text)}")
            return Result.ok(generated_text)
        else:
            self.logger.error(f"_generate_text failed: {llm_result.error}")
            return Result.fail(llm_result.error)

    def _perform_initial_generation(
        self, initial_text: str, target_length: int, perspective_mode: str,
        phase_focus: str, colloquial_level: str, emotion_arc: Optional[str],
        narrative_flow_prompt: Optional[str]
    ) -> Result[str, LLMError]:
        self.logger.info("初期テキスト生成を実行中...")
        
        prompt_params = SafeDict({
            "input_text": initial_text, # This is context
            "target_length": target_length,
            "perspective_mode": perspective_mode,
            "phase_focus": phase_focus,
            "colloquial_level": colloquial_level,
            "emotion_arc": emotion_arc or "指定なし",
            "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=25),
            "narrative_flow_section": narrative_flow_prompt or "（物語構成の指示なし）"
        })
        try:
            final_prompt = self.generation_template.format_map(prompt_params)
        except KeyError as e:
            self.logger.error(f"Initial generation template formatting error, missing key: {e}")
            return Result.fail(TemplateError(f"Initial generation template error: missing key {e}"))
        
        # Call the LLM with the formatted prompt
        return self.llm.generate(
            final_prompt,
            temperature=self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)
        )

    def process(
        self,
        initial_text: str,
        target_length: Optional[int] = None,
        perspective_mode: str = "subjective_first_person",
        phase_focus: str = "balanced",
        colloquial_level: str = "medium",
        emotion_arc: Optional[str] = None,
        max_loops_override: Optional[int] = None,
        threshold_override: Optional[float] = None,
        narrative_flow_prompt: Optional[str] = None,
        skip_initial_generation: bool = False,
        ndgs_input_data: Optional[JsonDict] = None
    ) -> Result[JsonDict, NGGSError]:
        max_loops = max_loops_override if max_loops_override is not None else self.config.DEFAULT_MAX_LOOPS
        improvement_threshold = threshold_override if threshold_override is not None else self.config.DEFAULT_IMPROVEMENT_THRESHOLD
        final_target_length = target_length if target_length is not None else self.config.DEFAULT_TARGET_LENGTH

        if not isinstance(initial_text, str) and not (skip_initial_generation and ndgs_input_data):
             if not ndgs_input_data:
                 return Result.fail(ConfigurationError("初期テキストは文字列である必要があり、提供されていません。"))
        
        max_loops = max(0, min(max_loops, 10))
        improvement_threshold = max(0.0, min(5.0, improvement_threshold))

        self.logger.info(
            f"テキスト処理開始。最大ループ:{max_loops},目標長:{final_target_length},改善閾値:{improvement_threshold:.1f},"
            f"視点:{perspective_mode},位相焦点:{phase_focus},口語レベル:{colloquial_level},"
            f"感情弧:{emotion_arc or '指定なし'},初期生成スキップ:{skip_initial_generation},"
            f"NDGS入力:{'あり' if ndgs_input_data else 'なし'}"
        )
        results_dict: JsonDict = {
            "job_id": self.config.generate_job_id(), "start_time": datetime.now(timezone.utc).isoformat(),
            "original_text_provided": initial_text if isinstance(initial_text, str) else "(NDGSデータから抽出予定)",
            "parameters": {
                "target_length": final_target_length, "perspective_mode": perspective_mode,
                "phase_focus": phase_focus, "colloquial_level": colloquial_level, "emotion_arc": emotion_arc,
                "max_loops": max_loops, "improvement_threshold": improvement_threshold,
                "skip_initial_generation": skip_initial_generation, "ndgs_input_provided": bool(ndgs_input_data),
                "llm_engine": self.llm.current_engine, "llm_model": self.llm._get_model_name()
            },
            "versions": [], "final_text": "", "best_loop_index": -1, "final_scores": {},
            "distributions": {"phase": {}, "layer": {}}, "html_report": "", "status": "Processing", "errors": []
        }
        current_initial_text = initial_text if isinstance(initial_text, str) else ""
        if ndgs_input_data and self.ndgs_parser:
            self.logger.info("NDGS入力データを解析中...")
            try:
                parsed_ndgs_context = self.ndgs_parser.parse(ndgs_input_data) # type: ignore
                if parsed_ndgs_context.is_ok:
                    ndgs_content = parsed_ndgs_context.unwrap()
                    current_initial_text = ndgs_content.get("initial_text", current_initial_text)
                    perspective_mode = ndgs_content.get("perspective_mode", perspective_mode)
                    results_dict["original_text_provided"] = current_initial_text
                    self.logger.info("NDGSデータ解析完了。パラメータが更新された可能性があります。")
                else:
                    err_msg = f"NDGS入力データの解析失敗: {parsed_ndgs_context.error}"; self.logger.error(err_msg)
                    results_dict["errors"].append(err_msg)
            except Exception as e:
                err_msg = f"NDGS入力データの処理中に予期せぬエラー: {e}"; self.logger.error(err_msg, exc_info=True)
                results_dict["errors"].append(err_msg)
        if not current_initial_text and not skip_initial_generation:
            return Result.fail(ConfigurationError("処理可能な初期テキストが見つかりませんでした (NDGSデータからも抽出不可)。"))

        current_text_for_loop = current_initial_text if skip_initial_generation else ""
        best_text = current_text_for_loop
        best_full_eval_data: JsonDict = {}
        
        final_narrative_flow_prompt = narrative_flow_prompt
        if final_narrative_flow_prompt is None and self.narrative_flow:
            try:
                theme = "記憶回帰型"
                final_narrative_flow_prompt = self.narrative_flow.generate_flow_template(
                    theme=theme, emotion_arc=emotion_arc, perspective=perspective_mode
                )
                self.logger.debug(f"生成された物語構成プロンプト: {truncate_text(final_narrative_flow_prompt, 100)}")
            except Exception as e:
                err_msg = f"物語構成プロンプトの生成失敗: {e}"; self.logger.error(err_msg, exc_info=True)
                results_dict["errors"].append(err_msg); final_narrative_flow_prompt = None
        try:
            loop_idx = 0
            version_data: JsonDict = {"loop": loop_idx, "status": "Pending"}
            results_dict["versions"].append(version_data)
            self.logger.info(f"--- 開始 ループ {loop_idx} ({'初期評価' if skip_initial_generation else '初期生成'}) ---")
            start_loop_time = time.monotonic()
            processing_error_occurred_this_loop = False

            if skip_initial_generation:
                self.logger.info("初期生成スキップ、提供されたテキストを評価します。")
                current_text_for_loop = current_initial_text
                version_data["text_source"] = "provided_initial_text"; version_data["text"] = current_text_for_loop
            else:
                version_data["text_source"] = "initial_generation"
                generation_result = self._perform_initial_generation(
                    initial_text=current_initial_text, target_length=final_target_length,
                    perspective_mode=perspective_mode, phase_focus=phase_focus, colloquial_level=colloquial_level,
                    emotion_arc=emotion_arc, narrative_flow_prompt=final_narrative_flow_prompt
                )
                if generation_result.is_err:
                    if not self._handle_error(generation_result.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                        self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（生成）")
                        return Result.fail(generation_result.error)
                    processing_error_occurred_this_loop = True; current_text_for_loop = ""; version_data["status"] = "失敗 (生成)"
                else:
                    current_text_for_loop = generation_result.unwrap(); version_data["text"] = current_text_for_loop
            
            if not processing_error_occurred_this_loop:
                eval_res = self._perform_full_evaluation(
                    text=current_text_for_loop, phase_focus=phase_focus,
                    colloquial_level=colloquial_level, emotion_arc=emotion_arc
                )
                if eval_res.is_err:
                    if not self._handle_error(eval_res.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                        self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（評価）")
                        return Result.fail(eval_res.error)
                    processing_error_occurred_this_loop = True; version_data["status"] = "失敗 (評価フレームワークエラー)"
                else:
                    full_eval_data_loop0 = eval_res.unwrap(); version_data.update(full_eval_data_loop0)
                    if full_eval_data_loop0.get("llm_eval_success", False):
                        best_text = current_text_for_loop; best_full_eval_data = full_eval_data_loop0
                        results_dict["best_loop_index"] = loop_idx; version_data["status"] = "成功"
                    else:
                        processing_error_occurred_this_loop = True; version_data["status"] = "失敗 (LLM評価)"
            
            if version_data["status"] == "Pending": version_data["status"] = "失敗 (不定)"
            if processing_error_occurred_this_loop and "失敗" not in version_data["status"]:
                 version_data["status"] = "完了（回復可能エラーあり）"

            loop_duration = time.monotonic() - start_loop_time
            self.logger.info(f"--- 完了 ループ {loop_idx} ({version_data.get('status', '?')}) - {loop_duration:.2f} 秒 ---")
            
            current_best_score = best_full_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0)
            min_loops_for_threshold_check = self.config.MIN_FEEDBACK_LOOPS if self.config.MIN_FEEDBACK_LOOPS > 0 else 1
            should_terminate_early = (
                max_loops == 0 or
                ("失敗" in version_data.get("status", "") and not version_data.get("status", "").startswith("失敗 (LLM評価)")) or
                (current_best_score >= improvement_threshold and results_dict["best_loop_index"] == 0 and loop_idx + 1 >= min_loops_for_threshold_check)
            )
            if should_terminate_early:
                final_status_msg = version_data.get("status", "完了")
                if results_dict["best_loop_index"] == -1 and "失敗" not in final_status_msg : final_status_msg = "失敗 (有効な結果なし)"
                elif "失敗" in final_status_msg: final_status_msg = "エラーで終了"
                elif current_best_score >= improvement_threshold and max_loops > 0 : final_status_msg = "完了 (閾値達成)"
                elif max_loops == 0 : final_status_msg = "完了 (初期評価のみ)"
                self.logger.info(f"ループ0完了後、処理終了。Status: {final_status_msg}")
                self._finalize_results(results_dict, best_text, best_full_eval_data, final_status_msg)
                return Result.ok(results_dict)

            # --- Improvement Loops (Part 13 will detail _perform_improvement_loop) ---
            for improvement_loop_idx in range(1, max_loops + 1):
                loop_idx = improvement_loop_idx
                version_data = {"loop": loop_idx, "status": "Pending"}
                results_dict["versions"].append(version_data)
                self.logger.info(f"--- 開始 ループ {loop_idx} (改善) ---")
                start_loop_time = time.monotonic()
                
                current_best_score = best_full_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0)
                if not best_full_eval_data or not best_text:
                    version_data["status"] = "スキップ (有効な前結果なし)"; self.logger.warning(f"ループ {loop_idx}: スキップ。前ループの有効なテキスト/評価がありません。"); results_dict["errors"].append(f"Loop {loop_idx}: Skipped due to no valid prior result."); break
                if current_best_score >= improvement_threshold and loop_idx >= min_loops_for_threshold_check:
                    version_data["status"] = f"スキップ (閾値達成: {current_best_score:.2f})"; self.logger.info(f"ループ {loop_idx}: 早期終了。スコア {current_best_score:.2f} >= 閾値 {improvement_threshold:.2f}"); break

                # Call to _perform_improvement_loop will be implemented in Part 13
                loop_result = self._perform_improvement_loop(
                    loop_idx=loop_idx,
                    current_best_text=best_text,
                    current_best_eval_data=best_full_eval_data,
                    results_dict=results_dict, # For error logging within helper
                    version_data=version_data, # For updating status and text
                    final_target_length=final_target_length,
                    perspective_mode=perspective_mode,
                    phase_focus=phase_focus,
                    colloquial_level=colloquial_level,
                    emotion_arc=emotion_arc,
                    narrative_flow_prompt=final_narrative_flow_prompt
                )
                loop_duration = time.monotonic() - start_loop_time
                self.logger.info(f"--- 完了 ループ {loop_idx} ({version_data.get('status', '?')}) - {loop_duration:.2f} 秒 ---")

                if loop_result.is_err: # Critical error from _perform_improvement_loop
                    self.logger.critical(f"ループ {loop_idx} で回復不能なエラー発生。処理を中止します。 Error: {loop_result.error}")
                    self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー（改善ループ）")
                    return Result.fail(loop_result.error)
                else:
                    new_eval_data = loop_result.unwrap()
                    new_overall_score = new_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0)
                    new_text = version_data.get("text", "")

                    improvement_margin = 0.05 # Require slight score increase
                    is_improvement = (
                        new_eval_data.get("llm_eval_success", False) and
                        new_text and
                        new_overall_score > (current_best_score + improvement_margin)
                    )
                    if is_improvement:
                        self.logger.info(f"ループ {loop_idx}: 改善を検出！ 新スコア: {new_overall_score:.2f} > 旧ベスト: {current_best_score:.2f}")
                        best_text = new_text; best_full_eval_data = new_eval_data
                        results_dict["best_loop_index"] = loop_idx
                    else:
                        reason = "(LLM評価失敗)" if not new_eval_data.get("llm_eval_success") else ""
                        self.logger.info(f"ループ {loop_idx}: 大きな改善なし。スコア: {new_overall_score:.2f} (ベスト: {current_best_score:.2f}) {reason}")
                    
                    if "失敗" in version_data.get("status", "") and not version_data.get("status","").startswith("失敗 (LLM評価)"):
                        self.logger.warning(f"ループ {loop_idx} はエラーで終了しました。改善ループを停止します。")
                        break
            
            final_status_msg = "完了"
            if results_dict["errors"]: final_status_msg = "エラーあり完了"
            if results_dict["best_loop_index"] == -1: final_status_msg = "失敗 (有効な結果なし)"
            elif best_full_eval_data.get("aggregated_scores", {}).get("overall_quality", 0.0) >= improvement_threshold and results_dict["best_loop_index"] >=0 : final_status_msg = "完了 (閾値達成)"
            elif loop_idx >= max_loops : final_status_msg = "完了 (最大ループ到達)"
            self.logger.info(f"全ループ処理完了。 Status: {final_status_msg}")
            self._finalize_results(results_dict, best_text, best_full_eval_data, final_status_msg)
            return Result.ok(results_dict)
        except Exception as e:
            error_type_name = type(e).__name__
            critical_err_msg = f"TextProcessor.processで予期せぬクリティカルエラー: {error_type_name} - {e}"
            self.logger.critical(critical_err_msg, exc_info=True)
            results_dict["status"] = "致命的エラー"; results_dict["errors"].append(critical_err_msg)
            self._finalize_results(results_dict, best_text, best_full_eval_data, "致命的エラー")
            return Result.fail(NGGSError(critical_err_msg, details={"exception_type": error_type_name}))

    def _handle_error(
        self, error: Exception, loop_idx: int, results_dict: JsonDict, version_data: JsonDict, is_fatal_loop: bool = False
    ) -> bool:
        error_type = type(error)
        severity: ErrorSeverity
        if is_fatal_loop:
            base_severity = ERROR_SEVERITY_MAP.get(error_type, ErrorSeverity.LOOP)
            severity = ErrorSeverity.FATAL if base_severity == ErrorSeverity.FATAL else ErrorSeverity.LOOP
        else:
            severity = ERROR_SEVERITY_MAP.get(error_type, ErrorSeverity.FATAL)
        error_msg = f"Loop {loop_idx}: [{severity.name}] {error_type.__name__} - {str(error)}"
        log_func = self.logger.critical if severity == ErrorSeverity.FATAL else \
                   self.logger.error if severity == ErrorSeverity.LOOP else self.logger.warning
        log_exc_info = severity == ErrorSeverity.FATAL
        is_safety_block = isinstance(error, ValueError) and ("コンテンツ生成ブロック" in str(error) or "SAFETY" in str(error).upper())
        if is_safety_block: log_exc_info = False
        log_func(error_msg, exc_info=log_exc_info)
        version_data["status"] = f"失敗 ({severity.name})"
        version_data["error_details"] = f"[{severity.name}] {error_type.__name__}: {str(error)}"
        if isinstance(error, NGGSError) and error.get_context():
            version_data["error_details"] += f" Context: {error.get_context()}"
        global_err_summary = f"Loop {loop_idx}: [{severity.name}] {error_type.__name__}"
        if global_err_summary not in results_dict["errors"]: results_dict["errors"].append(global_err_summary)
        if severity == ErrorSeverity.FATAL: return False
        return True

    def _finalize_results(
        self, results_dict: JsonDict, best_text: str, best_full_eval_data: JsonDict, final_status: str
    ) -> None:
        self.logger.info(f"結果を最終化中... Status: {final_status}")
        results_dict["final_text"] = best_text if best_text else "(有効な生成テキストなし)"
        results_dict["best_loop_index"] = best_full_eval_data.get("loop", results_dict.get("best_loop_index", -1))
        agg_scores = best_full_eval_data.get("aggregated_scores", {})
        results_dict["final_scores"] = {k: round(v,2) for k,v in agg_scores.items() if isinstance(v,(int,float))}
        phase_dist = best_full_eval_data.get("phase_distribution"); layer_dist = best_full_eval_data.get("layer_distribution")
        results_dict["distributions"] = {"phase": phase_dist if isinstance(phase_dist,dict) else {}, "layer": layer_dist if isinstance(layer_dist,dict) else {}}
        results_dict["completion_time"] = datetime.now(timezone.utc).isoformat()
        if "致命的エラー" not in results_dict.get("status", ""): results_dict["status"] = final_status
        self.logger.info(f"_finalize_results called. HTML report generation (Part 14) is next.")
        # HTML report generation will be called here:
        # try:
        #     results_dict["html_report"] = self._generate_html_report(results_dict)
        # except Exception as e:
        #     self.logger.error(f"HTMLレポート生成失敗: {e}", exc_info=True)
        #     results_dict["html_report"] = f"Error generating report: {e}"
        #     results_dict["errors"].append(f"HTML Report Error: {e}")


    # --- Improvement Loop Core (Part 13) ---
    def _perform_improvement_loop(
        self,
        loop_idx: int,
        current_best_text: str,
        current_best_eval_data: JsonDict,
        results_dict: JsonDict, # For global error logging
        version_data: JsonDict, # To update status, text, eval for this loop
        final_target_length: int,
        perspective_mode: str,
        phase_focus: str,
        colloquial_level: str,
        emotion_arc: Optional[str],
        narrative_flow_prompt: Optional[str] # Re-pass for consistency if needed
    ) -> Result[JsonDict, NGGSError]: # Returns the full_eval_data for this loop
        """
        Performs a single iteration of the improvement loop.
        Determines strategy, generates instructions, generates new text, and evaluates.
        """
        self.logger.info(f"改善ループ {loop_idx} を開始。")
        version_data["status"] = "改善中"
        processing_error_this_loop = False

        # 1. Determine Improvement Strategy
        strategy = self._determine_improvement_strategy(current_best_eval_data)
        version_data["improvement_strategy"] = strategy
        self.logger.info(f"ループ {loop_idx}: 改善戦略「{strategy}」を選択。")

        # 2. Generate Improvement Instructions
        instruction_result = self._generate_specific_improvement_instructions(
            strategy, current_best_eval_data, perspective_mode, phase_focus, colloquial_level, emotion_arc
        )

        improvement_instructions: Optional[str]
        if instruction_result.is_ok:
            improvement_instructions = instruction_result.unwrap()
            if not improvement_instructions: # Strategy template might return empty if not applicable
                self.logger.warning(f"ループ {loop_idx}: 特定戦略の指示が空です。LLM標準指示を試みます。")
                instruction_result = self._generate_llm_improvement_instructions(
                    current_best_text, current_best_eval_data, perspective_mode, phase_focus, colloquial_level, emotion_arc
                )
                if instruction_result.is_ok:
                    improvement_instructions = instruction_result.unwrap()
                else: # LLM standard instructions also failed
                    self.logger.error(f"ループ {loop_idx}: LLM標準指示の生成も失敗: {instruction_result.error}")
                    improvement_instructions = self._get_contextual_fallback_improvement(current_best_eval_data)
                    version_data["errors"] = version_data.get("errors", []) + [f"LLM Instruction Gen Error: {instruction_result.error}"]
        else: # Specific instructions failed (e.g., template error)
            self.logger.error(f"ループ {loop_idx}: 特定戦略の指示生成に失敗: {instruction_result.error}")
            improvement_instructions = self._get_contextual_fallback_improvement(current_best_eval_data)
            version_data["errors"] = version_data.get("errors", []) + [f"Specific Instruction Gen Error: {instruction_result.error}"]

        if not improvement_instructions or not improvement_instructions.strip():
            self.logger.warning(f"ループ {loop_idx}: 有効な改善指示が得られませんでした。デフォルトフォールバックを使用。")
            improvement_instructions = self.config.DEFAULT_FALLBACK_IMPROVEMENT
        
        version_data["improvement_instructions"] = improvement_instructions
        self.logger.debug(f"ループ {loop_idx} 改善指示:\n{truncate_text(improvement_instructions, 200)}")

        # 3. Generate New Text based on instructions
        # Adjust temperature for improvement loops (gradually decrease)
        current_temp = max(
            self.config.IMPROVEMENT_MIN_TEMPERATURE,
            self.config.IMPROVEMENT_BASE_TEMPERATURE - (self.config.IMPROVEMENT_TEMP_DECREASE_PER_LOOP * (loop_idx -1))
        )
        self.logger.info(f"ループ {loop_idx}: 改善生成に温度 {current_temp:.2f} を使用。")

        # Prepare prompt for improvement using self.improvement_template
        # This requires careful formatting of evaluation results and other context.
        # For now, we pass `current_best_text` as `input_text` and `improvement_instructions`
        # to the generic `_generate_text` which will use `self.improvement_template`
        # if `improvement_instructions` are provided. This needs refinement in Part 14.

        # Simplified: Assume _generate_text can use improvement_template if instructions are present.
        # A more robust way would be to format the improvement prompt fully here.
        # For now, we rely on the _generate_text stub to correctly use the improvement_template.
        # The _generate_text method needs to be made aware of which template to use.
        # Let's refine _generate_text slightly for this.
        # OR, create a new method _generate_improved_text.
        # For now, we pass instructions to _generate_text.
        
        # Create the full prompt for improvement here
        eval_summary_json = "{}"
        try:
            # Create a compact summary of the evaluation for the prompt
            scores_for_prompt = {
                "llm_scores": current_best_eval_data.get("llm_eval_result", {}).get("scores",{}),
                "aggregated_scores": current_best_eval_data.get("aggregated_scores", {}),
                "phase_distribution": current_best_eval_data.get("phase_distribution", {}),
                "layer_distribution": current_best_eval_data.get("layer_distribution", {})
            }
            eval_summary_json = json.dumps(scores_for_prompt, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
        except Exception as e:
            self.logger.warning(f"ループ {loop_idx}: 評価結果のJSON化失敗: {e}")
            eval_summary_json = "{ /* 評価JSON化エラー */ }"

        low_scores_str = ", ".join([f"{get_metric_display_name(k)}({v:.1f})" for k,v in current_best_eval_data.get("aggregated_scores", {}).items() if isinstance(v, (int,float)) and v < 3.0]) or "特になし"
        high_scores_str = ", ".join([f"{get_metric_display_name(k)}({v:.1f})" for k,v in current_best_eval_data.get("aggregated_scores", {}).items() if isinstance(v, (int,float)) and v >= 4.0]) or "特になし"


        improvement_prompt_params = SafeDict({
            "original_text": truncate_text(current_best_text, 800), # Context
            "evaluation_results_json": eval_summary_json,
            "low_score_items_str": low_scores_str,
            "high_score_items_str": high_scores_str,
            "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=15, layer=strategy if strategy in self.vocab_manager.VALID_LAYERS else None), # Strategy-based vocab
            "perspective_mode": perspective_mode,
            "phase_focus": phase_focus,
            "colloquial_level": colloquial_level,
            "emotion_arc": emotion_arc or "指定なし",
            "layer_distribution_analysis": self._generate_distribution_analysis_text(current_best_eval_data.get("layer_distribution",{}), "層"),
            "phase_distribution_analysis": self._generate_distribution_analysis_text(current_best_eval_data.get("phase_distribution",{}), "位相"),
            "improvement_section": improvement_instructions
        })

        try:
            improvement_generation_prompt = self.improvement_template.format_map(improvement_prompt_params)
        except KeyError as e:
            self.logger.error(f"Improvement template formatting error, missing key: {e} in loop {loop_idx}")
            version_data["status"] = "失敗 (改善プロンプトテンプレートエラー)"
            version_data["error_details"] = f"Template error: missing key {e}"
            # Return default aggregated scores as this loop failed critically before generation
            return Result.ok({"aggregated_scores": self._get_default_aggregated_scores(), "errors": [f"Template error: {e}"], "llm_eval_success": False})


        generation_result = self.llm.generate(
            improvement_generation_prompt,
            temperature=current_temp
        )

        new_text: str
        if generation_result.is_err:
            if not self._handle_error(generation_result.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                 # This error is critical for the entire process if it's fatal
                return Result.fail(generation_result.error) # Propagate to stop all processing
            processing_error_this_loop = True
            new_text = "" # No new text generated
            version_data["status"] = "失敗 (改善後生成)"
        else:
            new_text = generation_result.unwrap()
            version_data["text"] = new_text
            self.logger.info(f"ループ {loop_idx}: 改善テキスト生成成功 (長さ: {len(new_text)})。")

        # 4. Evaluate New Text
        if not processing_error_this_loop: # Only evaluate if generation was successful
            eval_res = self._perform_full_evaluation(
                text=new_text,
                phase_focus=phase_focus,
                colloquial_level=colloquial_level,
                emotion_arc=emotion_arc
            )
            if eval_res.is_err: # Critical error in evaluation framework itself
                if not self._handle_error(eval_res.error, loop_idx, results_dict, version_data, is_fatal_loop=True):
                    return Result.fail(eval_res.error) # Propagate
                processing_error_this_loop = True
                # version_data status already set by _handle_error
            else: # Evaluation ran, possibly with recoverable errors or LLM eval failure
                full_eval_data_this_loop = eval_res.unwrap()
                version_data.update(full_eval_data_this_loop) # Store all new eval data
                if full_eval_data_this_loop.get("llm_eval_success", False):
                    version_data["status"] = "成功"
                else:
                    processing_error_this_loop = True
                    version_data["status"] = "失敗 (LLM評価)"
        
        if version_data["status"] == "Pending": # If status wasn't set due to prior error
            version_data["status"] = "失敗 (不定)"
        if processing_error_this_loop and "失敗" not in version_data["status"]:
            version_data["status"] = "完了（回復可能エラーあり）"

        # Return the evaluation data of the newly generated text
        # The caller (process method) will use this to compare with best_full_eval_data
        return Result.ok(version_data) # version_data now contains the full_eval_data for this loop


    def _determine_improvement_strategy(self, last_eval_data: JsonDict) -> str:
        """
        Determines the improvement strategy based on the last evaluation scores.
        Returns a string key for the strategy (e.g., "gothic", "readability").
        """
        agg_scores = last_eval_data.get("aggregated_scores", {})
        if not agg_scores:
            self.logger.warning("評価スコアが利用できないため、デフォルト戦略「balance」を使用します。")
            return "balance" # Default strategy

        # Define thresholds for what's considered "low" for each metric
        # These thresholds might need tuning.
        # Lower score = more urgent need for improvement in that area.
        # Order of checks can imply priority.
        # Overall quality is the ultimate target.
        overall_quality = agg_scores.get("overall_quality", 0.0)

        # If overall quality is very low, focus on fundamental Gothic elements or readability.
        if overall_quality < 2.8:
            # Prioritize ETI if it's also low, otherwise readability.
            if agg_scores.get("eti", 5.0) < 3.0: return "gothic"
            if agg_scores.get("ri", 5.0) < 3.0: return "readability"
            return "gothic" # Default for very low overall

        # Specific weaknesses (lower score = higher priority)
        # Create a list of (score, strategy_key) for areas below a threshold
        potential_focus_areas = [
            (agg_scores.get("eti", 5.0), "gothic"),
            (agg_scores.get("subjective", 5.0), "subjective"),
            (agg_scores.get("ri", 5.0), "readability"),
            (agg_scores.get("layer", 5.0), "layer_balance"), # layer_balance_score
            (agg_scores.get("phase", 5.0), "phase_balance"), # phase_score
            (agg_scores.get("emotion", 5.0), "emotion_arc"), # emotion_arc_score
            (agg_scores.get("colloquial", 5.0), "colloquial_blend"), # colloquial_score
        ]
        
        # Filter for areas that are below a moderate threshold (e.g., 3.5)
        # and sort them by score (lowest first)
        weak_areas = sorted(
            [(score, name) for score, name in potential_focus_areas if isinstance(score, (int,float)) and score < 3.5],
            key=lambda x: x[0]
        )

        if weak_areas:
            # Focus on the weakest area first
            strategy = weak_areas[0][1]
            self.logger.info(f"改善戦略決定: 最も低いスコアのエリア「{strategy}」 ({weak_areas[0][0]:.2f}) に焦点を当てます。")
            return strategy
        else:
            # If all scores are reasonably good, focus on overall balance or a default.
            self.logger.info("全ての主要スコアが良好です。全体バランス戦略「balance」を選択します。")
            return "balance"


    def _generate_specific_improvement_instructions(
        self, strategy: str, last_eval_data: JsonDict,
        perspective_mode: Optional[str], phase_focus: Optional[str],
        colloquial_level: Optional[str], emotion_arc: Optional[str]
    ) -> Result[str, TemplateError]:
        """
        Generates specific improvement instructions based on the chosen strategy
        by calling a corresponding _create_*_template method.
        """
        self.logger.debug(f"戦略「{strategy}」に基づいた特定改善指示を生成中...")
        
        # Ensure necessary evaluation results are available
        agg_scores = last_eval_data.get("aggregated_scores", {})
        eti_result = last_eval_data.get("eti_result")
        ri_result = last_eval_data.get("ri_result")
        subj_result = last_eval_data.get("subjective_result")
        phase_dist = last_eval_data.get("phase_distribution", {})
        layer_dist = last_eval_data.get("layer_distribution", {})

        # Ensure results are of the correct type (EvaluationResult)
        if strategy == "gothic" and not isinstance(eti_result, EvaluationResult):
            return Result.fail(TemplateError(f"ETI結果が不正なため「{strategy}」戦略の指示を生成できません。"))
        if strategy == "readability" and not isinstance(ri_result, EvaluationResult):
            return Result.fail(TemplateError(f"RI結果が不正なため「{strategy}」戦略の指示を生成できません。"))
        if strategy == "subjective" and not isinstance(subj_result, EvaluationResult):
            return Result.fail(TemplateError(f"主観性評価結果が不正なため「{strategy}」戦略の指示を生成できません。"))

        # Dispatch to specific template creation methods (stubs for Part 13, full in Part 14)
        if strategy == "gothic":
            return self._create_gothic_template(eti_result, agg_scores, phase_dist, colloquial_level)
        elif strategy == "readability":
            return self._create_readability_template(ri_result, agg_scores, phase_dist, colloquial_level)
        elif strategy == "subjective":
            return self._create_subjective_template(subj_result, phase_dist, perspective_mode)
        elif strategy == "phase_balance":
            return self._create_phase_template(phase_dist, phase_focus)
        elif strategy == "layer_balance":
            layer_score = agg_scores.get("layer", 0.0) # layer_balance_score
            return self._create_layer_template(layer_dist, layer_score)
        elif strategy == "emotion_arc":
            emotion_score = agg_scores.get("emotion", 0.0) # emotion_arc_score
            return self._create_emotion_template(emotion_score, emotion_arc)
        elif strategy == "colloquial_blend":
            colloquial_score = agg_scores.get("colloquial", 0.0) # colloquial_score
            return self._create_colloquial_template(colloquial_level, colloquial_score)
        elif strategy == "balance":
            return self._create_balance_template(last_eval_data, perspective_mode, colloquial_level)
        else:
            self.logger.warning(f"不明な改善戦略「{strategy}」。LLM標準指示にフォールバックします。")
            return Result.ok("") # Return empty string to trigger LLM standard instructions


    # --- Stub _create_*_template methods (Full implementation in Part 14) ---
    def _create_gothic_template(self, eti_res: EvaluationResult, agg_s: Dict, ph_dist: Dict, cl_lvl: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_gothic_template (stub) called.")
        return Result.ok("# (ゴシック性強化指示スタブ)\n- 象徴的な語彙を増やしてください。\n- 不安や神秘の雰囲気を高めてください。")
    def _create_readability_template(self, ri_res: EvaluationResult, agg_s: Dict, ph_dist: Dict, cl_lvl: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_readability_template (stub) called.")
        return Result.ok("# (可読性向上指示スタブ)\n- 一文を短く、簡潔にしてください。\n- 段落構成を見直してください。")
    def _create_subjective_template(self, subj_res: EvaluationResult, ph_dist: Dict, persp: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_subjective_template (stub) called.")
        return Result.ok("# (主観性深化指示スタブ)\n- 主人公の内面描写を増やしてください。\n- モノローグを効果的に使用してください。")
    def _create_phase_template(self, ph_dist: Dict, ph_focus: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_phase_template (stub) called.")
        return Result.ok("# (位相バランス調整指示スタブ)\n- セリフとナレーションのバランスを調整してください。\n- 位相間の移行を自然にしてください。")
    def _create_layer_template(self, lay_dist: Dict, lay_score: float) -> Result[str, TemplateError]:
        self.logger.debug("_create_layer_template (stub) called.")
        return Result.ok("# (層バランス調整指示スタブ)\n- 心理層と感覚層の描写を増やしてください。\n- 各層の連携を意識してください。")
    def _create_emotion_template(self, emo_score: float, emo_arc: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_emotion_template (stub) called.")
        return Result.ok(f"# (感情変容指示スタブ)\n- 感情の弧「{emo_arc or '指定なし'}」に沿った変化を明確にしてください。")
    def _create_colloquial_template(self, cl_lvl: Optional[str], curr_score: float) -> Result[str, TemplateError]:
        self.logger.debug("_create_colloquial_template (stub) called.")
        return Result.ok(f"# (口語性調整指示スタブ)\n- 口語レベル「{cl_lvl or 'medium'}」に合わせて調整してください。")
    def _create_balance_template(self, last_eval: Dict, persp: Optional[str], cl_lvl: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_balance_template (stub) called.")
        return Result.ok("# (全体バランス調整指示スタブ)\n- 全体的な品質向上を目指し、各評価項目をバランス良く改善してください。")
    # --- End Stub _create_*_template methods ---


    def _generate_llm_improvement_instructions(
        self, current_text: str, last_eval_data: JsonDict,
        perspective_mode: Optional[str], phase_focus: Optional[str],
        colloquial_level: Optional[str], emotion_arc: Optional[str]
    ) -> Result[str, LLMError]:
        self.logger.debug("LLMによる標準改善指示を生成中...")
        try:
            agg_scores = last_eval_data.get("aggregated_scores", {})
            llm_eval_res_obj = last_eval_data.get("llm_eval_result")
            llm_scores = getattr(llm_eval_res_obj, 'scores', {}) if isinstance(llm_eval_res_obj, EvaluationResult) else {}
            llm_reasons = getattr(llm_eval_res_obj, 'reasoning', {}) if isinstance(llm_eval_res_obj, EvaluationResult) else {}

            low_keys = [k for k,v in agg_scores.items() if isinstance(v,(int,float)) and v < 3.5]
            high_keys = [k for k,v in agg_scores.items() if isinstance(v,(int,float)) and v >= 4.0]
            low_score_str = ", ".join([f"「{get_metric_display_name(k)}」({agg_scores.get(k,0.0):.1f})" for k in low_keys]) or "特になし"
            high_score_str = ", ".join([f"「{get_metric_display_name(k)}」({agg_scores.get(k,0.0):.1f})" for k in high_keys]) or "特になし"
            vocab_str = self.vocab_manager.get_vocabulary_for_prompt(count=15)

            scores_for_prompt = {
                "llm_scores": llm_scores,
                "llm_reasons": {k: truncate_text(v, 80) for k,v in llm_reasons.items()},
                "derived_scores": {k:v for k,v in agg_scores.items() if k in ["eti","ri","subjective","phase","layer","emotion","colloquial"] and isinstance(v,(int,float))},
                "phase_distribution": last_eval_data.get("phase_distribution"),
                "layer_distribution": last_eval_data.get("layer_distribution")
            }
            scores_for_prompt_clean = {k:v for k,v in scores_for_prompt.items() if v is not None and v != {} and v != []}
            try:
                scores_json = json.dumps(scores_for_prompt_clean, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
            except Exception as json_e:
                self.logger.warning(f"評価結果のJSON化失敗 (LLM指示用): {json_e}")
                scores_json = json.dumps({"llm_scores": llm_scores}, ensure_ascii=False, indent=2, default=str)

            format_data = SafeDict({
                "original_text": truncate_text(current_text, 800),
                "evaluation_results_json": scores_json,
                "low_score_items_str": low_score_str, "high_score_items_str": high_score_str,
                "vocabulary_list_str": vocab_str,
                "perspective_mode": perspective_mode or "指定なし", "phase_focus": phase_focus or "指定なし",
                "colloquial_level": colloquial_level or "指定なし", "emotion_arc": emotion_arc or "指定なし",
                "layer_distribution_analysis": self._generate_distribution_analysis_text(last_eval_data.get("layer_distribution",{}), "層"),
                "phase_distribution_analysis": self._generate_distribution_analysis_text(last_eval_data.get("phase_distribution",{}), "位相"),
                "improvement_section": "" # This will be filled by LLM if template is structured for it, or LLM generates its own section
            })
            prompt = self.improvement_template.format_map(format_data) # Use the main improvement template

            instruction_result = self.llm.generate(prompt, temperature=0.3) # Lower temp for focused instructions

            if instruction_result.is_ok:
                instruction_text = instruction_result.unwrap()
                extracted_instructions = extract_improvement_instructions(instruction_text)
                if extracted_instructions:
                    self.logger.info("LLMベースの改善指示を生成・抽出成功。")
                    return Result.ok(extracted_instructions)
                else:
                    self.logger.warning("LLM応答から構造化指示を抽出できず。応答全体を使用。")
                    return Result.ok(instruction_text.strip()) # Use full response if extraction fails
            else:
                self.logger.error(f"LLMによる標準改善指示の生成失敗: {instruction_result.error}")
                return Result.fail(instruction_result.error)
        except TemplateError as e:
            self.logger.error(f"改善指示テンプレートエラー: {e}", exc_info=True)
            return Result.fail(LLMError(f"Improvement template error: {e}"))
        except Exception as e:
            self.logger.error(f"LLM改善指示生成中の予期せぬエラー: {e}", exc_info=True)
            return Result.fail(LLMError(f"Unexpected error generating LLM instructions: {e}"))


    def _get_contextual_fallback_improvement(self, last_eval_data: JsonDict) -> str:
        """Generates context-aware fallback instructions if LLM/strategy fails."""
        self.logger.warning("コンテキストフォールバック改善指示を生成中。")
        agg_scores = last_eval_data.get("aggregated_scores", {})
        thresholds = {"eti":3.2, "ri":3.2, "subjective":3.5, "phase":3.2, "layer":3.2, "emotion":3.0}
        weakest_area = "balance"; min_score = 5.0

        for key, threshold_val in thresholds.items():
            # Map general key to specific aggregated score key if necessary
            # (Current agg_scores keys match these general keys)
            score = agg_scores.get(key, 5.0)
            if isinstance(score, (int,float)) and score < threshold_val and score < min_score:
                min_score = score; weakest_area = key
        
        self.logger.debug(f"フォールバック: 最弱エリア「{weakest_area}」(スコア: {min_score:.2f})")
        
        instructions_map = {
            "eti": "指示生成失敗。ゴシック性（ETI）の強化が必要です。\n- 二項対立の境界を曖昧にする表現（光と闇、現実と夢など）を増やしてください。\n- 象徴的な語彙やメタファーを効果的に使用してください。",
            "ri": "指示生成失敗。読みやすさ（RI）の改善が必要です。\n- 文の長さに変化をつけ、一文が長くなりすぎないようにしてください。\n- 適切な段落分けを行い、視覚的なリズムを整えてください。",
            "subjective": "指示生成失敗。主観的語りの深化が必要です。\n- 一人称視点を強化し、内面の思考や感情をより豊かに描写してください。\n- モノローグ（内的独白）を効果的に使用してください。",
            "phase": "指示生成失敗。位相移行の改善が必要です。\n- セリフ、実況、モノローグ、ナレーション等の位相のバランスを見直してください。\n- 位相間の移行が自然になるように、接続表現や描写を工夫してください。",
            "layer": "指示生成失敗。層バランスの改善が必要です。\n- 物質層・感覚層・心理層・象徴層の描写バランスを確認してください。\n- 不足している層の描写を追加してください。",
            "emotion": "指示生成失敗。感情変容の描写改善が必要です。\n- 物語を通じた感情の変化がより明確に伝わるように表現を調整してください。\n- 感情が変化するきっかけや内的な葛藤を描写してください。"
        }
        instructions = instructions_map.get(weakest_area, self.config.DEFAULT_FALLBACK_IMPROVEMENT)
        instructions += "\n\n語彙例: " + self.vocab_manager.get_vocabulary_for_prompt(count=10, layer=weakest_area if weakest_area in self.vocab_manager.VALID_LAYERS else None)
        return instructions.strip()

    def _generate_distribution_analysis_text(self, dist_data: Optional[Dict[str, Any]], dist_type: str) -> str:
        """Generates a brief text summary of a distribution dictionary."""
        if not dist_data or not isinstance(dist_data, dict):
            return f"{dist_type}分布データなし。"
        # Sort by value descending, filter for valid numeric ratios
        valid_dist_items = [(k,v) for k,v in dist_data.items() if isinstance(v, (int,float)) and v > 0.001]
        if not valid_dist_items:
            return f"{dist_type}分布: 有効な要素なし"
            
        sorted_dist = sorted(valid_dist_items, key=lambda item: item[1], reverse=True)
        parts = [f"{get_metric_display_name(k)} ({v*100:.1f}%)" for k, v in sorted_dist if v > 0.02] # Show > 2%
        if not parts: return f"{dist_type}分布: 主要要素なし（2%超）"
        return f"{dist_type}分布 - 主な要素: {', '.join(parts[:3])}" # Show top 3

# =============================================================================
# Part 13 End: TextProcessor Improvement Loop and Instruction Generation
# =============================================================================
# =============================================================================
# Part 14: TextProcessor Instruction Templates and Text Generation Method
# =============================================================================

class TextProcessor:
    # --- __init__ and other methods from Part 11, 12, 13 ---
    # (Assuming these are defined above in the actual combined script)
    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        ndgs_parser: Optional[Any] = None
    ):
        self.logger = logging.getLogger("NGGS-Lite.TextProcessor")
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TextProcessor requires a valid NGGSConfig instance.")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TextProcessor requires a valid LLMClient instance.")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TextProcessor requires a valid base Evaluator instance.")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TextProcessor requires a valid VocabularyManager instance.")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TextProcessor requires a valid NarrativeFlowFramework instance.")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TextProcessor requires a valid ExtendedETIEvaluator instance.")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TextProcessor requires a valid RIEvaluator instance.")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TextProcessor requires a valid SubjectiveEvaluator instance.")
        if ndgs_parser is not None and not hasattr(ndgs_parser, 'parse'):
            self.logger.warning("Provided ndgs_parser object may lack a 'parse' method.")

        self.config = config
        self.llm = llm_client
        self.evaluator = evaluator
        self.vocab_manager = vocab_manager
        self.narrative_flow = narrative_flow
        self.eti_evaluator = eti_evaluator
        self.ri_evaluator = ri_evaluator
        self.subjective_evaluator = subjective_evaluator
        self.ndgs_parser = ndgs_parser

        gen_template_res = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_res.is_err:
            self.logger.critical(f"Invalid generation_template: {gen_template_res.error}")
            raise gen_template_res.error # type: ignore
        self.generation_template = generation_template

        imp_template_res = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_res.is_err:
            self.logger.critical(f"Invalid improvement_template: {imp_template_res.error}")
            raise imp_template_res.error # type: ignore
        self.improvement_template = improvement_template

        self.logger.info(
            f"TextProcessor initialized (v{config.VERSION}). Engine: {self.llm.current_engine}, "
            f"Max Loops: {config.DEFAULT_MAX_LOOPS}, Threshold: {config.DEFAULT_IMPROVEMENT_THRESHOLD:.1f}"
        )

    # --- _create_*_template methods (Full Implementations for Part 14) ---

    def _create_gothic_template(
        self,
        eti_result: EvaluationResult,
        agg_scores: Dict[str, float],
        phase_distribution: Dict[str, float],
        colloquial_level: Optional[str]
    ) -> Result[str, TemplateError]:
        """ゴシック性（ETI）強化に焦点を当てた改善指示を生成します。"""
        self.logger.debug("ゴシック性強化テンプレートを生成中...")
        if not isinstance(eti_result, EvaluationResult) or not isinstance(eti_result.scores, dict):
            return Result.fail(TemplateError("ETI評価結果のスコアデータが不正です。"))

        instructions: List[str] = ["# ゴシック性（ETI）強化指示"]
        eti_scores = eti_result.scores
        
        # ETI構成要素のスコアを取得し、低いものを特定
        component_scores = {
            k: v for k, v in eti_scores.items()
            if k in self.config.EXTENDED_ETI_WEIGHTS and isinstance(v, (int, float))
        }
        weak_points = sorted(
            [(k, v) for k, v in component_scores.items() if v < 3.5], # 3.5未満を改善点とする
            key=lambda item: item[1]
        )[:2] # 上位2つの弱点を取得

        if weak_points:
            instructions.append("\n## 特に強化すべきETI要素:")
            for point_key, score_val in weak_points:
                disp_name = get_metric_display_name(point_key)
                guidance_map = {
                    "境界性": "光と闇、生と死、内と外といった二項対立の境界を曖昧にし、その狭間を描写してください。",
                    "両義性": "相反する感情（例：魅惑と恐怖、崇高とグロテスク）が共存するような両義的な描写を加えてください。",
                    "超越侵犯": "日常の規範や現実の限界を超えるような表現（狂気、異形、禁忌のテーマなど）を導入または強化してください。",
                    "不確定性": "明確な解釈を避け、多義的で謎めいた余韻を残すように、暗示や間接的表現を増やしてください。",
                    "内的変容": "登場人物の認識、記憶、自己同一性が揺らぎ、変容していく過程をより深く描写してください。",
                    "位相移行": "ゴシック的雰囲気を高める形で、語りの位相（特にモノローグや象徴的なナレーションへ）の移行を工夫してください。",
                    "主観性": "主観的な恐怖、不安、混乱、あるいは特異な感覚体験を、より鮮烈に、読者が追体験できるように描写してください。"
                }
                instructions.append(f"- **{disp_name} (現在スコア: {score_val:.1f})**: {guidance_map.get(point_key, f'{disp_name}に関連する表現を強化してください。')}")
                vocab_sugg = self.vocab_manager.get_vocabulary_by_eti_category(point_key, count=3)
                if vocab_sugg:
                    instructions.append(f"  - 推奨語彙例: {vocab_sugg}")
        else:
            instructions.append("\n- ETIの各要素は比較的良好です。現在の質を維持しつつ、特に「両義性」や「不確定性」の表現をさらに洗練させてください。")

        # 層バランスに関する指示
        layer_score = agg_scores.get("layer", 3.0) # layer_balance_score
        if layer_score < 3.2:
            instructions.append("\n## 層バランスの調整:")
            instructions.append("- ゴシック的雰囲気を深化させるため、特に「象徴層」と「心理層」の描写を増やし、「感覚層」と効果的に連携させてください。")
            instructions.append("  - 例: 古びた「物（物質層）」が引き起こす「不気味な感覚（感覚層）」、それが呼び覚ます「過去のトラウマ（心理層）」、そしてその物が持つ「呪われた運命（象徴層）」。")

        # 口語レベルに応じた文体調整
        safe_cl_level = colloquial_level or "medium"
        if safe_cl_level != "low": # 口語レベルが「低」でない場合
            instructions.append("\n## 文体調整:")
            instructions.append(f"- 現在の口語レベル（{safe_cl_level}）を考慮しつつ、特にナレーションや内省的な部分で、より格調高い文語表現やゴシック特有の荘重な言い回しを試みてください。")

        instructions.append("\n## 推奨ゴシック語彙:")
        instructions.append("- 以下の語彙も全体を通して文脈に合わせて効果的に使用し、ゴシック的な世界観を構築してください。")
        instructions.append(f"  - {self.vocab_manager.get_vocabulary_for_prompt(count=12, category='神秘', layer='symbolic') or '（推奨語彙なし）'}")
        
        return Result.ok("\n".join(instructions))

    def _create_readability_template(
        self,
        ri_result: EvaluationResult,
        agg_scores: Dict[str, float],
        phase_distribution: Dict[str, float],
        colloquial_level: Optional[str]
    ) -> Result[str, TemplateError]:
        """可読性（RI）向上に焦点を当てた改善指示を生成します。"""
        self.logger.debug("可読性向上テンプレートを生成中...")
        if not isinstance(ri_result, EvaluationResult) or not isinstance(ri_result.scores, dict):
            return Result.fail(TemplateError("RI評価結果のスコアデータが不正です。"))

        instructions: List[str] = ["# 可読性（RI）向上指示"]
        ri_scores = ri_result.scores
        
        component_scores = { # RIの主要構成要素
            k: v for k, v in ri_scores.items()
            if k in ["clarity", "visualRhythm", "emotionalFlow", "cognitiveLoad", "interpretiveResonance"] and isinstance(v, (int, float))
        }
        weak_points = sorted(
            [(k, v) for k, v in component_scores.items() if v < 3.2],
            key=lambda item: item[1]
        )[:2]

        if weak_points:
            instructions.append("\n## 特に改善すべき読みやすさの要素:")
            for point_key, score_val in weak_points:
                disp_name = get_metric_display_name(point_key)
                guidance_map = {
                    "clarity": "一文をより短くするか、接続詞（例：「しかし」「そのため」「なぜなら」）を効果的に用いて文と文の関係を明確にし、複雑な修飾関係を整理してください。",
                    "visualRhythm": "段落の長さに変化をつけ、適度な段落分けを行ってください。短い文と長い文をリズミカルに組み合わせることも検討してください。",
                    "emotionalFlow": "登場人物の感情の変化や物語の雰囲気の推移が、より自然で段階的に感じられるように、描写の順序や言葉選びを調整してください。唐突な感情の飛躍を避けましょう。",
                    "cognitiveLoad": "専門用語や非常に難解な漢字・語彙の使用は控えめにし、読者が文脈から意味を推測しやすい平易な言葉遣いを心がけてください。ただし、ゴシック文体の雰囲気は損なわないように。",
                    "interpretiveResonance": "象徴的な表現や暗示は維持しつつも、読者がその意味合いや背景にあるものをある程度推測できるような、文脈上の手がかりを僅かに加えてください。"
                }
                instructions.append(f"- **{disp_name} (現在スコア: {score_val:.1f})**: {guidance_map.get(point_key, f'{disp_name}に関連する改善を行ってください。')}")
        else:
            instructions.append("\n- 読みやすさの各要素は比較的良好です。引き続き、文間の論理的な繋がりや感情・雰囲気の自然な流れを意識し、読者がスムーズに物語世界に没入できるようにしてください。")

        # 位相バランスに関する指示
        phase_score = agg_scores.get("phase", 3.0) # phase_score
        if phase_score < 3.2:
            instructions.append("\n## 位相バランスの調整:")
            instructions.append("- セリフ、実況、モノローグ、ナレーションといった語りの位相が、物語全体でバランス良く出現するように調整してください。特定の位相に偏りすぎると単調な印象を与える可能性があります。")
            current_phase_dist_str = self._generate_distribution_analysis_text(phase_distribution, "現在の位相")
            instructions.append(f"  - {current_phase_dist_str}")


        # 口語レベルに応じた調整
        safe_cl_level = colloquial_level or "medium"
        instructions.append("\n## 口語レベルの調整:")
        instructions.append(f"- 設定された口語レベル「{safe_cl_level}」を意識してください。")
        if safe_cl_level == "high":
            instructions.append("  - ゴシック的な重厚な雰囲気は維持しつつも、現代の読者にとってより自然で理解しやすい、やや砕けた言い回しや表現を適度に増やすことを検討してください。")
        elif safe_cl_level == "low":
            instructions.append("  - 格調高い文語調や古風な表現を主としつつも、読者が完全に理解できなくなるほど難解すぎる表現や、過度に冗長な言い回しは避けてください。文脈によっては、重要な情報を伝える部分でわずかに平易さを加えることも有効です。")
        else: # medium
            instructions.append("  - 口語的な表現と文語的な表現のバランスを保ち、場面や語り手に応じてトーンを使い分けてください。極端な偏りを避け、自然な範囲での融合を目指してください。")

        instructions.append("\n## 推奨語彙（読みやすさ考慮）:")
        instructions.append("- 読みやすさを意識しつつ、物語の雰囲気を損なわない範囲で、以下の語彙も適切に取り入れてみてください。")
        instructions.append(f"  - {self.vocab_manager.get_vocabulary_for_prompt(count=10, category='自然') or '（推奨語彙なし）'}") # Example: natural, often simpler words

        return Result.ok("\n".join(instructions))

    def _create_subjective_template(
        self,
        subj_result: EvaluationResult,
        phase_distribution: Dict[str, float],
        perspective_mode: Optional[str]
    ) -> Result[str, TemplateError]:
        """主観的ナレーション深化に焦点を当てた改善指示を生成します。"""
        self.logger.debug("主観性深化テンプレートを生成中...")
        if not isinstance(subj_result, EvaluationResult) or not isinstance(subj_result.components, dict):
            return Result.fail(TemplateError("主観性評価結果のコンポーネントデータが不正です。"))

        instructions: List[str] = ["# 主観的語り（没入感）深化指示"]
        components = subj_result.components
        safe_perspective = perspective_mode or "subjective_first_person"

        fp_score = components.get("first_person_score", 5.0)
        if fp_score < 3.5 and "first_person" in safe_perspective : # 1人称視点の場合のみ指摘
            instructions.append(f"\n## 一人称視点の強化 (現在スコア: {fp_score:.1f}):")
            instructions.append("- 「私」（あるいは設定された一人称代名詞）の視点からの描写、思考、感情の表出を増やしてください。物語が「私」の体験を通して語られていることをより明確にしましょう。")

        inner_score = components.get("inner_expression_score", 5.0)
        inner_diversity = components.get("inner_expression_diversity", 0.0) # This key might be in analysis_metrics
        if inner_score < 3.8:
            instructions.append(f"\n## 内的表現の深化 (現在スコア: {inner_score:.1f}, 多様性: {inner_diversity:.2f}):")
            instructions.append("- 登場人物の思考、感情（喜び、悲しみ、怒り、恐怖など）、感覚（五感）、記憶、予感、直感などを、より具体的かつ豊かに描写してください。")
            instructions.append("- 特に、内的表現の「多様性」を高めることを意識し、同じような感情語の繰り返しを避け、様々な角度から内面を描写してください。")
            vocab_sugg = self.vocab_manager.get_vocabulary_for_prompt(count=5, layer="psychological", category="内的変容")
            if vocab_sugg: instructions.append(f"  - 推奨語彙例（心理・変容）: {vocab_sugg}")


        mono_score = components.get("monologue_quality_score", 5.0)
        mono_count = components.get("monologue_count", 0) # This key might be in analysis_metrics
        if mono_score < 3.5:
            instructions.append(f"\n## モノローグ（内的独白）の質の向上 (現在スコア: {mono_score:.1f}, 現在数: {mono_count}):")
            instructions.append("- 内的独白をより効果的に使用し、登場人物の葛藤、疑問、決意などを表現してください。")
            instructions.append("- 独白の長さに変化をつけ、時には短いフレーズで、時には詳細な内省で、思考の深さや揺らぎを表現しましょう。省略記号「…」や疑問形「～だろうか？」の活用も有効です。")

        # 位相分布に基づく指示
        monologue_ratio = phase_distribution.get("monologue", 0.0)
        serif_ratio = phase_distribution.get("serif", 0.0) + phase_distribution.get("serif_prime", 0.0)
        if monologue_ratio < 0.20 or (serif_ratio < 0.10 and "first_person" in safe_perspective): # 独白が少ない、または一人称でセリフが少ない
            instructions.append("\n## 主観的位相の活用:")
            if monologue_ratio < 0.20:
                instructions.append(f"- モノローグの割合（現在 {monologue_ratio*100:.1f}%）を増やし、登場人物の内面世界をより直接的に読者に提示してください。")
            if serif_ratio < 0.10 and "first_person" in safe_perspective:
                 instructions.append(f"- 一人称視点の場合、セリフの割合（現在 {serif_ratio*100:.1f}%）を適度に増やし、主人公の言葉を通して感情や性格を表現することも有効です。")


        consistency_score = components.get("consistency_score", 5.0)
        if consistency_score < 4.0: # やや厳しめにチェック
            consistency_warning = components.get("consistency_warning", "")
            instructions.append(f"\n## 視点の一貫性確保 (現在スコア: {consistency_score:.1f}):")
            instructions.append(f"- {consistency_warning if consistency_warning else '視点の混在が見られる可能性があります。'}")
            instructions.append(f"  - 設定された視点（{safe_perspective}）に物語全体を統一するか、意図的な視点シフトである場合はその効果や境界が読者に明確に伝わるように描写を工夫してください。")
        
        if not weak_points and monologue_ratio >= 0.20 and (serif_ratio >= 0.10 or "first_person" not in safe_perspective) and consistency_score >=4.0 :
             instructions.append("\n- 主観描写は全体的に良好です。さらに、登場人物の五感を通じた世界の知覚（感覚層）と、それに対する内的な反応（心理層）の連携を深め、読者がより強く物語世界に没入し、登場人物と一体化して体験できるような描写を目指してください。")

        instructions.append("\n## 推奨語彙（主観表現）:")
        instructions.append("- 主観的な思考や感情表現を豊かにするために、以下の語彙も文脈に応じて活用してください。")
        instructions.append(f"  - {self.vocab_manager.get_vocabulary_for_prompt(count=10, layer='psychological') or '感じる、思う、見える、聞こえる、気がする、～ようだ、記憶、夢、不安'}")
        return Result.ok("\n".join(instructions))

    def _create_phase_template(
        self,
        phase_distribution: Dict[str, float],
        phase_focus: Optional[str]
    ) -> Result[str, TemplateError]:
        """物語の位相バランスと移行の自然さ改善に焦点を当てた指示を生成します。"""
        self.logger.debug("位相バランス・移行改善テンプレートを生成中...")
        if not phase_distribution or not isinstance(phase_distribution, dict):
            return Result.fail(TemplateError("位相分布データが提供されていないか、形式が不正です。"))

        instructions: List[str] = ["# 位相バランスと移行の改善指示"]
        safe_phase_focus = phase_focus or "balanced"

        # 不足している位相、過剰な位相を特定
        target_ratios = self.config.PHASE_BALANCE_TARGETS
        tolerance = self.config.PHASE_DEVIATION_TOLERANCE
        
        missing_phases_info: List[str] = []
        over_phases_info: List[str] = []

        for phase_key, target_ratio in target_ratios.items():
            current_ratio = phase_distribution.get(phase_key, 0.0)
            if current_ratio < target_ratio - tolerance:
                missing_phases_info.append(f"{get_metric_display_name(phase_key)} (目標:{target_ratio*100:.0f}%, 現状:{current_ratio*100:.1f}%)")
            elif current_ratio > target_ratio + tolerance:
                over_phases_info.append(f"{get_metric_display_name(phase_key)} (目標:{target_ratio*100:.0f}%, 現状:{current_ratio*100:.1f}%)")

        if missing_phases_info:
            instructions.append("\n## 不足している位相の追加:")
            instructions.append(f"- 以下の位相の描写が目標より不足しています: {', '.join(missing_phases_info)}。これらの位相のシーンや要素を物語に追加・増量してください。")
        if over_phases_info:
            instructions.append("\n## 過剰な位相の調整:")
            instructions.append(f"- 以下の位相の描写が目標より過剰です: {', '.join(over_phases_info)}。これらの位相の分量を減らし、他の位相とのバランスを取るように調整してください。")
        
        if not missing_phases_info and not over_phases_info:
            instructions.append("\n- **位相バランス**: 現在の位相構成は比較的バランスが取れています。このバランスを維持しつつ、物語の展開に合わせて各位相の役割が最大限に活かされるよう微調整してください。")

        instructions.append("\n## 位相移行の自然化:")
        instructions.append("- 位相間の移行（例：ナレーション→セリフ、実況→モノローグ）がより滑らかで自然に感じられるように工夫してください。")
        instructions.append("  - 移行のきっかけとなる描写（情景、音、登場人物の行動や表情など）を挟む。")
        instructions.append("  - 共通のモチーフやキーワードを異なる位相間で用いて繋がりを持たせる。")
        instructions.append("  - 感覚描写（視覚、聴覚など）をブリッジとして利用する。")

        if safe_phase_focus != "balanced":
            focus_desc = get_metric_display_name(safe_phase_focus)
            current_focus_ratio = phase_distribution.get(safe_phase_focus, 0.0)
            instructions.append(f"\n## 重点位相「{focus_desc}」の強化 (目標比率: {self.config.PHASE_SCORE_FOCUS_TARGET_RATIO*100:.0f}%):")
            instructions.append(f"- 設定された重点位相である「{focus_desc}」の描写を増やし、物語におけるその役割をより際立たせてください (現在の割合: {current_focus_ratio*100:.1f}%)。")
        
        return Result.ok("\n".join(instructions))

    def _create_layer_template(
        self,
        layer_distribution: Dict[str, float],
        layer_score: float # Current aggregated layer_balance_score
    ) -> Result[str, TemplateError]:
        """四層構造（物質・感覚・心理・象徴）のバランス改善に焦点を当てた指示を生成します。"""
        self.logger.debug("層バランス改善テンプレートを生成中...")
        if not layer_distribution or not isinstance(layer_distribution, dict):
            return Result.fail(TemplateError("層分布データが提供されていないか、形式が不正です。"))

        instructions: List[str] = [f"# 四層構造バランス改善指示 (現在スコア: {layer_score:.1f})"]
        
        target_ratios = self.config.LAYER_BALANCE_TARGETS
        tolerance = self.config.LAYER_DEVIATION_TOLERANCE
        under_represented_layers: List[str] = []
        over_represented_layers: List[str] = []

        for layer_key, target_ratio in target_ratios.items():
            current_ratio = layer_distribution.get(layer_key, 0.0)
            if current_ratio < target_ratio - tolerance:
                under_represented_layers.append(f"{get_metric_display_name(layer_key)} (目標:{target_ratio*100:.0f}%, 現状:{current_ratio*100:.1f}%)")
            elif current_ratio > target_ratio + tolerance:
                over_represented_layers.append(f"{get_metric_display_name(layer_key)} (目標:{target_ratio*100:.0f}%, 現状:{current_ratio*100:.1f}%)")

        if under_represented_layers:
            instructions.append("\n## 不足している層の強化:")
            instructions.append(f"- 以下の層の描写が目標より不足しています: {', '.join(under_represented_layers)}。これらの層の描写を意識的に増やしてください。")
            guidance_map = {
                "physical": "  - **物質層**: 物語の舞台となる場所、具体的な物体、登場人物の身体的特徴や服装、行動などをより詳細に描写してください。",
                "sensory": "  - **感覚層**: 登場人物が五感（視覚、聴覚、嗅覚、味覚、触覚）を通じて体験する世界を鮮やかに描写してください。光と影、音、匂い、温度、手触りなど。",
                "psychological": "  - **心理層**: 登場人物の内面世界（思考、感情、記憶、願望、葛藤、無意識など）を深く掘り下げて描写してください。",
                "symbolic": "  - **象徴層**: 物語に深みと多義性を与えるために、比喩、暗示、象徴的なモチーフや出来事を効果的に使用してください。個々の事物が持つ象徴的な意味合いを意識しましょう。"
            }
            for layer_info_str in under_represented_layers:
                layer_name_internal = layer_info_str.split(" ")[0] # Extract internal key if needed, or map from display name
                # This mapping is brittle, better to use internal keys directly if possible
                internal_key_map = {v:k for k,v in {"physical":"物質層", "sensory":"感覚層", "psychological":"心理層", "symbolic":"象徴層"}.items()}
                internal_key = next((k for k,v in internal_key_map.items() if v == layer_name_internal), None)
                if internal_key and internal_key in guidance_map:
                     instructions.append(guidance_map[internal_key])

        if over_represented_layers:
            instructions.append("\n## 過剰な層の調整:")
            instructions.append(f"- 以下の層の描写が目標より過剰です: {', '.join(over_represented_layers)}。これらの層の描写を少し抑え、他の層とのバランスを取るように調整してください。")

        if not under_represented_layers and not over_represented_layers:
            instructions.append("\n- **層バランスの維持と洗練**: 現在の層のバランスは良好です。このバランスを維持しつつ、各層の描写の質をさらに高め、それらの連携をより有機的にしてください。")

        instructions.append("\n## 層間の連携強化:")
        instructions.append("- 各層の描写が孤立せず、互いに関連し合い、影響を与え合うように工夫してください。")
        instructions.append("  - 例: 「古びた屋敷（物質層）」の「かび臭い匂い（感覚層）」が主人公に「過去の陰鬱な記憶（心理層）」を呼び覚まし、その記憶が「一族の呪われた運命（象徴層）」を暗示する、など。")
        
        return Result.ok("\n".join(instructions))

    def _create_emotion_template(
        self,
        emotion_score: float, # Current aggregated emotion_arc_score
        emotion_arc: Optional[str]
    ) -> Result[str, TemplateError]:
        """物語を通じた感情変容の質向上に焦点を当てた指示を生成します。"""
        self.logger.debug("感情変容改善テンプレートを生成中...")
        instructions: List[str] = [f"# 感情変容の質向上指示 (現在スコア: {emotion_score:.1f})"]

        if not emotion_arc:
            instructions.append("\n- **感情の弧の設計**: 現在、明確な「感情の弧」が設定されていません。物語全体を通じて登場人物（特に視点人物）の感情がどのように変化していくか（例：不安→恐怖→絶望→微かな希望）の設計を検討し、それを意識して描写してください。")
            instructions.append("- もし感情の弧を設定しない場合は、物語のテーマや雰囲気に一貫した感情のトーンを維持するか、あるいは出来事に応じて自然で説得力のある感情の起伏を描写してください。")
        else:
            instructions.append(f"\n- **目標とする感情の弧**: {emotion_arc}")
            arc_steps = [step.strip() for step in emotion_arc.split("->")]
            if emotion_score < 3.5: # スコアが低い場合の具体的な指示
                instructions.append("\n## 具体的な改善点:")
                instructions.append("1. **感情変化のきっかけの明確化**: 登場人物の感情が次の段階へ移行する「きっかけ」となる出来事、発見、他者との相互作用、あるいは内的な気づきを、より明確かつ印象的に描写してください。")
                instructions.append("2. **感情の段階的・多面的描写**: 感情の変化を一足飛びに描くのではなく、徐々に深まったり、あるいは複数の感情が混在・葛藤したりしながら移り変わっていく様子を丁寧に描写してください。行動、表情、言葉遣い、思考、身体的反応など、多角的な描写を心がけましょう。")
                if len(arc_steps) > 1:
                    instructions.append(f"3. **「{arc_steps[0]}」から「{arc_steps[1]}」への移行の強化**: 特に物語序盤の感情変化を丁寧に。")
                if len(arc_steps) > 2:
                    instructions.append(f"4. **「{arc_steps[-2]}」から「{arc_steps[-1]}」への変容点の強調**: 物語終盤の感情の到達点とその変容の過程を、クライマックスとして力強く描写してください。")
            else: # スコアが比較的良い場合
                instructions.append("\n- **感情描写のさらなる洗練**: 現在の感情描写は良好です。さらに感情の機微や複雑さ（例: 表情には出さないが内心では激しく動揺している、喜びと悲しみが同時に存在するなど）を描写することで、登場人物の人間的な深みを増してください。")

        instructions.append("\n## 推奨語彙（感情表現）:")
        instructions.append("- 感情表現を豊かにするために、以下の語彙も文脈に応じて効果的に使用してください。")
        # カテゴリ「両義性」は相反する感情の語彙を含む可能性がある
        instructions.append(f"  - {self.vocab_manager.get_vocabulary_for_prompt(count=8, category='両義性', layer='psychological') or '（推奨語彙なし）'}")
        return Result.ok("\n".join(instructions))

    def _create_colloquial_template(
        self,
        colloquial_level: Optional[str],
        current_score: float
    ) -> Result[str, TemplateError]:
        """口語表現とゴシック文体の融合度調整に関する指示を生成します。"""
        self.logger.debug("口語性調整テンプレートを生成中...")
        safe_cl_level = colloquial_level or "medium"
        instructions: List[str] = [f"# 口語性とゴシック文体の融合度調整指示 (目標レベル: {safe_cl_level}, 現在スコア: {current_score:.1f})"]

        if current_score < 3.0: # スコアが低い場合、より具体的な調整を促す
            instructions.append(f"\n## 「{safe_cl_level}」レベルへの調整:")
            if safe_cl_level == "high":
                instructions.append("- 現在の文体は目標とする「高」レベルの口語性に対して、やや硬すぎるか、古風すぎる可能性があります。")
                instructions.append("  - セリフや主人公の思考を中心に、より現代的で自然な言い回しや砕けた表現を増やしてください。ただし、ゴシック文体の持つ独特の雰囲気や美意識は損なわないように注意が必要です。")
            elif safe_cl_level == "low":
                instructions.append("- 現在の文体は目標とする「低」レベルの口語性（つまり、より文語的・荘重）に対して、やや口語的すぎるか、軽すぎる可能性があります。")
                instructions.append("  - 特にナレーションや描写部分で、より格調高い文語表現、古風な言い回し、あるいは漢語調の言葉を増やし、文全体の荘重さを高めてください。")
            else: # medium
                instructions.append("- 現在の口語と文語のバランスが、目標とする「中」レベルから偏っている可能性があります。")
                instructions.append("  - 極端に口語的な部分や、逆に極端に文語的な部分がないか見直し、より自然な範囲での融合を目指してください。場面や語り手の性格に応じて、両者のバランスを柔軟に調整することが求められます。")
        else: # スコアが比較的良い場合
            instructions.append(f"\n## 「{safe_cl_level}」レベルの維持と洗練:")
            instructions.append("- 現在の口語とゴシック文体のバランスは目標レベルに比較的近いです。このバランスを維持しつつ、さらに洗練させてください。")

        instructions.append("\n## 層と位相による使い分けの意識:")
        instructions.append("- 描写の層（物質・感覚・心理・象徴）や語りの位相（セリフ・実況・モノローグ・ナレーション）に応じて、口語と文語の度合いを意識的に使い分けることで、より効果的な表現が可能です。")
        instructions.append("  - 例：セリフは比較的口語的に、象徴層の描写や荘厳なナレーションは文語的に、など。")

        instructions.append("\n## コントラストの活用（上級）:")
        instructions.append("- 日常的な口語表現が支配する中に、突如として非日常的で荘重なゴシック的表現が現れる、といったギャップ（コントラスト）を効果的に利用することで、読者に強い印象を与え、文体に深みを与えることができます。")
        
        return Result.ok("\n".join(instructions))

    def _create_balance_template(
        self,
        last_eval_data: JsonDict, # Contains aggregated_scores and other results
        perspective_mode: Optional[str],
        colloquial_level: Optional[str]
    ) -> Result[str, TemplateError]:
        """全体的なバランス調整と洗練のための指示を生成します。"""
        self.logger.debug("全体バランス調整テンプレートを生成中...")
        instructions: List[str] = ["# 全体バランス調整と総合的品質向上指示"]

        agg_scores = last_eval_data.get("aggregated_scores", {})
        safe_perspective = perspective_mode or "subjective_first_person"
        safe_cl_level = colloquial_level or "medium"

        # 主要な評価項目とその閾値（これを下回ると改善対象としてリストアップ）
        # スコアキーは aggregated_scores のキーと一致させる
        thresholds: Dict[str, float] = {
            "eti": 3.2, "ri": 3.2, "subjective": 3.5,
            "phase": 3.2, "layer": 3.2, "emotion": 3.0, "colloquial": 3.0,
            "gothic_atmosphere": 3.0, "stylistic_gravity": 3.0
        }
        
        weak_points_info: List[str] = []
        for key, threshold_val in thresholds.items():
            score = agg_scores.get(key)
            if isinstance(score, (int, float)) and score < threshold_val:
                weak_points_info.append(f"{get_metric_display_name(key)} ({score:.1f})")
        
        if weak_points_info:
            instructions.append("\n## 重点的な改善エリア:")
            instructions.append(f"- 特にスコアが低い項目（{', '.join(weak_points_info)}）を中心に、それぞれの側面に関連する表現技法を強化・調整してください。")
            instructions.append("  - 各項目の具体的な改善指針は、それぞれの専用戦略テンプレートを参考にしてください（例：ETIが低い場合はゴシック性強化戦略）。")
        else:
            instructions.append("\n## 全体的な洗練と深化:")
            instructions.append("- 各評価項目のスコアは比較的良好です。この高い水準を維持しつつ、以下の点を意識して作品全体の質をさらに高めてください。")

        instructions.append("\n## 要素間の連携強化:")
        instructions.append("- ETI（ゴシック性）、主観性、層構造、位相移行、感情描写といった各要素が、それぞれ独立して機能するだけでなく、互いに緊密に連携し合い、相乗効果を生み出すように意識してください。")
        instructions.append("  - 例: ある「象徴的（層）」なアイテムが、「不気味な雰囲気（ETI）」を醸し出し、主人公の「内的葛藤（主観性・心理層）」を引き出し、それが「モノローグ（位相）」として語られる、など。")

        instructions.append("\n## 文体の一貫性と多様性:")
        instructions.append(f"- 設定された口語レベル「{safe_cl_level}」と視点「{safe_perspective}」を基調としつつも、物語の展開や場面の雰囲気に合わせて、文体に適切な変化と多様性を持たせてください。単調さを避け、読者を飽きさせない工夫が重要です。")
        
        strong_points_info: List[str] = []
        for key, score_val in agg_scores.items():
            if isinstance(score_val, (int, float)) and score_val >= 4.0:
                 strong_points_info.append(f"{get_metric_display_name(key)} ({score_val:.1f})")
        if strong_points_info:
            instructions.append("\n## 維持・発展させるべき強み:")
            instructions.append(f"- 以下の項目は高評価です: {', '.join(strong_points_info)}。これらの長所を維持し、さらに発展させてください。")

        instructions.append("\n## 推奨総合語彙:")
        instructions.append("- 物語全体の質を高めるために、以下の語彙も状況に応じて適切に活用し、特に描写の層と語りの位相に合わせて効果的に配置してください。")
        instructions.append(f"  - {self.vocab_manager.get_vocabulary_for_prompt(count=15) or '（推奨語彙なし）'}") # General purpose vocab
        
        return Result.ok("\n".join(instructions))

    # --- Text Generation Method (Full Implementation for Part 14) ---
    def _generate_text(
        self,
        input_text_or_context: str, # For initial: context; for improvement: previous best text
        target_length: int,
        perspective_mode: str,
        phase_focus: str,
        colloquial_level: str,
        emotion_arc: Optional[str],
        narrative_flow_section: Optional[str], # For initial generation
        is_improvement: bool,
        # Arguments specific to improvement
        evaluation_results_json: Optional[Union[JsonDict, EvaluationResult]] = None, # Can be dict or EvaluationResult
        low_score_items_str: Optional[str] = None,
        high_score_items_str: Optional[str] = None,
        layer_distribution_analysis: Optional[str] = None,
        phase_distribution_analysis: Optional[str] = None,
        improvement_section_content: Optional[str] = None, # The actual instruction text
        temperature: Optional[float] = None
    ) -> Result[str, LLMError]:
        """
        Core text generation method. Formats the appropriate prompt and calls the LLM.
        Handles both initial generation and improvement generation.
        """
        self.logger.info(
            f"テキスト生成実行中... タイプ: {'改善' if is_improvement else '初期'}, "
            f"目標長: {target_length}"
        )

        final_prompt: str
        template_to_use: str
        prompt_params: Dict[str, Any]

        if is_improvement:
            template_to_use = self.improvement_template
            
            eval_json_str = "{}"
            if isinstance(evaluation_results_json, EvaluationResult):
                # If it's an EvaluationResult object, serialize its relevant parts
                # This assumes llm_eval_result was stored as the object itself.
                # For the prompt, we typically want a summary.
                scores_for_prompt = {
                    "llm_scores": getattr(evaluation_results_json, 'scores', {}),
                    "llm_reasons": {k: truncate_text(v, 80) for k,v in getattr(evaluation_results_json, 'reasoning', {}).items()},
                    # Add other relevant parts if needed, e.g., aggregated scores from the full eval data
                }
                try:
                    eval_json_str = json.dumps(scores_for_prompt, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
                except Exception as e:
                    self.logger.warning(f"改善プロンプト用評価結果のJSON化失敗: {e}")
                    eval_json_str = json.dumps({"error": "failed to serialize eval"}, ensure_ascii=False)
            elif isinstance(evaluation_results_json, dict): # If it's already a dict (e.g. from older versions)
                try:
                    eval_json_str = json.dumps(evaluation_results_json, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
                except Exception as e:
                    self.logger.warning(f"改善プロンプト用評価辞書のJSON化失敗: {e}")
                    eval_json_str = json.dumps({"error": "failed to serialize eval dict"}, ensure_ascii=False)


            prompt_params = {
                "original_text": truncate_text(input_text_or_context, 800), # Previous best text
                "evaluation_results_json": eval_json_str,
                "low_score_items_str": low_score_items_str or "特になし",
                "high_score_items_str": high_score_items_str or "特になし",
                "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=20), # General vocab for improvement
                "perspective_mode": perspective_mode,
                "phase_focus": phase_focus,
                "colloquial_level": colloquial_level,
                "emotion_arc": emotion_arc or "指定なし",
                "layer_distribution_analysis": layer_distribution_analysis or "（層分布分析なし）",
                "phase_distribution_analysis": phase_distribution_analysis or "（位相分布分析なし）",
                "improvement_section": improvement_section_content or self.config.DEFAULT_FALLBACK_IMPROVEMENT
            }
            self.logger.debug(f"改善プロンプトパラメータ準備完了。指示の長さ: {len(improvement_section_content or '')}")
        else: # Initial generation
            template_to_use = self.generation_template
            prompt_params = {
                "input_text": input_text_or_context, # Context for initial generation
                "target_length": target_length,
                "perspective_mode": perspective_mode,
                "phase_focus": phase_focus,
                "colloquial_level": colloquial_level,
                "emotion_arc": emotion_arc or "指定なし",
                "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=25),
                "narrative_flow_section": narrative_flow_section or "（物語構成の指示なし）"
                # improvement_section is not used for initial generation template
            }
            self.logger.debug("初期生成プロンプトパラメータ準備完了。")

        try:
            final_prompt = template_to_use.format_map(SafeDict(prompt_params))
        except KeyError as e:
            error_msg = f"{'改善' if is_improvement else '初期'}生成テンプレートのフォーマットエラー、キーが見つかりません: {e}"
            self.logger.error(error_msg)
            return Result.fail(TemplateError(error_msg))
        except Exception as e:
            error_msg = f"{'改善' if is_improvement else '初期'}生成テンプレートのフォーマット中に予期せぬエラー: {e}"
            self.logger.error(error_msg, exc_info=True)
            return Result.fail(TemplateError(error_msg))

        # Determine temperature
        temp_to_use: float
        if temperature is not None: # Explicit temperature override
            temp_to_use = temperature
        elif is_improvement: # Default improvement temperature (already calculated by caller)
            # This implies the caller of _perform_improvement_loop should pass the adjusted temp
            # For now, let's assume the passed `temperature` arg is the one to use for improvement.
            # If `temperature` is None here for an improvement, it's an issue.
             temp_to_use = temperature if temperature is not None else self.config.IMPROVEMENT_BASE_TEMPERATURE # Fallback
        else: # Default initial generation temperature
            temp_to_use = self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)

        self.logger.debug(f"LLM呼び出し: 温度={temp_to_use:.2f}, プロンプト長={len(final_prompt)}")
        if len(final_prompt) > 2000: # Log preview for very long prompts
             self.logger.debug(f"長文プロンプトプレビュー: {truncate_text(final_prompt, 300)}")


        llm_result = self.llm.generate(final_prompt, temperature=temp_to_use)

        if llm_result.is_ok:
            generated_text = llm_result.unwrap()
            # Optional: Basic length control (can be refined or removed if LLM handles max_output_tokens well)
            # self.logger.debug(f"LLM Raw Output Length: {len(generated_text)}")
            # if not is_improvement and len(generated_text) < target_length * 0.7:
            #     self.logger.warning(f"生成テキストが目標長 ({target_length}) より大幅に短い ({len(generated_text)})。")
            # elif len(generated_text) > target_length * 1.8: # If significantly longer
            #     self.logger.warning(f"生成テキストが目標長 ({target_length}) より大幅に長い ({len(generated_text)})。切り詰めます。")
            #     generated_text = generated_text[:int(target_length * 1.5)]

            self.logger.info(f"{'改善' if is_improvement else '初期'}テキスト生成成功。長さ: {len(generated_text)}")
            return Result.ok(generated_text)
        else:
            self.logger.error(f"{'改善' if is_improvement else '初期'}テキスト生成失敗: {llm_result.error}")
            return Result.fail(llm_result.error) # Propagate LLMError

    # Other TextProcessor methods (process, _perform_improvement_loop, etc.) from previous parts
    # are assumed to be here.

# =============================================================================
# Part 14 End: TextProcessor Instruction Templates and Text Generation Method
# =============================================================================
# =============================================================================
# Part 15: Integration Components (NDGS Parser & GLCAI Feedback) [v1.8]
# =============================================================================

# --- NDGS Integration (Basic Parser Placeholder) ---
class NDGSIntegration:
    """
    Handles basic parsing of NDGS output data (JSON format) to extract
    minimal context for NGGS-Lite v1.8.
    v1.8 Scope: Assumes file-based JSON input based on a defined (but not yet
    fully specified here) common schema. Focuses on extracting essential
    character and scene information. Error handling for basic validation.
    """
    def __init__(self, config: NGGSConfig):
        """
        Initializes the NDGS Integration component.

        Args:
            config: The NGGSConfig object.
        """
        self.config = config
        self.logger = logging.getLogger("NGGS-Lite.NDGSIntegration")
        # Placeholder for the actual JSON Schema validation object if used later
        # Example: self.schema_validator = load_schema(...)
        self.logger.info("NDGSIntegration (v1.8 Basic Parser) initialized.")

    def parse_from_file(self, file_path: Union[str, pathlib.Path]) -> Result[JsonDict, IntegrationError]:
        """
        Parses NDGS data from a specified JSON file.

        Args:
            file_path: Path to the NDGS output JSON file.

        Returns:
            Result containing a dictionary with parsed context (Ok)
            or an IntegrationError (Err).
        """
        self.logger.info(f"Attempting to parse NDGS data from file: {file_path}")
        read_result = safe_read_file(file_path)
        if read_result.is_err:
            # Ensure the error from safe_read_file is wrapped in IntegrationError
            # if it's not already one (though safe_read_file returns FileProcessingError)
            original_error = read_result.error
            err = IntegrationError(
                f"Failed to read NDGS input file '{file_path}': {original_error}",
                details={"path": str(file_path), "original_error_type": type(original_error).__name__}
            )
            self.logger.error(str(err))
            return Result.fail(err)

        try:
            ndgs_data_str = read_result.unwrap()
            if not ndgs_data_str: # Handle empty file content
                 err = IntegrationError("NDGS input file is empty.", details={"path": str(file_path)})
                 self.logger.error(str(err))
                 return Result.fail(err)

            ndgs_data = json.loads(ndgs_data_str)
            if not isinstance(ndgs_data, dict):
                err = IntegrationError(
                    "NDGS input file does not contain a valid JSON object.",
                    details={"path": str(file_path), "type": str(type(ndgs_data))}
                )
                self.logger.error(str(err))
                return Result.fail(err)

            return self.parse(ndgs_data)  # Delegate to the main parse method

        except json.JSONDecodeError as e:
            err = IntegrationError(
                f"Failed to parse NDGS JSON data from file '{file_path}': {e}",
                details={"path": str(file_path), "json_error": str(e)}
            )
            self.logger.error(str(err))
            return Result.fail(err)
        except Exception as e: # Catch any other unexpected errors
            err = IntegrationError(
                f"Unexpected error parsing NDGS file '{file_path}': {e}",
                details={"path": str(file_path)}
            )
            self.logger.error(str(err), exc_info=True)
            return Result.fail(err)

    def parse(self, ndgs_data: JsonDict) -> Result[JsonDict, IntegrationError]:
        """
        Parses the NDGS data dictionary based on the expected minimal schema (v1.8).

        Args:
            ndgs_data: The dictionary loaded from NDGS JSON output.

        Returns:
            Result containing a dictionary with extracted context suitable
            for NGGS parameters (Ok) or an IntegrationError (Err).
        """
        self.logger.debug("Parsing NDGS data dictionary...")
        parsed_context: JsonDict = {
            "characters": [],
            "scene": {},
            "initial_text": "", # Default to empty string
            "parsing_warnings": []
        }
        validation_warnings: List[str] = []

        # 1. Character Definitions
        characters_in = ndgs_data.get("character_definitions", {})
        if isinstance(characters_in, dict):
            for char_id, char_data in characters_in.items():
                if isinstance(char_data, dict):
                    char_info = {
                        "id": char_id,
                        "name": char_data.get("name", f"UnknownChar_{char_id}"),
                        "personality_summary": char_data.get("personality_summary", char_data.get("memo"))
                    }
                    # Filter out None values from personality_summary
                    if char_info["personality_summary"] is None:
                        del char_info["personality_summary"]
                    parsed_context["characters"].append(char_info)
                else:
                    validation_warnings.append(f"Invalid character data format for ID '{char_id}'. Expected dict, got {type(char_data)}.")
        elif ndgs_data.get("character_definitions") is not None: # Key exists but not a dict
             validation_warnings.append("'character_definitions' field is not a dictionary.")


        # 2. Scene Context
        scene_in = ndgs_data.get("scene_context", {})
        if isinstance(scene_in, dict):
            parsed_context["scene"] = {
                "overview": scene_in.get("scene_overview"),
                "atmosphere": scene_in.get("atmosphere")
            }
            # Filter out None values
            parsed_context["scene"] = {k: v for k, v in parsed_context["scene"].items() if v is not None}
        elif ndgs_data.get("scene_context") is not None:
            validation_warnings.append("'scene_context' field is not a dictionary.")

        # 3. Input Text / Dialogue Blocks
        input_text_source: Optional[str] = None
        if isinstance(ndgs_data.get("nggs_input_text"), str):
            input_text_source = ndgs_data.get("nggs_input_text")
            parsed_context["input_text_source_origin"] = "nggs_input_text_field"
        elif isinstance(ndgs_data.get("dialogue_blocks"), list):
            dialogue_blocks = ndgs_data.get("dialogue_blocks", [])
            if dialogue_blocks:
                first_block = dialogue_blocks[0]
                if isinstance(first_block, dict) and isinstance(first_block.get("text"), str):
                    input_text_source = first_block.get("text")
                    parsed_context["input_text_source_origin"] = "first_dialogue_block"
                else:
                    validation_warnings.append("First dialogue block is missing or has invalid 'text' field.")
        
        if input_text_source and input_text_source.strip():
            parsed_context["initial_text"] = input_text_source.strip()
        else:
            # If no specific input text is found, this might not be an error if NGGS can generate from scene/char context alone.
            # For v1.8, let's assume some form of text (even if just a title or brief description) is preferred.
            validation_warnings.append("No primary input text found in 'nggs_input_text' or 'dialogue_blocks[0].text'. NGGS might generate from broader context if possible.")
            # parsed_context["initial_text"] remains ""

        if validation_warnings:
            parsed_context["parsing_warnings"] = validation_warnings
            self.logger.warning("NDGS data parsing completed with warnings:")
            for warning in validation_warnings:
                self.logger.warning(f"  - {warning}")
        
        # Check if essential data was extracted.
        # For v1.8, 'initial_text' is crucial if NGGS-Lite doesn't have a "generate from scratch based on characters/scene" mode.
        # The current NGGS-Lite structure implies it always starts with some text (even if it's just context for initial generation).
        if not parsed_context["initial_text"] and not parsed_context["scene"] and not parsed_context["characters"]:
             err = IntegrationError("Failed to extract any meaningful context (text, scene, or characters) from NDGS data.")
             self.logger.error(str(err))
             return Result.fail(err)


        self.logger.info("NDGS data successfully parsed for context.")
        return Result.ok(parsed_context)


# --- GLCAI Vocabulary Feedback System ---
class GLCAIVocabularyFeedback:
    """
    Handles tracking and exporting vocabulary usage data from NGGS-Lite
    for potential analysis by GLCAI (v1.8 Scope: Data Collection).
    """
    def __init__(self, config: NGGSConfig, output_dir: pathlib.Path):
        """
        Initializes the GLCAI Feedback component.

        Args:
            config: The NGGSConfig object.
            output_dir: The base output directory where feedback files will be stored.
        """
        self.config = config
        self.logger = logging.getLogger("NGGS-Lite.GLCAIFeedback")
        
        # Define the dedicated feedback directory path
        # Ensure output_dir is a Path object
        base_output_dir = pathlib.Path(output_dir)
        self.feedback_dir: Optional[pathlib.Path] = base_output_dir / config.GLCAI_FEEDBACK_DIR_NAME
        
        self._ensure_directory_exists() # Call this after self.feedback_dir is set

        # Internal state to store tracked data for the current job
        self.current_job_id: Optional[str] = None
        # Structure: {word_str: {"count": int, "layers": List[str], "categories": List[str], "source": Optional[str]}}
        self.tracked_usage: Dict[str, Dict[str, Any]] = {}
        
        if self.feedback_dir:
            self.logger.info(f"GLCAIVocabularyFeedback initialized. Output directory: {self.feedback_dir.resolve()}")
        else:
            self.logger.warning("GLCAIVocabularyFeedback initialized, but feedback directory is not available. Saving will be disabled.")


    def _ensure_directory_exists(self) -> None:
        """Ensures the feedback directory exists."""
        if self.feedback_dir is None: # If directory was already deemed unusable
            return
        try:
            self.feedback_dir.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"GLCAI feedback directory verified/created: {self.feedback_dir}")
        except OSError as e:
            self.logger.critical(
                f"Failed to create GLCAI feedback directory {self.feedback_dir}: {e}. "
                "Vocabulary feedback saving will be disabled for this session."
            )
            self.feedback_dir = None # Mark as unusable

    def start_tracking(self, job_id: str) -> None:
        """Resets tracking state for a new job."""
        self.current_job_id = job_id
        self.tracked_usage = {} # Reset for the new job
        self.logger.info(f"Started vocabulary usage tracking for Job ID: {job_id}")

    def track_vocabulary_usage(
        self,
        text_to_analyze: str,
        vocabulary_items_list: Optional[List[VocabularyItem]] = None,
        word_list_only: Optional[List[str]] = None
    ) -> None:
        """
        Tracks the usage of vocabulary words within a given text.
        Can accept either a list of VocabularyItem objects or a simple list of words.

        Args:
            text_to_analyze: The generated or processed text to analyze.
            vocabulary_items_list: A list of VocabularyItem objects to check against the text.
            word_list_only: A list of word strings to check (if VocabularyItem objects are not available).
        """
        if not self.current_job_id:
            self.logger.warning("Cannot track vocabulary usage: Tracking not started (no current job ID).")
            return
        if not text_to_analyze or not isinstance(text_to_analyze, str):
            self.logger.debug("Skipping vocabulary usage tracking: Input text is empty or invalid.")
            return
        if not vocabulary_items_list and not word_list_only:
            self.logger.debug("Skipping vocabulary usage tracking: No vocabulary items or word list provided.")
            return

        self.logger.debug(f"Tracking vocabulary usage in text (length {len(text_to_analyze)})...")
        text_lower = text_to_analyze.lower()  # Use lowercase for case-insensitive counting

        items_to_check: List[Tuple[str, Optional[Set[str]], Optional[List[str]], Optional[str]]] = []

        if vocabulary_items_list:
            for item_obj in vocabulary_items_list:
                if isinstance(item_obj, VocabularyItem) and item_obj.word:
                    items_to_check.append((item_obj.word, item_obj.layers, item_obj.categories, item_obj.source))
        elif word_list_only:
            for word_str in word_list_only:
                if isinstance(word_str, str) and word_str.strip():
                    items_to_check.append((word_str.strip(), None, None, None)) # Layers/cats/source unknown

        if not items_to_check:
            self.logger.debug("No valid words to track after processing input lists.")
            return

        for word, layers, categories, source in items_to_check:
            # Count occurrences (case-insensitive)
            # Using simple count. For more accuracy on whole words, regex with word boundaries
            # (e.g., r'\b' + re.escape(word.lower()) + r'\b') could be used,
            # but simple count is often sufficient for this type of feedback.
            count = text_lower.count(word.lower())

            if count > 0:
                # Record usage if word is found
                if word not in self.tracked_usage:
                    self.tracked_usage[word] = {
                        "count": 0,
                        "layers": sorted(list(layers)) if layers else [], # Ensure consistent list format
                        "categories": sorted(categories) if categories else [], # Ensure consistent list format
                        "source": source or "unknown"
                    }
                self.tracked_usage[word]["count"] += count
        
        self.logger.debug(f"Finished tracking. Unique used words in this text: {len(self.tracked_usage)}")


    def save_feedback(self) -> Result[pathlib.Path, FileProcessingError]:
        """
        Saves the collected vocabulary usage data to a JSON file.
        Filename includes the Job ID.

        Returns:
            Result containing the path to the saved file (Ok) or an error (Err).
        """
        if not self.current_job_id:
            return Result.fail(FileProcessingError("Cannot save feedback: Tracking not started (no current job ID)."))
        if self.feedback_dir is None: # Check if directory was successfully created/is available
            return Result.fail(FileProcessingError("Cannot save feedback: Output directory for GLCAI feedback is not available."))

        # Prepare data structure for JSON output
        feedback_data = {
            "job_id": self.current_job_id,
            "timestamp_utc": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            "nggs_version": self.config.VERSION,
            "vocab_usage": self.tracked_usage # This is already {word: {details}}
        }

        # Generate filename, ensuring job_id is safe for filenames
        safe_job_id = re.sub(r'[^\w\-.]', '_', self.current_job_id)
        file_path = self.feedback_dir / f"nggs_vocab_feedback_{safe_job_id}.json"

        self.logger.info(f"Saving GLCAI vocabulary feedback data for Job ID {self.current_job_id} to: {file_path}")

        try:
            json_content = json.dumps(feedback_data, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
        except Exception as json_err:
            err = FileProcessingError(f"Failed to serialize feedback data to JSON: {json_err}")
            self.logger.error(str(err), exc_info=True)
            return Result.fail(err)

        # Use safe_write_file utility
        save_result = safe_write_file(file_path, json_content)

        if save_result.is_ok:
            self.logger.info("Vocabulary feedback data saved successfully.")
            return Result.ok(file_path) # Return the actual path
        else:
            # Error already logged by safe_write_file, but we can add context
            original_error = save_result.error
            err = FileProcessingError(
                f"Failed to write vocabulary feedback file '{file_path}': {original_error}",
                details=getattr(original_error, 'details', {}) # Preserve original details if any
            )
            self.logger.error(str(err)) # Log again with more context if needed
            return Result.fail(err)

# =============================================================================
# Part 15 End: Integration Components (NDGS Parser & GLCAI Feedback)
# =============================================================================
# =============================================================================
# Part 16: Batch Processing Class (v1.8)
# =============================================================================

class BatchProcessor:
    """
    Handles batch processing of multiple input text files using TextProcessor (v1.8).
    Provides progress tracking and generates a summary report. Includes basic
    parallel processing capability.
    """
    def __init__(self, config: NGGSConfig, processor: TextProcessor, output_dir: pathlib.Path):
        """
        Initializes the BatchProcessor.

        Args:
            config: NGGSConfig object.
            processor: Initialized TextProcessor instance.
            output_dir: Directory to save batch results and summary.

        Raises:
            ConfigurationError: If processor is invalid or output_dir cannot be handled.
        """
        if not isinstance(config, NGGSConfig):
            raise ConfigurationError("BatchProcessor requires a valid NGGSConfig instance.")
        if not isinstance(processor, TextProcessor): # Basic type check for the stub
            raise ConfigurationError("BatchProcessor requires a valid TextProcessor instance.")

        self.config = config
        self.processor = processor
        self.output_dir = output_dir.resolve()  # Ensure path is absolute
        self.logger = logging.getLogger("NGGS-Lite.Batch")

        # Batch state variables
        self.processed_files_count: int = 0
        self.total_files_to_process: int = 0
        self.successful_job_count: int = 0
        self.failed_job_count: int = 0
        self.skipped_file_count: int = 0
        self.batch_results_summary_list: List[JsonDict] = []
        self.batch_start_time: float = 0.0

        # Ensure output directory exists
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Batch output directory verified/created: {self.output_dir}")
        except PermissionError as e:
            self.logger.critical(f"Permission denied creating output directory: {self.output_dir}")
            raise ConfigurationError(f"Permission denied creating output directory: {self.output_dir}") from e
        except OSError as e: # Catch other OS errors like disk full, invalid path components
            self.logger.critical(f"Failed to create output directory {self.output_dir}: {e}")
            raise ConfigurationError(f"Failed to create output directory {self.output_dir}") from e

    def process_batch(
        self,
        input_dir: Union[str, pathlib.Path],
        max_workers: int = 1,
        file_pattern: str = "*.txt",
        save_individual_results: bool = True,
        **processor_args: Any  # Arguments to pass to processor.process
    ) -> JsonDict:
        """
        Processes all files matching the pattern in the input directory.

        Args:
            input_dir: Directory containing input text files.
            max_workers: Number of parallel workers (>=1). Set > 1 for parallel.
            file_pattern: Glob pattern for input files (e.g., "*.txt", "**/*.txt").
            save_individual_results: Whether to save JSON, HTML, TXT for each job.
            **processor_args: Arguments passed to TextProcessor.process for each file.

        Returns:
            A dictionary containing the batch processing summary.
        """
        input_dir_path = pathlib.Path(input_dir).resolve()
        if not input_dir_path.is_dir():
            err_msg = f"Batch input directory not found or not a directory: {input_dir_path}"
            self.logger.error(err_msg)
            # Return an empty summary indicating failure to find input
            return self._create_empty_summary(error_message=err_msg)


        self.logger.info(f"Searching for files matching '{file_pattern}' in '{input_dir_path}'...")
        try:
            input_files = sorted(list(input_dir_path.glob(file_pattern)))
        except Exception as e:
            err_msg = f"Error globbing for files with pattern '{file_pattern}' in '{input_dir_path}': {e}"
            self.logger.error(err_msg, exc_info=True)
            return self._create_empty_summary(error_message=err_msg)

        self.total_files_to_process = len(input_files)

        if not input_files:
            self.logger.warning(f"No files matching '{file_pattern}' found in: {input_dir_path}")
            return self._create_empty_summary()

        # Reset batch state
        self.processed_files_count = 0
        self.successful_job_count = 0
        self.failed_job_count = 0
        self.skipped_file_count = 0
        self.batch_results_summary_list = []
        self.batch_start_time = time.monotonic()

        self.logger.info(f"Starting batch processing for {self.total_files_to_process} files...")
        if max_workers > 1:
            self.logger.info(f"Using parallel processing with up to {max_workers} workers.")
        else:
            self.logger.info("Using sequential processing (1 worker).")

        self._print_progress()  # Initial progress

        shared_args = {"save_results": save_individual_results, "processor_args": processor_args}

        if max_workers > 1:
            try:
                from concurrent.futures import ThreadPoolExecutor, as_completed
                # Check if module was actually imported successfully
                if 'ThreadPoolExecutor' not in locals() or 'as_completed' not in locals():
                    raise ImportError("concurrent.futures components not available.")
                self._run_parallel(input_files, max_workers, shared_args)
            except ImportError:
                self.logger.warning("Module 'concurrent.futures' not found or failed to import. Falling back to sequential processing.")
                self._run_sequential(input_files, shared_args)
            except Exception as parallel_err:
                self.logger.critical(f"Unexpected error during parallel execution setup: {parallel_err}", exc_info=True)
                self.logger.warning("Falling back to sequential processing due to parallel setup error.")
                self._run_sequential(input_files, shared_args)
        else:
            self._run_sequential(input_files, shared_args)

        summary = self._generate_batch_summary()
        self._save_batch_summary(summary)
        self._print_progress(final=True)
        self.logger.info(
            f"Batch processing finished. Success: {self.successful_job_count}, "
            f"Failed: {self.failed_job_count}, Skipped: {self.skipped_file_count}"
        )
        return summary

    def _run_sequential(self, input_files: List[pathlib.Path], shared_args: Dict[str, Any]) -> None:
        """Processes files sequentially."""
        self.logger.info("Running jobs sequentially...")
        for file_path in input_files:
            try:
                summary_item = self._process_single_file(file_path, **shared_args)
                self.batch_results_summary_list.append(summary_item)
                status = summary_item.get("status", "Unknown").lower()
                if "fail" in status: self.failed_job_count += 1
                elif "skip" in status: self.skipped_file_count += 1
                else: self.successful_job_count += 1
            except Exception as e:
                self.logger.error(f"Critical error processing file {file_path.name} in sequence: {e}", exc_info=True)
                self.batch_results_summary_list.append({
                    "file": file_path.name, "status": "Failed (Critical Sequence Error)",
                    "error_snippet": [f"{type(e).__name__}: {str(e)}"], "job_id": "N/A"
                })
                self.failed_job_count += 1
            finally:
                self.processed_files_count += 1
                self._print_progress()

    def _run_parallel(self, input_files: List[pathlib.Path], max_workers: int, shared_args: Dict[str, Any]) -> None:
        """Processes files in parallel using ThreadPoolExecutor."""
        from concurrent.futures import ThreadPoolExecutor, as_completed # Import here
        self.logger.info(f"Running jobs in parallel (max_workers={max_workers})...")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self._process_single_file, file_path, **shared_args): file_path
                for file_path in input_files
            }
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    summary_item = future.result()
                    self.batch_results_summary_list.append(summary_item)
                    status = summary_item.get("status", "Unknown").lower()
                    if "fail" in status: self.failed_job_count += 1
                    elif "skip" in status: self.skipped_file_count += 1
                    else: self.successful_job_count += 1
                except Exception as e:
                    self.logger.error(f"Error processing file {file_path.name} in thread: {e}", exc_info=True)
                    self.batch_results_summary_list.append({
                        "file": file_path.name, "status": "Failed (Thread Exception)",
                        "error_snippet": [f"{type(e).__name__}: {str(e)}"], "job_id": "N/A"
                    })
                    self.failed_job_count += 1
                finally:
                    self.processed_files_count += 1
                    self._print_progress()

    def _process_single_file(
        self,
        file_path: pathlib.Path,
        save_results: bool,
        processor_args: Dict[str, Any]
    ) -> JsonDict:
        """
        Reads a single file, processes it, saves results, and returns a summary.
        """
        self.logger.info(f"Processing file: {file_path.name}...")
        start_file_time = time.monotonic()
        file_summary: JsonDict = {"file": file_path.name, "job_id": "N/A", "status": "Pending"}

        read_result = safe_read_file(file_path)
        if read_result.is_err:
            error_detail = str(read_result.error)
            self.logger.error(f"Failed to read file {file_path.name}: {error_detail}")
            file_summary["status"] = "Failed (Read Error)"
            file_summary["error_snippet"] = [error_detail]
            return file_summary

        input_text = read_result.unwrap()
        if not input_text or not input_text.strip():
            self.logger.warning(f"File {file_path.name} is empty or whitespace only. Skipping.")
            file_summary["status"] = "Skipped (Empty)"
            return file_summary

        job_results: JsonDict = {}
        try:
            process_result = self.processor.process(initial_text=input_text, **processor_args)
            if process_result.is_ok:
                job_results = process_result.unwrap()
            else:
                err = process_result.error
                job_results = {
                    "job_id": self.config.generate_job_id("error_job_"),
                    "status": "Failed Critically (Processor)",
                    "errors": [str(err)], "final_text": "", "final_scores": {},
                    "parameters": processor_args, "start_time": datetime.now(timezone.utc).isoformat()
                }
                self.logger.error(f"Critical failure processing {file_path.name}: {err}")
        except Exception as e:
            self.logger.critical(f"Unexpected error during processor.process for {file_path.name}: {e}", exc_info=True)
            job_results = {
                "job_id": self.config.generate_job_id("unexpected_err_"),
                "status": "Failed (Unexpected Processor Error)",
                "errors": [f"Unexpected: {type(e).__name__} - {str(e)}"], "final_text": "", "final_scores": {},
                "parameters": processor_args, "start_time": datetime.now(timezone.utc).isoformat()
            }

        job_id = job_results.get("job_id", f"batch_{file_path.stem}_{int(time.time())}")
        file_summary["job_id"] = job_id
        if save_results:
            self._save_job_results(job_id, job_results)

        file_duration = time.monotonic() - start_file_time
        self.logger.info(f"Finished file: {file_path.name} in {file_duration:.2f}s. Status: {job_results.get('status', 'Unknown')}")

        file_summary["status"] = job_results.get("status", "Unknown")
        file_summary["final_score_overall"] = job_results.get("final_scores", {}).get("overall_quality")
        file_summary["duration_seconds"] = round(file_duration, 2)
        errors_list = job_results.get("errors", [])
        if errors_list and isinstance(errors_list, list) and errors_list:
            file_summary["error_snippet"] = [str(errors_list[0])]
        return file_summary

    def _save_job_results(self, job_id: str, job_results: JsonDict) -> None:
        """Saves individual job results (JSON, HTML, TXT)."""
        safe_job_id = re.sub(r'[^\w\-.]', '_', job_id)
        job_output_dir = self.output_dir / safe_job_id # Create a subdirectory for each job
        try:
            job_output_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            self.logger.error(f"Failed to create output subdirectory {job_output_dir} for job {job_id}: {e}. Saving to main batch output dir.")
            job_output_dir = self.output_dir # Fallback to main output dir

        json_path = job_output_dir / f"{safe_job_id}_results.json"
        save_res_json = safe_write_file(json_path, json.dumps(job_results, ensure_ascii=False, indent=2, cls=CompactJSONEncoder))
        if save_res_json.is_err: self.logger.error(f"Failed to save JSON results for {job_id}: {save_res_json.error}")

        html_report_content = job_results.get("html_report")
        if html_report_content and isinstance(html_report_content, str):
            html_path = job_output_dir / f"{safe_job_id}_report.html"
            save_res_html = safe_write_file(html_path, html_report_content)
            if save_res_html.is_err: self.logger.error(f"Failed to save HTML report for {job_id}: {save_res_html.error}")
        elif "Fail" not in job_results.get("status", ""):
            self.logger.debug(f"No HTML report content found for successful job {job_id}.")

        final_text_content = job_results.get("final_text")
        if final_text_content and isinstance(final_text_content, str):
            text_path = job_output_dir / f"{safe_job_id}_final.txt"
            save_res_text = safe_write_file(text_path, final_text_content)
            if save_res_text.is_err: self.logger.error(f"Failed to save final text for {job_id}: {save_res_text.error}")
        elif "Fail" not in job_results.get("status", ""):
            self.logger.debug(f"No final text content found for successful job {job_id}.")

    def _generate_batch_summary(self) -> JsonDict:
        """Generates the final batch summary dictionary."""
        end_time = time.monotonic()
        total_duration = end_time - self.batch_start_time if self.batch_start_time > 0 else 0.0
        
        # Use the state counters, assuming they are correctly updated
        return {
            "batch_start_iso": datetime.fromtimestamp(self.batch_start_time, timezone.utc).isoformat() if self.batch_start_time > 0 else None,
            "batch_end_iso": datetime.now(timezone.utc).isoformat(),
            "execution_time_seconds": round(total_duration, 2),
            "total_files_found": self.total_files_to_process,
            "files_processed": self.processed_files_count,
            "successful_jobs": self.successful_job_count,
            "failed_jobs": self.failed_job_count,
            "skipped_files": self.skipped_file_count,
            "results_summary": self.batch_results_summary_list, # List of individual summaries
            "timestamp_utc": datetime.now(timezone.utc).isoformat()
        }

    def _save_batch_summary(self, summary: JsonDict) -> None:
        """Saves the batch summary report (JSON and HTML)."""
        if not self.output_dir or not self.output_dir.is_dir() or not os.access(self.output_dir, os.W_OK):
            self.logger.error(f"Cannot save batch summary: Output directory {self.output_dir} is not accessible.")
            return

        json_summary_path = self.output_dir / "_batch_summary_report.json"
        save_res_json = safe_write_file(json_summary_path, json.dumps(summary, ensure_ascii=False, indent=2, cls=CompactJSONEncoder))
        if save_res_json.is_ok: self.logger.info(f"Batch summary JSON report saved: {json_summary_path}")
        else: self.logger.error(f"Failed to save batch summary JSON report: {save_res_json.error}")

        try:
            html_summary = self._generate_html_batch_summary(summary)
            html_summary_path = self.output_dir / "_batch_summary_report.html"
            save_res_html = safe_write_file(html_summary_path, html_summary)
            if save_res_html.is_ok: self.logger.info(f"Batch HTML summary report saved: {html_summary_path}")
            else: self.logger.error(f"Failed to save batch HTML summary report: {save_res_html.error}")
        except Exception as e:
            self.logger.error(f"Failed to generate or save HTML batch summary: {e}", exc_info=True)

    def _generate_html_batch_summary(self, summary: JsonDict) -> str:
        """Generates an HTML summary report for the batch process."""
        exec_time = summary.get("execution_time_seconds", 0)
        time_str = self._format_time(exec_time)
        config_version = self.config.VERSION # Get version from config

        html_parts = [
            f"<!DOCTYPE html><html lang='ja'><head><meta charset='UTF-8'>",
            f"<title>NGGS-Lite v{config_version} バッチ処理サマリー</title>",
            "<style>",
            "body { font-family: 'Segoe UI', Meiryo, sans-serif; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; font-size: 14px; }",
            ".container { max-width: 1200px; margin: auto; background-color: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }",
            "h1, h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }",
            "h1 { text-align: center; }",
            ".summary-box { display: flex; flex-wrap: wrap; justify-content: space-around; margin-bottom: 20px; padding: 15px; background-color: #ecf0f1; border-radius: 5px; }",
            ".summary-item { text-align: center; margin: 10px; padding: 10px; min-width: 120px; background-color: #fff; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.05);}",
            ".summary-item span { display: block; font-size: 1.8em; font-weight: bold; color: #3498db; margin-bottom: 5px; }",
            ".summary-item .status-ok span { color: #2ecc71; }",
            ".summary-item .status-error span { color: #e74c3c; }",
            ".summary-item .status-warn span { color: #f39c12; }",
            "table { width: 100%; border-collapse: collapse; margin-top: 20px; font-size: 0.95em; }",
            "th, td { border: 1px solid #ddd; padding: 10px; text-align: left; }",
            "th { background-color: #3498db; color: white; }",
            "tr:nth-child(even) { background-color: #f9f9f9; }",
            "td a { color: #2980b9; text-decoration: none; } td a:hover { text-decoration: underline; }",
            ".status-ok { color: #27ae60; font-weight: bold; }",
            ".status-error, .status-failed, .status-failed-critically, .status-failed-read-error, .status-failed-thread-exception, .status-failed-critical-sequence-error, .status-failed-unexpected-processor-error { color: #c0392b; font-weight: bold; }",
            ".status-skipped, .status-warn { color: #d35400; font-weight: bold; }",
            ".error-col { max-width: 300px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }",
            "</style></head><body><div class='container'>",
            f"<h1>NGGS-Lite バッチ処理サマリー (v{config_version})</h1>",
            f"<p style='text-align:center; font-size:0.9em; color:#777;'>レポート生成日時: {summary.get('timestamp_utc', 'N/A')}</p>",
            "<div class='summary-box'>",
            f"<div class='summary-item'><span>{summary.get('total_files_found', 0)}</span>検出ファイル数</div>",
            f"<div class='summary-item'><span>{summary.get('files_processed', 0)}</span>処理済み</div>",
            f"<div class='summary-item status-ok'><span>{summary.get('successful_jobs', 0)}</span>成功</div>",
            f"<div class='summary-item status-error'><span>{summary.get('failed_jobs', 0)}</span>失敗</div>",
            f"<div class='summary-item status-warn'><span>{summary.get('skipped_files', 0)}</span>スキップ</div>",
            f"<div class='summary-item'><span>{time_str}</span>総処理時間</div>",
            "</div>",
            "<h2>個別ファイル処理結果</h2>",
            "<table><thead><tr><th>ファイル名</th><th>ジョブID</th><th>ステータス</th><th>総合スコア</th><th>処理時間(秒)</th><th>エラー概要</th></tr></thead><tbody>"
        ]

        def get_status_class(status_str: str) -> str:
            s_lower = status_str.lower()
            if "fail" in s_lower or "error" in s_lower: return "status-error"
            if "skip" in s_lower or "warn" in s_lower: return "status-warn"
            if "success" in s_lower or "完了" in status_str : return "status-ok" #完了 is success
            return ""

        # Sort results: Failed > Skipped > Success (by score desc)
        def sort_key_batch_results(r_item: JsonDict):
            s = r_item.get("status", "").lower()
            score = r_item.get("final_score_overall", 0.0)
            if not isinstance(score, (int, float)): score = 0.0 # Handle non-numeric scores
            
            if "fail" in s or "error" in s: return (2, -score, r_item.get("file", "")) # Failed first, then by score (higher score for failed is unusual but for sorting)
            if "skip" in s or "warn" in s: return (1, -score, r_item.get("file", "")) # Skipped next
            return (0, -score, r_item.get("file", "")) # Successful last, sorted by score descending

        sorted_results = sorted(summary.get("results_summary", []), key=sort_key_batch_results)

        for res in sorted_results:
            status = res.get('status', 'Unknown')
            status_class = get_status_class(status)
            score_val = res.get('final_score_overall')
            score_str = f"{score_val:.2f}" if isinstance(score_val, (int, float)) else "N/A"
            error_list = res.get('error_snippet', [])
            error_str = str(error_list[0]) if error_list and isinstance(error_list, list) else ""
            duration = res.get('duration_seconds', "N/A")
            duration_str = f"{duration:.2f}s" if isinstance(duration, (int, float)) else "N/A"
            
            job_id_val = res.get('job_id', 'N/A')
            file_name_val = res.get('file', 'N/A')
            
            # Link to individual report if job was successful and ID exists
            report_link = ""
            if job_id_val != 'N/A' and "fail" not in status.lower() and "skip" not in status.lower():
                 safe_job_id = re.sub(r'[^\w\-.]', '_', job_id_val)
                 report_link = f"./{safe_job_id}/{safe_job_id}_report.html" # Assuming subfolder per job
            
            file_cell_html = f"<a href='{report_link}'>{escape_html(file_name_val)}</a>" if report_link else escape_html(file_name_val)

            html_parts.append(f"<tr><td>{file_cell_html}</td><td>{escape_html(job_id_val)}</td>")
            html_parts.append(f"<td class='{status_class}'>{escape_html(status)}</td><td>{score_str}</td><td>{duration_str}</td>")
            html_parts.append(f"<td class='error-col' title='{escape_html(error_str)}'>{escape_html(truncate_text(error_str, 70))}</td></tr>")

        html_parts.append("</tbody></table></div></body></html>")
        return "".join(html_parts)

    def _print_progress(self, final: bool = False) -> None:
        """Prints batch processing progress to the console."""
        if self.total_files_to_process == 0 and not final : return # Avoid division by zero if called before total_files set
        
        percent = (self.processed_files_count / self.total_files_to_process) * 100 if self.total_files_to_process > 0 else 100.0
        
        elapsed_time = 0.0
        if self.batch_start_time > 0:
            elapsed_time = time.monotonic() - self.batch_start_time
        
        elapsed_str = self._format_time(elapsed_time)
        remaining_str = "N/A"

        if self.processed_files_count > 0 and not final and elapsed_time > 0 and self.total_files_to_process > self.processed_files_count:
            time_per_file = elapsed_time / self.processed_files_count
            remaining_seconds = time_per_file * (self.total_files_to_process - self.processed_files_count)
            remaining_str = self._format_time(remaining_seconds)
        
        bar_length = 30
        filled_length = int(bar_length * self.processed_files_count // self.total_files_to_process) if self.total_files_to_process > 0 else bar_length
        bar = '█' * filled_length + '-' * (bar_length - filled_length)
        
        status_line = (
            f"処理済: {self.processed_files_count}/{self.total_files_to_process} | "
            f"成功: {self.successful_job_count}, 失敗: {self.failed_job_count}, スキップ: {self.skipped_file_count}"
        )
        time_line = f"経過: {elapsed_str}, 残り推定: {remaining_str}" if not final else f"総時間: {elapsed_str}"
        
        # Use carriage return for dynamic update, newline for final
        end_char = '\n' if final else ''
        # Ensure the line is cleared before printing to avoid ghosting on some terminals
        clear_line = '\r' + ' ' * 120 + '\r' # Adjust width as needed
        
        try:
            sys.stdout.write(f"{clear_line}[{bar}] {percent:.1f}% | {status_line} | {time_line}{end_char}")
            sys.stdout.flush()
        except BlockingIOError: # pragma: no cover
             # This can happen in some environments (e.g. CI runners)
             # Fallback to simple print if flush fails due to blocking
             print(f"[{bar}] {percent:.1f}% | {status_line} | {time_line}{end_char}")
        except Exception: # Catch any other stdout errors
             print(f"Progress: {percent:.1f}%, {status_line}, {time_line}{end_char}")


    def _format_time(self, seconds: float) -> str:
        """Formats seconds into H:MM:SS or M:SS or S.fs format."""
        if seconds < 0: return "0.0s"
        
        secs_int = int(seconds)
        millis = int((seconds - secs_int) * 10) # Single decimal for seconds
        
        minutes, secs_int = divmod(secs_int, 60)
        hours, minutes = divmod(minutes, 60)
        
        if hours > 0:
            return f"{hours}:{minutes:02d}:{secs_int:02d}"
        elif minutes > 0:
            return f"{minutes}:{secs_int:02d}"
        else:
            return f"{secs_int}.{millis}s"

    def _create_empty_summary(self, error_message: Optional[str] = None) -> JsonDict:
        """Creates an empty summary dict if no files are processed or an error occurs early."""
        now_iso = datetime.now(timezone.utc).isoformat()
        summary: JsonDict = {
            "batch_start_iso": now_iso,
            "batch_end_iso": now_iso,
            "execution_time_seconds": 0.0,
            "total_files_found": 0,
            "files_processed": 0,
            "successful_jobs": 0,
            "failed_jobs": 0,
            "skipped_files": 0,
            "results_summary": [],
            "timestamp_utc": now_iso
        }
        if error_message:
            summary["batch_error"] = error_message
        return summary

# =============================================================================
# Part 16 End: Batch Processing Class
# =============================================================================
# =============================================================================
# Part 17: Main Execution Function and Entry Point (v1.8)
# =============================================================================

def parse_arguments() -> argparse.Namespace:
    """Parses command line arguments for NGGS-Lite v1.8."""
    # Temporarily instantiate NGGSConfig to get defaults for help messages
    # This assumes NGGSConfig can be instantiated without external files for its defaults.
    try:
        temp_config = NGGSConfig()
        default_paths = temp_config.get_default_paths() # Assuming this method exists
        default_output_dir_val = temp_config.DEFAULT_OUTPUT_DIR
        default_max_loops_val = temp_config.DEFAULT_MAX_LOOPS
        default_threshold_val = temp_config.DEFAULT_IMPROVEMENT_THRESHOLD
        default_target_length_val = temp_config.DEFAULT_TARGET_LENGTH
        default_gemini_model_val = temp_config.GEMINI_MODEL_NAME
        default_log_file_val = temp_config.DEFAULT_LOG_FILE or "Disabled"
    except Exception as e:
        # Fallback defaults if NGGSConfig init fails during arg parsing setup
        print(f"Warning: Could not load NGGSConfig defaults for help text ({e}). Using hardcoded fallback values.", file=sys.stderr)
        default_paths = {"DEFAULT_TEMPLATES_DIR": "./templates_v1.8_fallback",
                         "DEFAULT_VOCAB_PATH": "./data/gothic_vocabulary_v1.8_fallback.json",
                         "GLCAI_VOCAB_PATH": "./data/glcai_export_latest_fallback.json"}
        default_output_dir_val = "./nggs_lite_output_v1.8_fallback"
        default_max_loops_val = 3
        default_threshold_val = 4.3
        default_target_length_val = 1200
        default_gemini_model_val = "models/gemini-1.5-pro-latest"
        default_log_file_val = "nggs_lite_v1_8_fallback.log"

    parser = argparse.ArgumentParser(
        description=f"NGGS-Lite v{NGGSConfig.VERSION}: Neo-Gothic Text Generation (v1.8).",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # --- Input/Output ---
    input_group = parser.add_mutually_exclusive_group(required=False) # Made not strictly required if --ndgs-input is primary
    input_group.add_argument("--input", "-i", type=str,
                             help="Path to the input text file (UTF-8) for single mode.")
    input_group.add_argument("--text", "-t", type=str,
                             help="Direct input text string for single mode.")
    input_group.add_argument("--batch", "-b", type=str, metavar="INPUT_DIR",
                             help="Run in batch mode on files in INPUT_DIR.")
    
    parser.add_argument("--ndgs-input", "-n", type=str, metavar="NDGS_JSON_FILE",
                        help="Path to NDGS output JSON file. If provided, this may serve as the primary input source, "
                             "potentially overriding --input/--text for initial context.")
    
    parser.add_argument("--output", "-o", type=str, default=None, # Default handled by config
                        help=f"Output directory for results. (Default: '{default_output_dir_val}')")

    # --- Core Parameters ---
    parser.add_argument("--loops", "-l", type=int, default=None, choices=range(0, 11), metavar="[0-10]", # Increased max loops
                        help=f"Max improvement loops. 0 for eval only. (Default: {default_max_loops_val})")
    parser.add_argument("--length", type=int, default=None,
                        help=f"Approx. target text length. (Default: {default_target_length_val})")
    parser.add_argument("--threshold", type=float, default=None, metavar="[0.0-5.0]",
                        help=f"Score threshold to stop early. (Default: {default_threshold_val:.1f})")

    # --- File Paths ---
    parser.add_argument("--templates", type=str, default=None,
                        help=f"Directory containing prompt templates. (Default: '{default_paths['DEFAULT_TEMPLATES_DIR']}')")
    parser.add_argument("--nggs-vocab", type=str, default=None,
                        help=f"Path to the NGGS default vocabulary JSON file. (Default: '{default_paths['DEFAULT_VOCAB_PATH']}')")
    parser.add_argument("--glcai-vocab", type=str, default=None,
                        help=f"Path to the GLCAI exported vocabulary JSON file. (Default: '{default_paths['GLCAI_VOCAB_PATH']}')")

    # --- Style Control ---
    parser.add_argument("--perspective", type=str,
                        choices=["subjective_first_person", "perspective_shift", "dream_perspective", "objective"],
                        default="subjective_first_person", help="Perspective mode.")
    parser.add_argument("--phase-focus", type=str,
                        choices=["balanced", "serif", "monologue", "narration", "live_report"],
                        default="balanced", help="Preferred narrative phase.")
    parser.add_argument("--colloquial-level", type=str, choices=["high", "medium", "low"],
                        default="medium", help="Level of colloquialism.")
    parser.add_argument("--emotion-arc", type=str, default=None,
                        help="Desired emotional arc string (e.g., '違和感->恐怖->啓示').")

    # --- LLM Engine ---
    parser.add_argument("--model", type=str, default=None,
                        help=f"LLM model name to use. (Default: '{default_gemini_model_val}')")

    # --- Execution Modes ---
    parser.add_argument("--skip-initial-generation", "--skip-gen", action="store_true",
                        help="Skip initial generation and evaluate the input text directly.")
    parser.add_argument("--max-workers", type=int, default=1, metavar="N",
                        help="Number of parallel workers for batch mode (1 for sequential).")
    parser.add_argument("--no-individual-results", action="store_true",
                        help="In batch mode, do not save individual result files (JSON/HTML/TXT).")
    parser.add_argument("--verbose", "-v", action="count", default=0,
                        help="Increase logging verbosity (-v for INFO, -vv for DEBUG).")
    parser.add_argument("--log-file", type=str, default=None, # Default None uses config default
                        help=f"Path to log file. Set to 'NONE' to disable file logging. (Default: '{default_log_file_val}')")
    parser.add_argument("--mock", action="store_true", help="Force mock LLM mode for testing.")

    parsed_args = parser.parse_args()

    # Post-parsing validation for input source
    if not parsed_args.batch and not parsed_args.input and not parsed_args.text and not parsed_args.ndgs_input:
        parser.error("No input source specified. Use --input, --text, --batch, or --ndgs-input.")

    return parsed_args


def setup_components(
    config: NGGSConfig, args: argparse.Namespace
) -> Tuple[TextProcessor, GLCAIVocabularyFeedback, NDGSIntegration]: # Added NDGSIntegration to return
    """
    Initializes and wires up all necessary components for NGGS-Lite v1.8.
    """
    logger_setup = logging.getLogger("NGGS-Lite.Setup")
    logger_setup.info(f"--- NGGS-Lite v{config.VERSION} Component Setup ---")

    # --- LLM Client ---
    logger_setup.info(f"Initializing LLMClient (Engine: {config.LLM_ENGINE}, Mock: {args.mock})...")
    llm_client = LLMClient(config=config, use_mock=args.mock)
    config.LLM_ENGINE = llm_client.current_engine # Update config with actual engine used

    # --- Load Templates ---
    template_dir_path = pathlib.Path(args.templates if args.templates else config.DEFAULT_TEMPLATES_DIR).resolve()
    logger_setup.info(f"Loading templates from: {template_dir_path}")
    # Assuming DEFAULT_TEMPLATES is a global dict defined in an earlier part (e.g., Part 4)
    # For stub purposes, we'll define a dummy one here.
    _DUMMY_DEFAULT_TEMPLATES = {
        "generation": "Gen template stub: {input_text}",
        "evaluation": "Eval template stub: {generated_text}",
        "improvement": "Improve template stub: {original_text}"
    }
    templates = {}
    for name in ["generation", "evaluation", "improvement"]:
        path = template_dir_path / f"{name}.txt" # Assuming .txt extension
        load_result = load_template(path, name, _DUMMY_DEFAULT_TEMPLATES) # Use dummy for stub
        if load_result.is_err:
            logger_setup.critical(f"Failed to load required template '{name}': {load_result.error}")
            raise load_result.error # type: ignore
        templates[name] = load_result.unwrap()
    logger_setup.info("Templates loaded successfully.")

    # --- Vocabulary Manager ---
    logger_setup.info("Initializing VocabularyManager...")
    vocab_manager = VocabularyManager(config) # Assumes VocabularyManager is defined
    if not vocab_manager.has_vocabulary():
        logger_setup.warning("Vocabulary Manager initialized, but no vocabulary loaded.")
    else:
        logger_setup.info("VocabularyManager initialized successfully.")

    # --- Narrative Flow Framework ---
    logger_setup.info("Initializing NarrativeFlowFramework...")
    narrative_flow = NarrativeFlowFramework(vocab_manager, config) # Assumes NarrativeFlowFramework is defined
    logger_setup.info("NarrativeFlowFramework initialized.")

    # --- Evaluators ---
    logger_setup.info("Initializing Evaluators...")
    evaluator = Evaluator(llm_client, config, templates["evaluation"])
    eti_evaluator = ExtendedETIEvaluator(config, narrative_flow)
    ri_evaluator = RIEvaluator(config)
    subjective_evaluator = SubjectiveEvaluator(config)
    logger_setup.info("Evaluators initialized.")

    # --- NDGS Integration Parser ---
    logger_setup.info("Initializing NDGSIntegration Parser...")
    ndgs_parser = NDGSIntegration(config) # Assumes NDGSIntegration is defined
    logger_setup.info("NDGSIntegration Parser initialized.")

    # --- GLCAI Feedback Component ---
    output_dir_path = pathlib.Path(args.output if args.output else config.DEFAULT_OUTPUT_DIR).resolve()
    logger_setup.info(f"Initializing GLCAIVocabularyFeedback (Output Dir: {output_dir_path})...")
    glcai_feedback = GLCAIVocabularyFeedback(config, output_dir_path) # Assumes GLCAIVocabularyFeedback is defined
    logger_setup.info("GLCAIVocabularyFeedback initialized.")

    # --- TextProcessor ---
    logger_setup.info("Initializing TextProcessor...")
    processor = TextProcessor( # Assumes TextProcessor is defined
        config=config, llm_client=llm_client, evaluator=evaluator,
        vocab_manager=vocab_manager, narrative_flow=narrative_flow,
        eti_evaluator=eti_evaluator, ri_evaluator=ri_evaluator,
        subjective_evaluator=subjective_evaluator,
        generation_template=templates["generation"],
        improvement_template=templates["improvement"],
        ndgs_parser=ndgs_parser
    )
    logger_setup.info("TextProcessor initialized.")
    logger_setup.info("--- Component Setup Complete ---")
    return processor, glcai_feedback, ndgs_parser # Return ndgs_parser as well


def run_single_job(
    args: argparse.Namespace,
    config: NGGSConfig,
    processor: TextProcessor,
    glcai_feedback: GLCAIVocabularyFeedback,
    ndgs_parser: NDGSIntegration, # Added ndgs_parser
    output_dir_path: pathlib.Path
) -> JsonDict:
    """Runs a single generation job, saves results, and handles feedback."""
    logger_main_single = logging.getLogger("NGGS-Lite.SingleJob")
    logger_main_single.info("--- Starting Single Job ---")

    input_text_content: Optional[str] = None
    ndgs_data_dict: Optional[JsonDict] = None
    input_source_desc: str = "N/A"

    if args.ndgs_input:
        ndgs_path = pathlib.Path(args.ndgs_input).resolve()
        input_source_desc = f"NDGS Input File: {ndgs_path.name}"
        logger_main_single.info(f"Processing NDGS input from: {ndgs_path}")
        # Parse NDGS file using the dedicated parser to get structured data
        # The parser itself handles reading the file content.
        parsed_ndgs_result = ndgs_parser.parse_from_file(ndgs_path)
        if parsed_ndgs_result.is_err:
            # If NDGS parsing fails, it's a critical error for this input mode.
            logger_main_single.error(f"Failed to parse NDGS input file: {parsed_ndgs_result.error}")
            raise IntegrationError(f"NDGS input parsing failed: {parsed_ndgs_result.error}") from parsed_ndgs_result.error
        ndgs_data_dict = parsed_ndgs_result.unwrap()
        # initial_text for processor.process will be extracted by processor from ndgs_data_dict
        input_text_content = ndgs_data_dict.get("initial_text", "") # Get for logging, processor re-extracts
    elif args.text:
        input_text_content = args.text
        input_source_desc = "Command Line Argument (--text)"
    elif args.input:
        input_path = pathlib.Path(args.input).resolve()
        input_source_desc = f"Input File: {input_path.name}"
        logger_main_single.info(f"Reading input text from file: {input_path}")
        read_result = safe_read_file(input_path)
        if read_result.is_err:
            logger_main_single.error(f"Failed to read input file {input_path}: {read_result.error}")
            raise read_result.error # type: ignore
        input_text_content = read_result.unwrap()
    else:
        # This case should be caught by argparse if no input is provided and ndgs_input is also missing.
        # If ndgs_input was the only option and failed, it's handled above.
        # If all are optional and none given, it's a config error.
        raise ConfigurationError("No valid input source provided for single job.")

    logger_main_single.info(f"Input Source: {input_source_desc}")
    if input_text_content and len(input_text_content) < 200: # Log short inputs
        logger_main_single.debug(f"Input Text Preview: {input_text_content[:100]}...")
    elif ndgs_data_dict:
        logger_main_single.debug(f"Processing with NDGS data dictionary. Initial text snippet from NDGS: {truncate_text(input_text_content, 50)}")


    # Call Processor
    # initial_text is passed; if ndgs_data_dict is also passed, processor should prioritize it.
    process_result = processor.process(
        initial_text=input_text_content if input_text_content is not None else "", # Ensure string
        target_length=args.length,
        perspective_mode=args.perspective,
        phase_focus=args.phase_focus,
        colloquial_level=args.colloquial_level,
        emotion_arc=args.emotion_arc,
        max_loops_override=args.loops,
        threshold_override=args.threshold,
        skip_initial_generation=args.skip_initial_generation,
        ndgs_input_data=ndgs_data_dict # Pass the structured NDGS data
    )

    # Handle Results
    if process_result.is_ok:
        results_dict = process_result.unwrap()
        job_id = results_dict.get("job_id", "unknown_job")
        logger_main_single.info(f"Single job ({job_id}) finished. Status: {results_dict.get('status', 'Unknown')}")

        # Ensure output directory exists for saving results
        try:
            output_dir_path.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            logger_main_single.error(f"Could not create output directory {output_dir_path}: {e}. Results may not be saved.")
            # Potentially re-raise or handle more gracefully
        
        # Save job results (JSON, HTML, TXT) into a subfolder named by job_id
        safe_job_id = re.sub(r'[^\w\-.]', '_', job_id)
        job_specific_output_dir = output_dir_path / safe_job_id
        try:
            job_specific_output_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            logger_main_single.error(f"Could not create job-specific output directory {job_specific_output_dir}: {e}. Saving to main output dir.")
            job_specific_output_dir = output_dir_path


        json_path = job_specific_output_dir / f"{safe_job_id}_results.json"
        save_res_json = safe_write_file(json_path, json.dumps(results_dict, ensure_ascii=False, indent=2, cls=CompactJSONEncoder))
        if save_res_json.is_ok: logger_main_single.info(f"Saved job result: {json_path.name}")
        else: logger_main_single.error(f"Failed to save JSON result for {job_id}: {save_res_json.error}")

        if results_dict.get("html_report"):
            html_path = job_specific_output_dir / f"{safe_job_id}_report.html"
            save_res_html = safe_write_file(html_path, results_dict["html_report"])
            if save_res_html.is_ok: logger_main_single.info(f"Saved HTML report: {html_path.name}")
            else: logger_main_single.error(f"Failed to save HTML report for {job_id}: {save_res_html.error}")

        if results_dict.get("final_text"):
            text_path = job_specific_output_dir / f"{safe_job_id}_final.txt"
            save_res_text = safe_write_file(text_path, results_dict["final_text"])
            if save_res_text.is_ok: logger_main_single.info(f"Saved final text: {text_path.name}")
            else: logger_main_single.error(f"Failed to save final text for {job_id}: {save_res_text.error}")

        # GLCAI Feedback
        try:
            glcai_feedback.start_tracking(job_id)
            best_text = results_dict.get("final_text", "")
            # Assuming processor has vocab_manager attached and it has 'items'
            vocab_items_for_tracking = getattr(getattr(processor, 'vocab_manager', None), 'items', None)
            if best_text and vocab_items_for_tracking:
                glcai_feedback.track_vocabulary_usage(best_text, vocabulary_items_list=vocab_items_for_tracking)
                save_feedback_res = glcai_feedback.save_feedback()
                if save_feedback_res.is_err:
                    logger_main_single.error(f"Failed to save GLCAI feedback for {job_id}: {save_feedback_res.error}")
            else:
                logger_main_single.warning(f"Skipping GLCAI feedback for {job_id}: No final text or vocab items.")
        except Exception as feedback_err:
            logger_main_single.error(f"Error during GLCAI feedback for {job_id}: {feedback_err}", exc_info=True)

        # Console Summary
        print("\n" + "="*30 + f"\n--- 生成テキスト (ジョブID: {job_id}) ---\n" + "="*30)
        print(results_dict.get("final_text", "(エラー: 最終テキストが見つかりません)"))
        print("="*30 + "\n最終スコア:")
        final_scores_print = results_dict.get('final_scores', {})
        if final_scores_print:
            score_order = ["overall_quality", "eti", "ri", "subjective", "phase", "layer", "emotion", "colloquial"]
            displayed = set()
            for k_disp in score_order:
                if k_disp in final_scores_print: print(f"  - {get_metric_display_name(k_disp)}: {final_scores_print[k_disp]:.2f}"); displayed.add(k_disp)
            for k_rem, v_rem in final_scores_print.items():
                if k_rem not in displayed: print(f"  - {get_metric_display_name(k_rem)}: {v_rem:.2f}")
        else: print("  N/A")
        print(f"\n完全なレポートは次の場所に保存されました: {job_specific_output_dir.resolve()}")
        print("="*30 + "\n")
        return results_dict
    else:
        err = process_result.error
        logger_main_single.critical(f"単一ジョブが致命的エラーで失敗: {err}")
        return {
            "job_id": config.generate_job_id("critical_fail_"), "status": "Failed Critically (Processor)",
            "errors": [str(err)], "start_time": datetime.now(timezone.utc).isoformat(),
            "completion_time": datetime.now(timezone.utc).isoformat(), "parameters": vars(args)
        }


def main() -> int:
    """Main execution function for NGGS-Lite v1.8."""
    args = parse_arguments()
    exit_code = 0
    start_time_global = time.monotonic()
    
    # This global logger will be reconfigured by setup_logging
    global logger # pylint: disable=global-statement
    
    # --- Base Config & Logging Setup ---
    try:
        config = NGGSConfig() # Load base config with defaults
        
        # Determine log level from verbosity count
        if args.verbose == 1: log_level_str = "INFO"
        elif args.verbose >= 2: log_level_str = "DEBUG"
        else: log_level_str = config.DEFAULT_LOG_LEVEL
        
        log_file_path_arg = args.log_file
        # Use config default if args.log_file is not explicitly set to something (even "NONE")
        # If args.log_file is "NONE", it means disable. If None, use config.
        effective_log_file = log_file_path_arg if log_file_path_arg is not None else config.DEFAULT_LOG_FILE

        logger = setup_logging(log_level_str, effective_log_file, name="NGGS-Lite", config_instance=config)
        logger.info(f"--- NGGS-Lite v{config.VERSION} 初期化中 ---")
        logger.debug(f"コマンドライン引数 (Raw): {sys.argv}")
        logger.debug(f"コマンドライン引数 (Parsed): {args}")
    except Exception as config_err:
        print(f"致命的エラー: 初期設定またはロギング設定中に失敗: {config_err}", file=sys.stderr)
        traceback.print_exc()
        return 1

    try:
        # --- Override Config with Command-Line Args ---
        if args.model is not None: config.GEMINI_MODEL_NAME = args.model
        if args.nggs_vocab is not None: config.DEFAULT_VOCAB_PATH = args.nggs_vocab
        if args.glcai_vocab is not None: config.GLCAI_VOCAB_PATH = args.glcai_vocab
        if args.output is not None: config.DEFAULT_OUTPUT_DIR = args.output # Allow output dir override
        if args.loops is not None: config.DEFAULT_MAX_LOOPS = args.loops
        if args.length is not None: config.DEFAULT_TARGET_LENGTH = args.length
        if args.threshold is not None: config.DEFAULT_IMPROVEMENT_THRESHOLD = args.threshold
        # Add other config overrides here if needed

        logger.debug(f"有効な設定 (引数適用後): {config}") # Log effective config

        # Determine output directory (priority: args.output > config.DEFAULT_OUTPUT_DIR)
        output_dir_path = pathlib.Path(args.output if args.output else config.DEFAULT_OUTPUT_DIR).resolve()
        logger.info(f"出力ディレクトリ: {output_dir_path}")
        try:
            output_dir_path.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            logger.critical(f"出力ディレクトリの作成に失敗しました {output_dir_path}: {e}")
            raise ConfigurationError(f"出力ディレクトリ作成失敗: {e}") from e


        # --- Initialize Components ---
        processor, glcai_feedback, ndgs_parser_instance = setup_components(config, args)

        # --- Execute Mode ---
        if args.batch:
            input_dir = pathlib.Path(args.batch).resolve()
            batch_processor = BatchProcessor(config, processor, output_dir_path) # Assumes BatchProcessor is defined
            
            # Prepare processor_args for batch mode, filtering out None values
            batch_processor_args = {
                "target_length": args.length, "perspective_mode": args.perspective,
                "phase_focus": args.phase_focus, "colloquial_level": args.colloquial_level,
                "emotion_arc": args.emotion_arc, "max_loops_override": args.loops,
                "threshold_override": args.threshold,
                "skip_initial_generation": args.skip_initial_generation
                # NDGS input is handled per-file if a master NDGS file for batch is not a concept
            }
            batch_processor_args_filtered = {k: v for k, v in batch_processor_args.items() if v is not None}

            summary = batch_processor.process_batch(
                input_dir=input_dir,
                max_workers=args.max_workers,
                save_individual_results=(not args.no_individual_results),
                **batch_processor_args_filtered
            )
            exit_code = 1 if summary.get("failed_jobs", 0) > 0 else 0
        else:
            # Single Mode (handles --input, --text, or --ndgs-input)
            run_single_job(args, config, processor, glcai_feedback, ndgs_parser_instance, output_dir_path)
            # run_single_job doesn't explicitly return exit code, assume 0 unless exception
            # Exception handling below will set exit_code for failures in run_single_job

    except (ConfigurationError, FileProcessingError, TemplateError, VocabularyError, IntegrationError,
            FileNotFoundError, PermissionError, ValueError, LLMError, EvaluationError, NGGSError) as e:
        logger.critical(f"処理エラー: {type(e).__name__} - {e}", exc_info=False)
        if isinstance(e, NGGSError) and e.get_context(): logger.critical(f"エラーコンテキスト: {e.get_context()}")
        print(f"\nエラー: {e}", file=sys.stderr)
        exit_code = 1
    except KeyboardInterrupt:
        logger.warning("処理がユーザーによって中断されました (Ctrl+C)。")
        print("\n処理が中断されました。", file=sys.stderr)
        exit_code = 130 # Standard exit code for SIGINT
    except Exception as e:
        logger.critical(f"予期せぬ致命的なエラーが発生しました: {type(e).__name__} - {e}", exc_info=True)
        print(f"\n予期せぬ致命的エラー: {type(e).__name__} - {e}。", file=sys.stderr)
        print("詳細はログを確認するか、-vv オプション付きで実行してください。", file=sys.stderr)
        exit_code = 1
    finally:
        duration = time.monotonic() - start_time_global
        if 'config' in locals() and hasattr(config, 'VERSION'): # Check if config was initialized
            logger.info(f"総実行時間: {duration:.2f} 秒")
            logger.info(f"--- NGGS-Lite v{config.VERSION} 終了 (終了コード: {exit_code}) ---")
        else: # Config failed to load
            print(f"総実行時間: {duration:.2f} 秒", file=sys.stderr)
            print(f"--- NGGS-Lite 終了 (終了コード: {exit_code}) ---", file=sys.stderr)
        logging.shutdown()

    return exit_code

# =============================================================================
# Part 17 End: Main Execution Function and Entry Point
# =============================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NGGS-Lite v1.8.0: Neo-Gothic Text Generation System (Gemini Edition - Refactored)

Purpose:
Generates high-quality Gothic-style text using Google Gemini,
incorporating iterative evaluation and improvement based on
ETI (Existential Tremor Index), RI (Readability Index), Layer Balance,
Phase Transitions, Subjectivity, and Emotion Arc.
This script is refactored based on the "NGGS-Lite v1.8 Refactoring & Upgrade Plan (Final Validation & Detailing)".
"""

# =============================================================================
# Part 1: Imports, Constants, Core Configuration (NGGSConfig)
# =============================================================================

import os
import sys
import json
import argparse
import logging
import logging.handlers
import time
import signal
import re
import random
import unicodedata
import traceback
import math
import statistics # statisticsモジュールをインポート
import pathlib
from typing import (
    Dict, List, Any, Optional, Union, Tuple, Generic, TypeVar,
    Callable, Set, Type, Final, TypeAlias
)
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from enum import Enum
from abc import ABC, abstractmethod

# --- Attempt to import Google Generative AI library ---
try:
    import google.generativeai as genai
    from google.api_core import exceptions as google_exceptions
    GEMINI_AVAILABLE: Final[bool] = True
except ImportError:
    print("ERROR: google-generativeai library not found.", file=sys.stderr)
    print("Please install it using: pip install google-generativeai", file=sys.stderr)
    # Define dummy classes for type hints and basic checks if library is missing
    class google_exceptions:  # type: ignore
        class GoogleAPIError(Exception): pass
        class ResourceExhausted(GoogleAPIError): pass
        class InternalServerError(GoogleAPIError): pass
        class ServiceUnavailable(GoogleAPIError): pass
        class DeadlineExceeded(GoogleAPIError): pass
        class InvalidArgument(GoogleAPIError): pass
        class PermissionDenied(GoogleAPIError): pass
        class Unauthenticated(GoogleAPIError): pass
        class NotFound(GoogleAPIError): pass
        class FailedPrecondition(GoogleAPIError): pass
    genai = None  # type: ignore
    GEMINI_AVAILABLE = False

# --- Attempt to import OpenAI library (Placeholder for v1.8 - Not actively supported) ---
try:
    import openai  # type: ignore
    # from openai import RateLimitError, APIError, APITimeoutError, BadRequestError, AuthenticationError # Not used in v1.8
    OPENAI_AVAILABLE: Final[bool] = True
except ImportError:
    openai = None
    # Define dummy classes for type hints if needed (though not actively used)
    # class RateLimitError(Exception): pass
    # class APIError(Exception): pass
    # class APITimeoutError(Exception): pass
    # class BadRequestError(Exception): pass
    # class AuthenticationError(Exception): pass
    OPENAI_AVAILABLE = False

# --- Attempt to import concurrent.futures for BatchProcessor ---
try:
    import concurrent.futures
    CONCURRENT_FUTURES_AVAILABLE: Final[bool] = True
except ImportError:
    CONCURRENT_FUTURES_AVAILABLE = False
    # BatchProcessor will fallback to sequential if this is False

# --- Base Directory Initialization (Standardized) ---
try:
    BASE_DIR: Final[pathlib.Path] = pathlib.Path(__file__).resolve().parent
except NameError:
    # Fallback for environments where __file__ might not be defined (e.g., interactive, some execution contexts)
    BASE_DIR = pathlib.Path.cwd()

# --- Type Alias for JSON-like dictionaries ---
JsonDict: TypeAlias = Dict[str, Any]

# --- Dummy ConfigurationError for Part 1 completion ---
# This will be properly defined in Part 2.
class ConfigurationError(Exception):
    """Base exception for configuration-related errors."""
    def __init__(self, message: str, details: Optional[JsonDict] = None):
        super().__init__(message)
        self.details = details if details is not None else {}

# --- Core Configuration Class (NGGSConfig v1.8.0 - Refactored) ---
@dataclass
class NGGSConfig:
    """
    NGGS-Lite v1.8.0 Configuration - Centralized parameters.
    Refactored based on the upgrade plan for clarity, testability,
    and alignment with user guide specifications.
    """
    # --- System Version ---
    VERSION: str = "1.8.0"

    # --- LLM Engine Configuration ---
    LLM_ENGINE: str = "gemini"  # Primary engine ('gemini')

    # --- Gemini API Settings (Aligned with User Guide) ---
    GEMINI_API_KEY_ENV: str = "GENAI_API_KEY"
    GEMINI_API_KEY: Optional[str] = field(default_factory=lambda: os.environ.get(NGGSConfig.GEMINI_API_KEY_ENV))
    GEMINI_MODEL_NAME: str = "models/gemini-1.5-pro-latest"
    GEMINI_RPM_LIMIT: int = int(os.environ.get("GENAI_RPM_LIMIT", "30"))

    # --- API Retry & Timeout Settings (Common) ---
    API_MAX_RETRIES: int = 4
    API_BASE_RETRY_DELAY: float = 1.5  # seconds
    API_MAX_RETRY_DELAY: float = 45.0  # seconds
    API_RATE_LIMIT_DELAY: float = 10.0 # seconds (Initial delay on ResourceExhausted/RateLimitError)
    API_TIMEOUT: int = 300  # seconds for API call

    # --- Generation Parameters (Gemini Defaults) ---
    GENERATION_CONFIG_DEFAULT: JsonDict = field(default_factory=lambda: {
        "temperature": 0.80,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 8192,
        "candidate_count": 1,
        # "stop_sequences": [] # Usually not needed for single turn generation
    })
    EVALUATION_TEMPERATURE: float = 0.20 # For objective evaluation
    IMPROVEMENT_BASE_TEMPERATURE: float = 0.70
    IMPROVEMENT_MIN_TEMPERATURE: float = 0.55
    IMPROVEMENT_TEMP_DECREASE_PER_LOOP: float = 0.05

    # --- Text Generation & Evaluation Control ---
    DEFAULT_TARGET_LENGTH: int = 1200
    DEFAULT_MAX_LOOPS: int = 3
    DEFAULT_IMPROVEMENT_THRESHOLD: float = 4.3 # Overall score target

    # --- ETI (Extended Existential Tremor Index) Evaluation Weights (User Guide Aligned) ---
    EXTENDED_ETI_WEIGHTS: Dict[str, float] = field(default_factory=lambda: {
        "境界性": 0.15,       # Boundary
        "両義性": 0.15,       # Ambivalence
        "超越侵犯": 0.15,     # Transcendental Violation
        "不確定性": 0.15,     # Uncertainty
        "内的変容": 0.15,     # Internal Transformation
        "位相移行": 0.15,     # Phase Transition (heuristic score)
        "主観性": 0.10       # Subjectivity (heuristic score)
    })

    # --- Phase Distribution Targets & Scoring Parameters (User Guide Aligned) ---
    PHASE_BALANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        "serif": 0.15,          # セリフ
        "live_report": 0.25,    # 実況
        "monologue": 0.35,      # モノローグ
        "narration": 0.20,      # ナレーション
        "serif_prime": 0.05     # 変容後セリフ (detection might be challenging)
    })
    PHASE_DEVIATION_TOLERANCE: float = 0.10
    PHASE_SCORE_DIVERSITY_BONUS: float = 0.75
    PHASE_SCORE_BALANCE_PENALTY_FACTOR: float = 3.0
    PHASE_SCORE_FOCUS_TARGET_RATIO: float = 0.40

    # --- Layer Distribution Targets & Scoring Parameters (User Guide Aligned) ---
    LAYER_BALANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        "physical": 0.20,       # 物質層
        "sensory": 0.25,        # 感覚層
        "psychological": 0.35,  # 心理層
        "symbolic": 0.20        # 象徴層
    })
    LAYER_DEVIATION_TOLERANCE: float = 0.08
    LAYER_BALANCE_PENALTY_FACTOR: float = 3.5

    # --- File Paths & Directory Settings (User Guide Aligned) ---
    # Note: These are relative to BASE_DIR or overridden by command-line arguments.
    DEFAULT_OUTPUT_DIR: str = "./nggs_lite_output_v1.8"
    DEFAULT_TEMPLATES_DIR: str = "./templates_v1.8"
    DEFAULT_VOCAB_PATH: str = "./data/gothic_vocabulary_v1.8.json"
    GLCAI_VOCAB_PATH: str = "./data/glcai_export_latest.json"
    VOCAB_LOAD_PRIORITY: List[str] = field(default_factory=lambda: ['glcai', 'default']) # GLCAI > Default File > Internal

    RESUME_DIR_NAME: str = "resume"
    STATS_DIR_NAME: str = "stats"
    PROMPT_DIR_NAME: str = "prompts"
    HTML_REPORT_DIR_NAME: str = "reports"
    GLCAI_FEEDBACK_DIR_NAME: str = "glcai_feedback" # User Guide Aligned

    STATS_FILENAME: str = "generation_stats_v1_8.jsonl"
    RESUME_SUFFIX: str = ".resume.json"
    BACKUP_SUFFIX_FORMAT: str = ".bak.{timestamp}" # Timestamp: YYYYMMDD_HHMMSS_ms
    LOCK_SUFFIX: str = ".lock"
    CORRUPTED_SUFFIX: str = ".corrupted"

    # --- HTML Report Settings (Placeholder for v1.8, more detail in future versions) ---
    REPORT_CHART_COLORS: Dict[str, str] = field(default_factory=lambda: {
        'serif': '#4e79a7', 'live_report': '#f28e2c', 'monologue': '#e15759',
        'narration': '#76b7b2', 'serif_prime': '#59a14f',
        'physical': '#edc949', 'sensory': '#af7aa1', 'psychological': '#ff9da7',
        'symbolic': '#9c755f', 'other': '#bab0ab'
    })

    # --- Logging Configuration (User Guide Aligned) ---
    LOG_LEVELS: Dict[str, int] = field(default_factory=lambda: {
        "DEBUG": logging.DEBUG, "INFO": logging.INFO,
        "WARNING": logging.WARNING, "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL
    })
    DEFAULT_LOG_LEVEL: str = "INFO"
    DEFAULT_LOG_FILE: Optional[str] = "nggs_lite_v1_8.log" # Relative to execution or specified
    LOG_FORMAT: str = '%(asctime)s.%(msecs)03dZ [%(levelname)-7s] [%(name)-25s:%(lineno)d] %(message)s'
    LOG_DATE_FORMAT: str = '%Y-%m-%dT%H:%M:%S' # ISO 8601 for UTC

    # --- Processing settings (Maintained for compatibility, can be overridden) ---
    FEEDBACK_LOOPS: int = 3  # Aligns with DEFAULT_MAX_LOOPS
    MIN_FEEDBACK_LOOPS: int = 1
    MIN_SCORE_THRESHOLD: float = 4.3  # Aligns with DEFAULT_IMPROVEMENT_THRESHOLD
    SAVE_PROMPTS: bool = True # Default to saving prompts, can be overridden

    # --- Keyword Lists (User Guide Aligned, defined within NGGSConfig) ---
    LAYER_KEYWORDS: Dict[str, List[str]] = field(default_factory=lambda: {
        "physical": ["石", "壁", "床", "扉", "窓", "本", "階段", "建物", "部屋", "道", "体", "手", "足", "指", "物", "布", "金属", "木", "椅子", "机", "燭台", "鍵", "杯", "服", "人形", "街", "路地", "城", "塔", "地下室", "屋敷", "鏡台", "肖像画", "古文書"],
        "sensory": ["光", "影", "音", "声", "匂い", "香り", "味", "風", "冷たい", "熱い", "湿った", "暗い", "明るい", "色", "霧", "月", "見る", "聞く", "触れる", "ざらざら", "ぬめり", "響き", "静寂", "囁き", "呻き", "雨音", "足音", "軋み", "赤い", "黒い", "白い", "甘美な", "異臭", "冷気", "暖かさ", "視線", "気配", "腐臭", "黴臭い", "無音", "残響"],
        "psychological": ["思う", "考える", "感じる", "記憶", "夢", "不安", "恐怖", "悲しみ", "喜び", "怒り", "驚き", "戸惑い", "心", "意識", "感情", "懐かしさ", "孤独", "混乱", "疑問", "葛藤", "忘却", "心理", "憂愁", "狂気", "苦悶", "絶望", "衝動", "予感", "直感", "眩暈", "罪悪感", "後悔", "執着", "願望", "メランコリー", "虚無感", "既視感", "焦燥", "安堵"],
        "symbolic": ["鏡", "象徴", "時間", "運命", "境界", "魂", "意味", "存在", "二重性", "深淵", "影", "扉（象徴的）", "窓（象徴的）", "迷宮", "輪廻", "契約", "封印", "呪い", "永遠", "生と死", "謎", "虚無", "回帰", "螺旋", "血（象徴的）", "薔薇（象徴的）", "時計（象徴的）", "仮面", "鍵（象徴的）", "アルカナ", "グリモワール", "宇宙的恐怖", "秩序と混沌", "宿命論", "真実の探求"]
    })
    SUBJECTIVE_INNER_KEYWORDS: List[str] = field(default_factory=lambda: [
        '思う', '感じる', '考える', '気がする', '気づく', '思い出す', '忘れる', '見える', '聞こえる',
        '匂いがする', '味がする', '触れる', '懐かしい', '不安だ', '怖い', '嬉しい', '悲しい', '怒っている',
        '驚いた', '戸惑っている', '心が', '気持ちが', '記憶が蘇る', '意識が遠のく', '夢を見た', '幻覚か', '願う', '望む',
        '期待する', '怯える', '悩む', '迷う', '混乱する', '疑問に思う', '疑う', '違和感がある', '理解した', '認識する',
        '知覚する', '直感が働く', '衝動に駆られる', '欲望を感じる', '希望を抱く', '憧れる', '後悔している', '寂しい', '切ない', '虚しい',
        'ぞくぞくする', '予感がする', '確かめたい', '信じられない', 'なぜだろう', 'どうしてだろう', '～かもしれない', '～に違いない', '～のはずだ'
    ])
    SUBJECTIVE_FIRST_PERSON_PRONOUNS: List[str] = field(default_factory=lambda: [
        '私', '僕', '俺', 'わたし', 'わたくし', 'あたし', '自分', 'ぼく', 'おれ', 'わし', '我', 'われ', '余'
    ])
    SUBJECTIVE_THIRD_PERSON_PRONOUNS: List[str] = field(default_factory=lambda: [ # Primarily for consistency checks, not generation
        '彼', '彼女', '奴', 'あいつ', 'その男', 'その女', '例の男', '例の女', 'かの者'
    ])

    # --- Fallback Improvement Instruction (User Guide Aligned) ---
    DEFAULT_FALLBACK_IMPROVEMENT: str = """
# デフォルト改善指示 (Fallback v1.8)
現在のテキストを、より質の高い新ゴシック文体へと改善してください。特に以下の点を意識してください。

1.  **主観的語りの深化**: 登場人物の内面（思考、感情、感覚）を、行動や環境描写と結びつけながら、より深く掘り下げてください。一人称視点の場合、その視点人物の知覚や解釈を前面に出してください。
2.  **ゴシック的雰囲気の醸成**: 廃墟、影、霧、古物、閉鎖空間、異常な気象などを効果的に描写し、ゴシック特有の不安、神秘、退廃、あるいは美と恐怖の混在する雰囲気を強めてください。
3.  **間接的表現と暗示**: 感情や状況を直接説明せず、環境描写、行動、サブテキスト、象徴的なアイテムや出来事を通じて暗示的に表現してください。読者の解釈の余地を残し、多層的な読みを可能にしてください。
4.  **位相移行の調整**: セリフ、実況、モノローグ、ナレーション等の語りの位相を、物語の効果を高めるように自然に移行させてください。単調な繰り返しを避け、各位相が持つ特性を活かしてください。
5.  **語彙の選択と文体の洗練**: よりゴシック的な響きを持つ語彙（例：深淵、月光、囁き、古城、憂愁、影、両義性、境界、退廃的、荘厳）を文脈に合わせて適切に使用し、文全体の格調を高めてください。
6.  **層バランスの考慮**: 物質層（具体的な物や場所）、感覚層（五感を通じた体験）、心理層（内面世界）、象徴層（隠された意味やテーマ性）の描写がバランス良く含まれるように調整し、物語に深みを与えてください。
"""

    # --- Utility Methods within Config (Maintained for single-file structure) ---
    @staticmethod
    def get_default_paths(base_dir_override: Optional[pathlib.Path] = None) -> Dict[str, pathlib.Path]:
        """Get default path settings based on BASE_DIR or an override."""
        base = base_dir_override if base_dir_override is not None else BASE_DIR
        return {
            "DEFAULT_OUTPUT_DIR": base / NGGSConfig.DEFAULT_OUTPUT_DIR,
            "DEFAULT_TEMPLATES_DIR": base / NGGSConfig.DEFAULT_TEMPLATES_DIR,
            "DEFAULT_VOCAB_PATH": base / NGGSConfig.DEFAULT_VOCAB_PATH,
            "GLCAI_VOCAB_PATH": base / NGGSConfig.GLCAI_VOCAB_PATH,
        }

    @classmethod
    def load_from_file(cls, filepath: Union[str, pathlib.Path]) -> 'NGGSConfig':
        """
        Loads configuration from a JSON file, overriding defaults.
        Note: For v1.8, primary configuration is via class defaults and CLI args.
        This method is a placeholder for more extensive file-based config in the future.
        """
        path = pathlib.Path(filepath).resolve()
        if not path.is_file():
            # ConfigurationError will be defined in Part 2
            raise FileNotFoundError(f"Settings file not found: {path}")
        try:
            if path.suffix.lower() == ".json":
                with open(path, 'r', encoding='utf-8') as f:
                    settings_data = json.load(f)
            else:
                raise ValueError(f"Unsupported settings file format: {path.suffix}")

            instance = cls()  # Start with defaults
            config_logger = logging.getLogger("NGGS-Lite.Config") # Get logger instance

            for key, value in settings_data.items():
                if hasattr(instance, key):
                    # Basic type check/conversion might be needed for robustness
                    # For nested dataclasses/dicts, recursive update might be required
                    try:
                        current_type = type(getattr(instance, key))
                        if current_type is type(value) or value is None: # Allow None to override
                            setattr(instance, key, value)
                        else: # Attempt conversion for basic types
                            if current_type is int: setattr(instance, key, int(value))
                            elif current_type is float: setattr(instance, key, float(value))
                            elif current_type is bool: setattr(instance, key, str(value).lower() in ['true', '1', 'yes'])
                            elif current_type is str: setattr(instance, key, str(value))
                            else:
                                config_logger.warning(f"Config key '{key}': Type mismatch (expected {current_type}, got {type(value)}). Skipping override.")
                    except Exception as e_conv:
                        config_logger.warning(f"Config key '{key}': Error converting value '{value}' to {current_type}: {e_conv}. Skipping override.")
                else:
                    config_logger.warning(f"Config key '{key}' not found in NGGSConfig. Skipping.")
            
            config_logger.info(f"Configuration partially loaded from {filepath}. Review defaults and CLI args.")
            return instance
        except Exception as e:
            # Use RuntimeError as ConfigurationError is not yet fully defined in this part
            raise RuntimeError(f"Error loading settings from {filepath}: {e}") from e

    @staticmethod
    def generate_job_id(prefix: str = "nggsv18_") -> str:
        """Generates a unique job ID with microseconds."""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_%f") # Includes microseconds
        return f"{prefix}{timestamp}"

    @classmethod
    def initialize_base_directories(cls, base_output_dir: pathlib.Path, config: 'NGGSConfig') -> None:
        """Creates base output directories defined in the config, if they don't exist."""
        # This method assumes 'config' is an instance of NGGSConfig.
        dirs_to_create = [
            base_output_dir,
            base_output_dir / config.RESUME_DIR_NAME,
            base_output_dir / config.STATS_DIR_NAME,
            base_output_dir / config.PROMPT_DIR_NAME,
            base_output_dir / config.HTML_REPORT_DIR_NAME,
            base_output_dir / config.GLCAI_FEEDBACK_DIR_NAME,
        ]
        init_logger = logging.getLogger("NGGS-Lite.Init") # Get logger instance
        init_logger.info(f"Ensuring base directories exist under: {base_output_dir}")
        for dir_path in dirs_to_create:
            try:
                dir_path.mkdir(parents=True, exist_ok=True)
                init_logger.debug(f"Directory verified/created: {dir_path}")
            except PermissionError as e_perm:
                err_msg = f"Directory creation permission error: {dir_path} - {e_perm}"
                init_logger.critical(err_msg)
                # Use RuntimeError as ConfigurationError is not yet fully defined in this part
                raise RuntimeError(err_msg) from e_perm
            except OSError as e_os: # Catch other OS-level errors
                err_msg = f"Directory creation OS error: {dir_path} - {e_os}"
                init_logger.critical(err_msg)
                raise RuntimeError(err_msg) from e_os
            except Exception as e_general: # Catch-all for other unexpected errors
                err_msg = f"Unexpected error creating directory: {dir_path} - {e_general}"
                init_logger.critical(err_msg)
                raise RuntimeError(err_msg) from e_general
        init_logger.debug("Base directory initialization complete.")

# =============================================================================
# Part 1 End: Imports, Constants, Core Configuration (NGGSConfig)
# =============================================================================
# =============================================================================
# Part 2: Core Exceptions and Result Type (Refactored)
# =============================================================================
# Part 1で定義されたJsonDict, GEMINI_AVAILABLE, google_exceptions, ConfigurationError を
# このスコープで利用可能であることを前提とします。
# logging, enum, dataclasses, typing.TypeVarなどもPart1でインポート済みです。

import logging # Part 1でインポート済みだが、明示的に使用するため再確認
from enum import Enum # Part 1でインポート済みだが、明示的に使用するため再確認
from dataclasses import dataclass, field # Part 1でインポート済みだが、明示的に使用するため再確認
from typing import ( # Part 1でインポート済みの型もあるが、このPartで必要なものを明示
    Dict, List, Any, Optional, Union, Tuple, Generic, TypeVar, Type, Final
)

# Part 1で定義された型エイリアスと定数をここで利用可能と仮定
# JsonDict: TypeAlias = Dict[str, Any] (Part 1で定義済み)
# GEMINI_AVAILABLE: Final[bool] (Part 1で定義済み)
# google_exceptions: Any (Part 1で定義済み、実物またはダミー)

# --- Custom Exception Classes (Hierarchy Refined v1.8) ---
class NGGSError(Exception):
    """
    NGGS-Liteの全てのカスタム例外の基底クラス。
    エラーに関する詳細なコンテキスト情報を保持できます。
    推奨されるdetailsキー:
      - 'failed_component': エラーが発生したコンポーネント名 (例: "VocabularyManager")
      - 'input_preview': 問題を引き起こした可能性のある入力データのプレビュー
      - 'operation': 実行しようとしていた操作 (例: "load_template")
      - 'original_exception_type': 元の例外の型名 (ラップしている場合)
      - 'original_exception_message': 元の例外のメッセージ (ラップしている場合)
    """
    def __init__(self, message: str, details: Optional[JsonDict] = None):
        """
        NGGSErrorを初期化します。

        Args:
            message: エラーメッセージ。
            details: エラーに関する追加の詳細情報を含む辞書 (オプション)。
        """
        super().__init__(message)
        self.details: JsonDict = details if details is not None else {}

    def get_context(self) -> JsonDict:
        """エラーコンテキストの詳細を返します。"""
        return self.details.copy() # 変更を防ぐためコピーを返す

    def __str__(self) -> str:
        """エラーメッセージとコンテキストの要約を文字列として返します。"""
        base_message = super().__str__()
        if self.details:
            try:
                # ログ出力用にコンテキスト情報を簡潔にフォーマット
                details_str_parts = []
                for k, v in self.details.items():
                    v_str = str(v)
                    if len(v_str) > 70: # 長すぎる値は切り詰める
                        v_str = v_str[:67] + "..."
                    details_str_parts.append(f"{k}='{v_str}'")
                details_summary = ", ".join(details_str_parts)
                return f"{base_message} (Context: {details_summary})"
            except Exception:  # フォーマット中にエラーが発生した場合のフォールバック
                return f"{base_message} (Context available but failed to format for __str__)"
        return base_message

# ConfigurationErrorはPart 1の末尾でダミー定義されているが、ここで正式に定義
class ConfigurationError(NGGSError):
    """設定やセットアップに関連するエラー。"""
    pass

class FileProcessingError(NGGSError):
    """ファイルの読み書き中に発生するエラー。"""
    pass

class TemplateError(NGGSError):
    """テンプレートの読み込みや検証に関連するエラー。"""
    pass

class VocabularyError(NGGSError):
    """語彙の処理に関連するエラー。"""
    pass

class LLMError(NGGSError):
    """LLMとのインタラクション中に発生するエラーの基底クラス。"""
    pass

class GenerationError(LLMError):
    """テキスト生成フェーズ特有のエラー。
    LLMがコンテンツを生成できなかった場合（例：空応答、安全フィルターによるブロック）など。
    """
    pass

class EvaluationError(LLMError):
    """テキスト評価中や評価結果の解析中に発生するエラー。"""
    pass

class JsonParsingError(EvaluationError):
    """LLMからの評価応答（JSON形式を期待）の解析失敗に特化したエラー。"""
    pass

class PhaseTransitionError(NGGSError):
    """物語の位相分析や遷移に関連するエラー。"""
    pass

class StrategyError(NGGSError):
    """改善戦略の選択や生成に関連するエラー。"""
    pass

class IntegrationError(NGGSError):  # v1.8で新規追加
    """NGGSと他のシステム（例：NDGS, GLCAI）との連携に関するエラー。"""
    pass


# --- Error Severity Definition ---
class ErrorSeverity(Enum):
    """エラーの重大度を表します。"""
    FATAL = "fatal"          # プロセス全体を停止させるエラー
    LOOP = "loop"            # 現在の生成/改善ループをスキップさせるエラー
    RECOVERABLE = "recoverable"  # フォールバックやデフォルト値の使用を許可するエラー

# --- Error Severity Mapping (v1.8 Adjusted) ---
# 例外タイプをその重大度にマッピングし、一元的なエラー処理を可能にします。
# google_exceptions のマッピングは GEMINI_AVAILABLE フラグに依存します。

_ERROR_SEVERITY_MAP_BASE: Dict[Type[Exception], ErrorSeverity] = {
    # NGGS-Lite Specific Errors
    ConfigurationError: ErrorSeverity.FATAL,
    FileProcessingError: ErrorSeverity.FATAL,  # セットアップや出力に関するファイルエラーの多くは致命的
    TemplateError: ErrorSeverity.FATAL,      # テンプレートは処理に不可欠
    VocabularyError: ErrorSeverity.RECOVERABLE, # デフォルト値で処理を続行できる可能性あり
    IntegrationError: ErrorSeverity.RECOVERABLE, # 連携失敗時も処理続行を試みる (またはLOOP)
    GenerationError: ErrorSeverity.LOOP,       # 生成失敗時はループをスキップ
    EvaluationError: ErrorSeverity.RECOVERABLE, # 基本評価失敗時はデフォルトスコアを使用
    JsonParsingError: ErrorSeverity.RECOVERABLE,  # デフォルトスコアを使用
    PhaseTransitionError: ErrorSeverity.RECOVERABLE, # デフォルトのスコア/分布を使用
    StrategyError: ErrorSeverity.RECOVERABLE,    # 標準的なLLM指示にフォールバック
    LLMError: ErrorSeverity.LOOP,            # 一般的なLLMエラーはループスキップ可能

    # Python Standard / Common Dependencies
    FileNotFoundError: ErrorSeverity.FATAL,      # 必須ファイルが見つからない場合
    PermissionError: ErrorSeverity.FATAL,        # 権限エラー
    IOError: ErrorSeverity.FATAL,                # 広範なI/Oエラー
    OSError: ErrorSeverity.FATAL,                # ディレクトリエラーなどを含むOSレベルのエラー
    json.JSONDecodeError: ErrorSeverity.RECOVERABLE, # コンテキスト依存で回復可能
    TimeoutError: ErrorSeverity.LOOP,            # ネットワーク/APIタイムアウト
    ConnectionError: ErrorSeverity.LOOP,         # ネットワーク接続問題
    TypeError: ErrorSeverity.RECOVERABLE,        # データ形式の問題が多い
    KeyError: ErrorSeverity.RECOVERABLE,         # 辞書アクセス問題が多い
    ValueError: ErrorSeverity.RECOVERABLE,       # コンテキスト依存 (例: 安全性ブロック vs データ問題)
    AttributeError: ErrorSeverity.RECOVERABLE,   # 属性アクセスエラー
    IndexError: ErrorSeverity.RECOVERABLE,       # インデックス範囲外エラー
    RuntimeError: ErrorSeverity.LOOP,            # ループ内で回復可能なことが多い
    NotImplementedError: ErrorSeverity.FATAL,    # 機能未実装

    # Default for unmapped exceptions
    Exception: ErrorSeverity.FATAL
}

# GEMINI_AVAILABLE フラグに基づいて Gemini 固有の例外マッピングを追加
ERROR_SEVERITY_MAP: Dict[Type[Exception], ErrorSeverity] = _ERROR_SEVERITY_MAP_BASE.copy()
if GEMINI_AVAILABLE and google_exceptions: # google_exceptionsがNoneでないことも確認
    ERROR_SEVERITY_MAP.update({
        google_exceptions.ResourceExhausted: ErrorSeverity.LOOP,
        google_exceptions.InternalServerError: ErrorSeverity.LOOP,
        google_exceptions.ServiceUnavailable: ErrorSeverity.LOOP,
        google_exceptions.DeadlineExceeded: ErrorSeverity.LOOP,
        google_exceptions.InvalidArgument: ErrorSeverity.FATAL,  # 不正なリクエストはリトライ困難
        google_exceptions.PermissionDenied: ErrorSeverity.FATAL,
        google_exceptions.Unauthenticated: ErrorSeverity.FATAL,
        google_exceptions.NotFound: ErrorSeverity.FATAL,          # 例: モデルが見つからない
        google_exceptions.FailedPrecondition: ErrorSeverity.FATAL,
        # google_exceptions.GoogleAPIError: ErrorSeverity.LOOP, # より具体的な型でカバー
    })
elif not GEMINI_AVAILABLE and google_exceptions: # Geminiが利用不可でもダミークラスが存在する場合
    # Part 1で定義されたダミーのgoogle_exceptionsクラスをマッピング
    # (ダミークラスの属性名が実際の例外クラス名と一致していることを前提とする)
    dummy_exception_map = {
        k: v for k, v in {
            getattr(google_exceptions, 'ResourceExhausted', None): ErrorSeverity.LOOP,
            getattr(google_exceptions, 'InternalServerError', None): ErrorSeverity.LOOP,
            getattr(google_exceptions, 'ServiceUnavailable', None): ErrorSeverity.LOOP,
            getattr(google_exceptions, 'DeadlineExceeded', None): ErrorSeverity.LOOP,
            getattr(google_exceptions, 'InvalidArgument', None): ErrorSeverity.FATAL,
            getattr(google_exceptions, 'PermissionDenied', None): ErrorSeverity.FATAL,
            getattr(google_exceptions, 'Unauthenticated', None): ErrorSeverity.FATAL,
            getattr(google_exceptions, 'NotFound', None): ErrorSeverity.FATAL,
            getattr(google_exceptions, 'FailedPrecondition', None): ErrorSeverity.FATAL,
        }.items() if k is not None # 属性が存在しない場合はスキップ
    }
    ERROR_SEVERITY_MAP.update(dummy_exception_map) # type: ignore


# --- Result Type (Generic for Success/Error Handling) ---
T = TypeVar('T')  # 成功時の値の型
E = TypeVar('E', bound=Exception)  # エラー時の例外の型 (Exceptionのサブクラスに限定)

@dataclass(frozen=True) # Immutableなデータクラス
class Result(Generic[T, E]):
    """
    成功(Ok)または失敗(Err)いずれかの結果を保持するジェネリックなResult型。
    Rustのstd::result::Resultに触発された設計。
    エラーは常にExceptionインスタンスであることを保証します。
    """
    _is_success: bool # 内部フラグ: Trueなら成功、Falseなら失敗
    _value: Optional[T] = field(default=None, compare=False, repr=False) # 成功時の値 (失敗時はNone)
    _error: Optional[E] = field(default=None, compare=False)             # 失敗時のエラー (成功時はNone)

    @property
    def is_ok(self) -> bool:
        """このResultが成功(Ok)を表す場合はTrueを返します。"""
        return self._is_success

    @property
    def is_err(self) -> bool:
        """このResultが失敗(Err)を表す場合はTrueを返します。"""
        return not self._is_success

    @property
    def value(self) -> Optional[T]:
        """成功(Ok)の場合は値を、失敗(Err)の場合はNoneを返します。"""
        return self._value if self.is_ok else None

    @property
    def error(self) -> Optional[E]:
        """失敗(Err)の場合はエラーオブジェクトを、成功(Ok)の場合はNoneを返します。"""
        return self._error if self.is_err else None

    @classmethod
    def ok(cls, value: T) -> 'Result[T, Any]': # エラー型はAnyで、実際にはNoneになる
        """成功(Ok)を表すResultインスタンスを作成します。"""
        # Ok(None) も許容する (値がない成功を表す場合があるため)
        return cls(_is_success=True, _value=value, _error=None)

    @classmethod
    def fail(cls, error: E) -> 'Result[Any, E]': # 成功値の型はAnyで、実際にはNoneになる
        """失敗(Err)を表すResultインスタンスを作成します。"""
        if not isinstance(error, Exception):
            # errorがExceptionインスタンスでない場合は、NGGSErrorでラップする
            err_msg = f"Result.failにはExceptionインスタンスが必要です (受け取った型: {type(error)})."
            try:
                # ロガーが利用可能であればエラーを記録
                core_logger = logging.getLogger("NGGS-Lite.Core")
                if core_logger.hasHandlers(): # ロガーが設定されているか確認
                    core_logger.error(err_msg + " NGGSErrorでラップします。")
                else: # ロガーが設定されていない場合のフォールバック
                    print(f"ERROR (Result.fail): {err_msg} NGGSErrorでラップします。", file=sys.stderr)
            except Exception: # ロガー取得自体でエラーが発生した場合
                print(f"ERROR (Result.fail - logging error): {err_msg} NGGSErrorでラップします。", file=sys.stderr)
            
            # NGGSErrorでラップする。errorがNGGSErrorのコンストラクタと互換性がない場合も考慮
            try:
                wrapped_error = NGGSError(str(error), details={"original_type": str(type(error)), "original_value": str(error)}) # type: ignore
            except Exception as e_wrap:
                # ラップ処理自体でエラーが発生した場合の最終フォールバック
                wrapped_error = NGGSError(f"Failed to wrap non-exception error: {e_wrap}", details={"original_type": str(type(error))}) # type: ignore
            return cls(_is_success=False, _value=None, _error=wrapped_error) # type: ignore
        return cls(_is_success=False, _value=None, _error=error)

    # Errはfailのエイリアスとして維持 (既存コードとの互換性のため)
    @classmethod
    def Err(cls, error: E) -> 'Result[Any, E]':
        # pylint: disable=invalid-name
        return cls.fail(error)

    def unwrap(self) -> T:
        """
        成功(Ok)の場合は値を返します。失敗(Err)の場合は保持しているエラーを送出します。
        注意: 成功(Ok)でも値がNoneの場合があり得るため、呼び出し側で適切に処理する必要があります。
        """
        if self.is_ok:
            # TがOptionalでない場合、self._valueがNoneだと型エラーになる可能性があるが、
            # Result.ok(None) を許容しているため、ここではそのまま返す。
            return self._value # type: ignore
        else:
            if self.error is not None:
                raise self.error
            else:
                # fail()のロジックにより、このケースは理論上発生しないはず
                raise NGGSError("unwrap()がエラーなしのErr Resultに対して呼び出されました。")

    def unwrap_or(self, default_value: T) -> T:
        """成功(Ok)の場合は値を、失敗(Err)の場合は指定されたデフォルト値を返します。"""
        return self._value if self.is_ok and self._value is not None else default_value

    def expect(self, message: str) -> T:
        """
        成功(Ok)の場合は値を返します。失敗(Err)の場合は、指定されたメッセージを持つNGGSErrorを送出します。
        元のエラーは新しいNGGSErrorの原因として保持されます。
        """
        if self.is_ok:
            return self._value # type: ignore
        else:
            original_error_str = str(self.error) if self.error else "エラーオブジェクトなし"
            error_details: JsonDict = {"original_error_message": original_error_str}
            if isinstance(self.error, NGGSError):
                # 元のエラーがNGGSErrorの場合、そのコンテキストも引き継ぐ
                error_details.update(self.error.get_context())
                error_details["original_nggs_error_code"] = self.error.code # type: ignore
                error_details["original_nggs_error_source"] = self.error.source # type: ignore
            elif self.error is not None:
                error_details["original_error_type"] = type(self.error).__name__

            # NGGSErrorを生成し、元のエラーを原因として設定
            raise NGGSError(message, details=error_details) from self.error


# --- LLM Response Dataclass ---
@dataclass
class LLMResponse:
    """
    LLMからの応答を標準化して保持するデータクラス。
    text: LLMが生成した主要なテキストコンテンツ。
    metadata: モデル名、使用トークン数、API呼び出し時間など、応答に関する追加メタデータ。
    prompt_feedback: (Gemini等) プロンプトレベルでのフィードバック (例: ブロック理由)。
    finish_reason: (Gemini等) 生成が終了した理由 (例: STOP, SAFETY, MAX_TOKENS)。
    safety_ratings: (Gemini等) コンテンツの安全性評価。
    """
    text: str
    metadata: JsonDict = field(default_factory=dict)
    prompt_feedback: Optional[Any] = None # 実際の型はAPIライブラリに依存
    finish_reason: Optional[Any] = None   # 実際の型はAPIライブラリに依存
    safety_ratings: Optional[List[Any]] = None # 実際の型はAPIライブラリに依存

    def to_result(self) -> Result[str, GenerationError]:
        """
        LLMResponseをResultオブジェクトに変換します。
        テキストが空の場合や、安全フィルターでブロックされた場合はGenerationErrorを返します。
        """
        # 改善計画に基づき、finish_reason や prompt_feedback も考慮
        if self.finish_reason: # Gemini API の Candidate.FinishReason など
            # finish_reason が SAFETY や RECITATION などの問題を示す場合
            # (実際のEnum値や文字列はLLMClient側での判定に依存するが、ここでは一般的な例)
            # genai.types.Candidate.FinishReason.SAFETY などと比較する想定
            # ここでは文字列で比較する例を示す（実際の比較はLLMClient側で行うべき）
            finish_reason_str = str(self.finish_reason).upper()
            if "SAFETY" in finish_reason_str or "RECITATION" in finish_reason_str:
                details: JsonDict = {"metadata": self.metadata, "finish_reason": str(self.finish_reason)}
                if self.safety_ratings:
                    details["safety_ratings"] = [str(r) for r in self.safety_ratings]
                return Result.fail(GenerationError("LLM生成コンテンツが安全フィルターまたはポリシーによりブロックされました。", details=details))

        if self.prompt_feedback: # Gemini API の PromptFeedback など
            # prompt_feedback にブロック理由が含まれる場合
            # block_reason_str = str(getattr(self.prompt_feedback, 'block_reason', None))
            # if "SAFETY" in block_reason_str: # ダミーのチェック
            # 上記のチェックはLLMClient側で行い、GenerationErrorを発生させる方が適切。
            # ここでは、textが空の場合のチェックに集中。
            pass # LLMClient側で処理

        if not self.text or not self.text.strip():
            details = {"metadata": self.metadata}
            if self.finish_reason: details["finish_reason"] = str(self.finish_reason)
            return Result.fail(GenerationError("LLMが空のテキストコンテンツを返しました。", details=details))
        
        return Result.ok(self.text.strip()) # 成功時はトリムされたテキストを返す

# =============================================================================
# Part 2 End: Core Exceptions and Result Type (Refactored)
# =============================================================================
# =============================================================================
# Part 3: Utility Functions (v1.8 Refined)
# =============================================================================
# Part 1, 2で定義された型やクラス (NGGSConfig, Result, FileProcessingError, JsonDictなど)
# および標準ライブラリ (logging, json, pathlib, os, sys, time, datetime, enum, shutilなど)
# が利用可能であることを前提とします。

import logging
import json
import pathlib
import os
import sys
import time
from datetime import datetime, timedelta, timezone # Part 1から
from enum import Enum # Part 1から
import shutil # backup_file で使用

# Part 1 から NGGSConfig をインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig # モジュール分割されている場合
# Part 2 から Result, FileProcessingError をインポート
# from nggs_lite_part2 import Result, FileProcessingError, JsonDict # モジュール分割されている場合

# --- Helper Classes (Defined here for use in Utils & JSON serialization) ---

class SafeDict(dict):
    """
    KeyErrorを送出する代わりに'{key}'を返す辞書のサブクラス。
    主に、キーが存在しない可能性がある文字列のフォーマットに使用されます。
    """
    def __missing__(self, key: str) -> str:
        try:
            # NGGS-Lite.Utilsロガーが利用可能であれば使用
            utils_logger = logging.getLogger("NGGS-Lite.Utils")
            # ロガーが設定済みの場合のみログ出力 (hasHandlers()はPython 3.7+)
            # Python 3.6以前を考慮する場合は、より単純なチェックか、
            # logging.getLogger().handlers が空でないかで判定
            if utils_logger.handlers:
                utils_logger.debug(f"SafeDict missing key: {key}")
        except Exception:
            # ロギングアクセス自体がエラーになる場合 (初期化の非常に早い段階など) は無視
            pass
        return f'{{{key}}}'  # 見つからない場合はキー自体を返す

class CompactJSONEncoder(json.JSONEncoder):
    """
    特定の型（Path, Enum, timedelta, datetimeなど）を処理し、
    ログやプロンプトに適したコンパクトな出力を生成するカスタムJSONエンコーダ。
    EvaluationResultのようなオブジェクトもダックタイピングで処理します。
    """
    def default(self, obj: Any) -> Any:
        if isinstance(obj, pathlib.Path):
            return str(obj)
        if isinstance(obj, Enum): # Enum型は .value でシリアライズ
            return obj.value
        if isinstance(obj, datetime):
            if obj.tzinfo is None: # naiveなdatetimeはUTCとして扱う
                obj = obj.replace(tzinfo=timezone.utc)
            return obj.isoformat().replace('+00:00', 'Z') # ISO 8601形式 (UTC)
        if isinstance(obj, timedelta):
            return obj.total_seconds()

        # EvaluationResultのようなオブジェクトのダックタイピング
        # (Part 8で定義されるEvaluationResultを想定)
        # scores, reasoning, analysis, components, distribution, confidence 属性を持つか確認
        if all(hasattr(obj, attr) for attr in ['scores', 'reasoning']):
            # truncate_text関数がこのスコープで利用可能であることを前提
            # グローバルに定義するか、このクラスのメソッドとして渡す必要がある
            # ここではグローバル関数として利用できると仮定
            analysis_text = getattr(obj, 'analysis', None)
            truncated_analysis = truncate_text(analysis_text, 150) if analysis_text else None

            return {
                "scores": getattr(obj, 'scores', {}), # dictであることを期待
                "reasoning": getattr(obj, 'reasoning', {}), # dictであることを期待
                "analysis": truncated_analysis,
                "components": getattr(obj, 'components', {}), # dictであることを期待
                "distribution": getattr(obj, 'distribution', {}), # dictであることを期待
                "confidence": getattr(obj, 'confidence', {}) # dictであることを期待
            }

        try:
            return super().default(obj)
        except TypeError:
            # シリアライズ不可能な他のオブジェクトは簡潔に表現
            return f"<{type(obj).__name__} object (unserializable)>"


# --- Logging Setup Function ---
# グローバルなloggerインスタンスはPart 1の末尾で初期化されている想定
# この関数は、そのグローバルloggerを引数やNGGSConfigに基づいて再設定する
def setup_logging(
    log_level_str: Optional[str] = None,
    log_file: Optional[str] = None, # 'NONE'でファイルログ無効化
    name: str = "NGGS-Lite", # ルートロガー名
    config_instance: Optional[NGGSConfig] = None # NGGSConfigインスタンスを渡す
) -> logging.Logger:
    """
    NGGSConfigのデフォルトまたは提供された設定を使用して、アプリケーション全体のロギングを設定します。
    コンソールと、オプションでローテーションするファイルの両方にログを出力します。
    """
    effective_config: NGGSConfig
    if config_instance is None:
        try:
            # この関数が呼び出される時点ではNGGSConfigは定義済みのはず
            effective_config = NGGSConfig()
        except Exception as e_cfg_init:
            # NGGSConfigのデフォルトインスタンス作成に失敗するクリティカルなケース
            print(f"CRITICAL: Logging setup failed to create default NGGSConfig: {e_cfg_init}", file=sys.stderr)
            logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
            return logging.getLogger(name) # 基本的なロガーを返す
    else:
        effective_config = config_instance

    # NGGSConfigからロギング関連の設定値を取得
    final_log_level_str = log_level_str if log_level_str is not None else effective_config.DEFAULT_LOG_LEVEL
    # log_file引数がNoneの場合のみconfigのデフォルトを使用。"NONE"が渡された場合はそれを尊重。
    final_log_file_path_str = log_file if log_file is not None else effective_config.DEFAULT_LOG_FILE
    log_format_str = effective_config.LOG_FORMAT
    log_date_format_str = effective_config.LOG_DATE_FORMAT
    log_levels_map = effective_config.LOG_LEVELS
    # NGGSConfigにファイルローテーションパラメータがない場合、ここでデフォルト値を定義
    log_max_bytes = getattr(effective_config, 'LOG_MAX_BYTES_DEFAULT_VAL', 5 * 1024 * 1024) # 例: 5MB
    log_backup_count = getattr(effective_config, 'LOG_BACKUP_COUNT_DEFAULT_VAL', 3) # 例: 3 backups

    log_level_int = log_levels_map.get(final_log_level_str.upper(), logging.INFO)
    
    # ルートロガーを取得または作成
    # (Part 1で初期化されたグローバル`logger`と同じインスタンスを操作する)
    root_logger_instance = logging.getLogger(name)

    # 既存のハンドラをクリアして再設定 (複数回呼び出される可能性を考慮)
    if root_logger_instance.hasHandlers():
        for handler_item in root_logger_instance.handlers[:]:
            try:
                handler_item.flush()
                handler_item.close()
            except Exception:
                pass # ハンドラ終了中のエラーは無視
            root_logger_instance.removeHandler(handler_item)

    root_logger_instance.setLevel(log_level_int) # ロガー自体のレベルを設定

    # フォーマッタ設定 (UTCタイムゾーン使用)
    logging.Formatter.converter = time.gmtime # type: ignore
    log_formatter = logging.Formatter(log_format_str, datefmt=log_date_format_str)

    # コンソールハンドラ (常に追加)
    console_log_handler = logging.StreamHandler(sys.stdout) # 標準出力へ
    console_log_handler.setLevel(log_level_int)
    console_log_handler.setFormatter(log_formatter)
    root_logger_instance.addHandler(console_log_handler)

    # ファイルハンドラ (ローテーション機能付き、オプション)
    actual_log_file_to_use: Optional[str] = None
    if final_log_file_path_str and isinstance(final_log_file_path_str, str) and \
       final_log_file_path_str.upper() != 'NONE':
        actual_log_file_to_use = final_log_file_path_str

    log_initialization_message: str
    if actual_log_file_to_use:
        try:
            log_file_resolved_path = pathlib.Path(actual_log_file_to_use).resolve()
            # 親ディレクトリが存在しない場合は作成
            log_file_resolved_path.parent.mkdir(parents=True, exist_ok=True)

            rotating_file_handler = logging.handlers.RotatingFileHandler(
                log_file_resolved_path,
                maxBytes=log_max_bytes,
                backupCount=log_backup_count,
                encoding='utf-8'
            )
            rotating_file_handler.setLevel(log_level_int)
            rotating_file_handler.setFormatter(log_formatter)
            root_logger_instance.addHandler(rotating_file_handler)
            log_initialization_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level_int)}. "
                f"Logging to console and file: {log_file_resolved_path}"
            )
        except PermissionError:
            # コンソールハンドラは既に設定されているため、root_logger_instanceでエラーログ出力
            root_logger_instance.error(
                f"Permission denied to write log file: {actual_log_file_to_use}. File logging disabled."
            )
            log_initialization_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level_int)}. "
                f"Logging to console ONLY (File permission error)."
            )
        except Exception as e_file_log:
            root_logger_instance.error(
                f"Failed to set up file logging to {actual_log_file_to_use}: {e_file_log}", exc_info=True
            )
            log_initialization_message = (
                f"Logging setup complete. Level: {logging.getLevelName(log_level_int)}. "
                f"Logging to console ONLY (File setup error)."
            )
    else:
        log_initialization_message = (
            f"Logging setup complete. Level: {logging.getLevelName(log_level_int)}. Logging to console only."
        )

    # 初期化メッセージをログに出力 (ハンドラが設定されていれば)
    if root_logger_instance.hasHandlers():
        root_logger_instance.info(f"--- NGGS-Lite v{effective_config.VERSION} Logging Initialized ---")
        root_logger_instance.info(log_initialization_message)
    else: # ハンドラ設定に失敗した場合のフォールバック
        print(f"--- NGGS-Lite v{effective_config.VERSION} Logging Initialized (Fallback Print) ---", file=sys.stderr)
        print(log_initialization_message, file=sys.stderr)

    return root_logger_instance


# --- Text Processing Utilities ---
def truncate_text(text: Optional[Any], max_length: int = 100) -> str:
    """
    テキストを指定された最大長に切り詰め、必要であれば省略記号(...)を追加します。
    Noneや文字列に変換できない入力は安全に処理します。
    """
    if text is None:
        return ""
    
    text_str: str
    if not isinstance(text, str):
        try:
            text_str = str(text)
        except Exception:
            # ロガーが利用可能か確認してからログ出力
            try:
                utils_trunc_logger = logging.getLogger("NGGS-Lite.Utils.Truncate")
                if utils_trunc_logger.handlers:
                    utils_trunc_logger.warning(
                        f"Failed to convert non-string input (type: {type(text)}) to string in truncate_text."
                    )
            except Exception: # ロギング自体でエラーが発生した場合
                pass # 無視
            return "" # 変換失敗時は空文字列を返す
    else:
        text_str = text

    if len(text_str) <= max_length:
        return text_str
    else:
        # 省略記号のためのスペースを考慮
        if max_length < 3: # 省略記号を表示するスペースがない場合
            return text_str[:max_length]
        return text_str[:max_length - 3] + "..."


def get_metric_display_name(key: Optional[str]) -> str:
    """
    内部メトリックキーを人間が読みやすい日本語名に変換します (v1.8 Refined)。
    None入力や未知のキーは適切に処理します。
    """
    if not key or not isinstance(key, str):
        return "不明な指標"

    # ユーザーガイド1.2節の全評価指標、NGGSConfigパラメータキーに対応
    display_map: Dict[str, str] = {
        # LLM基本評価項目 (例)
        "gothic_atmosphere": "ゴシック的雰囲気",
        "stylistic_gravity": "文体の荘重さ/質",
        "indirect_emotion": "感情表現の間接性",
        "vocabulary_richness": "語彙の質と豊かさ",
        "overall_quality": "全体的な質 (LLM評価)", # LLM評価であることを明記
        "subjective_depth": "主観的語りの深さ (LLM評価)",
        "phase_transition": "位相移行の自然さ (LLM評価)",
        "colloquial_gothic_blend": "口語/ゴシック融合度 (LLM評価)",
        "layer_balance": "層バランス (LLM評価)",
        "emotion_arc_quality": "感情変容の質 (LLM評価)",

        # ETI (拡張存在論的震え指数) 関連
        "境界性": "境界性 (ETI要素)",
        "両義性": "両義性 (ETI要素)",
        "超越侵犯": "超越侵犯 (ETI要素)",
        "不確定性": "不確定性 (ETI要素)",
        "内的変容": "内的変容 (ETI要素)",
        "位相移行": "位相移行 (ETI要素 - ヒューリスティック)", # ETI内のヒューリスティック評価
        "主観性": "主観性 (ETI要素 - ヒューリスティック)",   # ETI内のヒューリスティック評価
        "eti_total_calculated": "ETI計算値総合",
        "eti_from_llm": "ETI (LLM直接評価値)", # LLMが直接ETIを評価する場合
        "eti": "ETIスコア (主要値)", # 最も代表的なETIスコア

        # RI (可読性指数) 関連
        "clarity": "構造的明瞭性 (RI要素)",
        "visualRhythm": "視覚的リズム (RI要素)",
        "emotionalFlow": "感情フロー (RI要素)",
        "cognitiveLoad": "認知処理負荷 (RI要素)",
        "interpretiveResonance": "解釈的余韻 (RI要素)",
        "ri_total_calculated": "RI計算値総合",
        "ri": "RIスコア (主要値)",

        # 主観性スコア (SubjectiveEvaluator) 関連
        "first_person_score": "一人称使用スコア (主観性要素)",
        "inner_expression_score": "内的表現スコア (主観性要素)",
        "monologue_quality_score": "モノローグ質スコア (主観性要素)",
        "consistency_score": "視点一貫性スコア (主観性要素)",
        "subjective_score": "主観性評価総合スコア",
        "subjective": "主観性スコア (主要値)",

        # 派生スコア (TextProcessor計算)
        "phase_score": "位相バランススコア",
        "layer_balance_score": "層バランススコア", # LLM評価のlayer_balanceとは別
        "emotion_arc_score": "感情変容達成度スコア", # LLM評価のemotion_arc_qualityとは別
        "colloquial_score": "口語/ゴシック融合度スコア", # LLM評価のcolloquial_gothic_blendとは別
        # 短縮形
        "phase": "位相バランススコア",
        "layer": "層バランススコア",
        "emotion": "感情変容達成度スコア",
        "colloquial": "口語/ゴシック融合度スコア",

        # NGGSConfig パラメータキーの例
        "target_length": "目標文字数",
        "perspective_mode": "視点モード",
        "phase_focus": "位相焦点",
        "colloquial_level": "口語レベル",
        "emotion_arc": "感情の弧（目標）",
        "max_loops": "最大改善ループ数",
        "improvement_threshold": "改善停止閾値スコア",
        "llm_engine": "使用LLMエンジン",
        "llm_model": "使用LLMモデル",
        "skip_initial_generation": "初期生成スキップフラグ",

        # 分布キー (NarrativeFlowFramework分析結果など)
        "serif": "セリフ",
        "live_report": "実況",
        "monologue": "モノローグ",
        "narration": "ナレーション",
        "serif_prime": "セリフ（変容後）",
        "physical": "物質層",
        "sensory": "感覚層",
        "psychological": "心理層",
        "symbolic": "象徴層",
        "other": "その他（分類不能）"
    }

    if key in display_map:
        return display_map[key]
    else:
        # 未知のキーの場合は、キーを整形して返す
        formatted_key = key.replace('_', ' ').strip()
        # 各単語の先頭を大文字にする (Title Case風)
        return ' '.join(word.capitalize() for word in formatted_key.split()) if formatted_key else "不明なキー"


# --- File I/O Utilities (Using pathlib and Result type) ---
# FileProcessingError は Part 2 で定義済みと仮定
# Result は Part 2 で定義済みと仮定

def safe_read_file(
    file_path: Union[str, pathlib.Path],
    encoding: str = 'utf-8'
) -> Result[str, FileProcessingError]:
    """
    テキストファイルを安全に読み込み、Resultオブジェクトを返します。
    成功時は Result.ok(内容文字列)、失敗時は Result.fail(FileProcessingError)。
    """
    read_logger = logging.getLogger("NGGS-Lite.IO.Read")
    try:
        path_obj = pathlib.Path(file_path).resolve()
        if not path_obj.exists():
            raise FileNotFoundError(f"指定されたファイルが見つかりません: {path_obj}")
        if not path_obj.is_file():
            # IsADirectoryErrorはOSErrorのサブクラス
            raise IsADirectoryError(f"指定されたパスはファイルでありディレクトリではありません: {path_obj}")
        if not os.access(path_obj, os.R_OK): # 読み取り権限チェック
            raise PermissionError(f"ファイルへの読み取り権限がありません: {path_obj}")

        content_str = path_obj.read_text(encoding=encoding, errors='replace') # 不正文字は置換
        read_logger.debug(f"ファイル '{path_obj.name}' から {len(content_str)} 文字を正常に読み込みました。")
        return Result.ok(content_str)

    except (FileNotFoundError, IsADirectoryError, PermissionError) as e_access:
        # ファイルアクセス関連の特定しやすいエラー
        err = FileProcessingError(f"ファイルアクセスエラー: {e_access}", details={"path": str(file_path), "error_type": type(e_access).__name__})
        read_logger.error(str(err)) # エラーとしてログ記録
        return Result.fail(err)
    except IOError as e_io: # 広範なI/Oエラー
        err = FileProcessingError(f"ファイル読み込みIOエラー ({file_path}): {e_io}", details={"path": str(file_path)})
        read_logger.error(str(err), exc_info=False) # 一般的なIOエラーはトレースバック省略も検討
        return Result.fail(err)
    except Exception as e_unexpected: # その他の予期せぬエラー
        err = FileProcessingError(f"ファイル読み込み中の予期せぬエラー ({file_path}): {e_unexpected}", details={"path": str(file_path)})
        read_logger.error(str(err), exc_info=True) # 予期せぬエラーはトレースバック推奨
        return Result.fail(err)


def safe_write_file(
    file_path: Union[str, pathlib.Path],
    content: str,
    encoding: str = 'utf-8',
    create_backup: bool = False # バックアップ作成オプション
) -> Result[pathlib.Path, FileProcessingError]:
    """
    テキストコンテントを安全にファイルに書き込み、オプションでバックアップを作成します。
    成功時は Result.ok(書き込まれたファイルのPathオブジェクト)、失敗時は Result.fail(FileProcessingError)。
    """
    write_logger = logging.getLogger("NGGS-Lite.IO.Write")
    backup_file_path: Optional[pathlib.Path] = None # バックアップファイルのパスを追跡

    try:
        path_obj = pathlib.Path(file_path).resolve()
        parent_directory = path_obj.parent

        # 親ディレクトリの存在確認と作成 (必要であれば)
        if not parent_directory.exists():
            write_logger.info(f"親ディレクトリを作成します: {parent_directory}")
            parent_directory.mkdir(parents=True, exist_ok=True) # OSErrorやPermissionErrorを送出する可能性
        elif not parent_directory.is_dir():
            raise NotADirectoryError(f"親パスはディレクトリではありません: {parent_directory}")

        # ディレクトリへの書き込み権限を明示的に確認
        if not os.access(parent_directory, os.W_OK | os.X_OK): # 書き込みと実行(アクセス)権限
            raise PermissionError(f"ディレクトリへの書き込み/アクセス権限がありません: {parent_directory}")

        # バックアップ作成 (要求され、かつファイルが存在する場合)
        if create_backup and path_obj.exists() and path_obj.is_file():
            backup_result = backup_file(path_obj) # 専用のバックアップ関数を使用
            if backup_result.is_err:
                # バックアップ失敗は警告ログに留め、書き込み処理は続行
                write_logger.warning(
                    f"ファイル '{path_obj.name}' のバックアップ作成に失敗しましたが、書き込みは続行します。エラー: {backup_result.error}"
                )
            else:
                backup_file_path = backup_result.unwrap() # 成功したバックアップパスを保存

        # コンテンツ書き込み
        try:
            path_obj.write_text(content, encoding=encoding, errors='replace')
            write_logger.debug(f"ファイル '{path_obj.name}' に {len(content)} 文字を正常に書き込みました。")
            return Result.ok(path_obj) # 成功時は書き込まれたPathオブジェクトを返す
        except Exception as e_write_content: # write_text中の広範なエラーをキャッチ
            write_logger.error(f"ファイル '{path_obj.name}' へのコンテンツ書き込みに失敗: {e_write_content}", exc_info=False)
            # 書き込み失敗時にバックアップが存在すればリストアを試みる
            if backup_file_path and backup_file_path.exists():
                write_logger.info(f"書き込み失敗のため、ファイル '{path_obj.name}' のバックアップからのリストアを試みます...")
                try:
                    # shutil.moveはファイルシステムをまたぐ移動にも対応できるが、ここではrenameで十分
                    backup_file_path.rename(path_obj)
                    write_logger.warning(f"書き込み失敗後、ファイル '{path_obj.name}' をバックアップから正常にリストアしました。")
                except Exception as e_restore:
                    write_logger.critical(
                        f"致命的: ファイル '{path_obj.name}' の書き込み失敗、かつバックアップからのリストアも失敗: {e_restore}"
                    )
            # 元の書き込みエラーをFileProcessingErrorでラップして再送出
            raise FileProcessingError(f"ファイル書き込みエラー: {e_write_content}", details={"path": str(path_obj)}) from e_write_content

    except (NotADirectoryError, PermissionError) as e_dir_perm:
        # ディレクトリ構造や権限に関するエラー
        err = FileProcessingError(f"ディレクトリ/権限エラー: {e_dir_perm}", details={"path": str(file_path)})
        write_logger.error(str(err))
        return Result.fail(err)
    except OSError as e_os_prepare: # mkdirエラーやその他のOSレベルファイルシステムエラー
        err = FileProcessingError(f"ファイル書き込み準備中のOSエラー ({file_path}): {e_os_prepare}", details={"path": str(file_path)})
        write_logger.error(str(err), exc_info=True)
        return Result.fail(err)
    except Exception as e_unexpected_prepare: # セットアップやその他の操作中の予期せぬエラー
        err = FileProcessingError(f"ファイル書き込み中の予期せぬエラー ({file_path}): {e_unexpected_prepare}", details={"path": str(file_path)})
        write_logger.error(str(err), exc_info=True)
        return Result.fail(err)


def backup_file(filepath: pathlib.Path) -> Result[pathlib.Path, FileProcessingError]:
    """
    指定されたファイルのタイムスタンプ付きバックアップを専用関数で作成します。
    成功時は Result.ok(バックアップファイルのPathオブジェクト)、失敗時は Result.fail(FileProcessingError)。
    """
    backup_logger = logging.getLogger("NGGS-Lite.IO.Backup")
    if not filepath.is_file(): # is_file() は存在も暗黙的にチェック
        err = FileProcessingError(
            f"バックアップ元ファイルが見つからないか、ファイルではありません: {filepath}",
            details={"path": str(filepath)}
        )
        # バックアップ失敗は致命的ではない場合もあるため、警告レベルでログ
        backup_logger.warning(str(err))
        return Result.fail(err)

    try:
        # NGGSConfigからバックアップサフィックス形式を取得
        # この関数が呼び出される時点ではNGGSConfigは利用可能と仮定
        # (または、引数でconfigインスタンスを受け取る設計も考えられる)
        try:
            # グローバルスコープまたは適切な方法でNGGSConfigインスタンスを取得
            # ここでは仮に NGGSConfig() でデフォルトインスタンスを取得する
            # 実際には、main関数などで生成されたconfigインスタンスを渡すのが望ましい
            config_for_backup_format = NGGSConfig()
            backup_suffix_template = config_for_backup_format.BACKUP_SUFFIX_FORMAT
        except Exception: # NGGSConfig取得失敗時のフォールバック
            backup_logger.warning("NGGSConfigからバックアップ形式を取得できませんでした。デフォルト形式 '.bak.{timestamp}' を使用します。")
            backup_suffix_template = ".bak.{timestamp}"

        # タイムスタンプ生成 (ミリ秒精度)
        current_timestamp_utc = datetime.now(timezone.utc)
        timestamp_str_for_file = current_timestamp_utc.strftime("%Y%m%d_%H%M%S_%f")[:-3] # マイクロ秒をミリ秒に
        
        backup_final_suffix = backup_suffix_template.format(timestamp=timestamp_str_for_file)
        backup_file_path_obj = filepath.with_name(f"{filepath.name}{backup_final_suffix}")

        # バックアップファイル名の衝突回避 (ミリ秒精度ならほぼ不要だが念のため)
        collision_retry_count = 0
        max_collision_retries = 5 # 無限ループ防止
        while backup_file_path_obj.exists() and collision_retry_count < max_collision_retries:
            collision_retry_count += 1
            backup_logger.warning(
                f"バックアップファイル名が衝突しました。追加のサフィックスを付与します: {backup_file_path_obj.name}"
            )
            backup_file_path_obj = filepath.with_name(f"{filepath.name}{backup_final_suffix}.retry{collision_retry_count}")
        
        if backup_file_path_obj.exists(): # リトライ後も衝突する場合
            raise FileProcessingError(
                f"ファイル '{filepath.name}' の一意なバックアップファイル名の生成に失敗しました ({max_collision_retries}回リトライ後)。"
            )

        # shutil.copy2 を使用してメタデータも保持してコピー
        shutil.copy2(str(filepath), str(backup_file_path_obj))
        backup_logger.info(f"ファイルのバックアップを作成しました (コピー経由): {backup_file_path_obj.name}")
        return Result.ok(backup_file_path_obj)

    except FileProcessingError as e_fp_backup: # 内部で発生したFileProcessingErrorをそのまま伝播
        backup_logger.error(f"バックアップ処理中にFileProcessingError: {e_fp_backup}")
        return Result.fail(e_fp_backup)
    except Exception as e_backup_general: # その他の予期せぬエラー
        err = FileProcessingError(
            f"ファイル '{filepath.name}' のバックアップ作成中に予期せぬエラー: {e_backup_general}",
            details={"path": str(filepath), "original_error_type": type(e_backup_general).__name__}
        )
        backup_logger.error(str(err), exc_info=True)
        return Result.fail(err)

# =============================================================================
# Part 3 End: Utility Functions (Logging, Text, File I/O)
# =============================================================================
# =============================================================================
# Part 4: Utility Functions (Continued), LLM Client (Initialization)
# =============================================================================
# Part 1, 2, 3で定義された型やクラス (NGGSConfig, Result, TemplateError, VocabularyError,
# ConfigurationError, LLMResponse, SafeDict, FileProcessingError, JsonDict,
# safe_read_file, GEMINI_AVAILABLE, OPENAI_AVAILABLE, google_exceptions, genaiなど)
# および標準ライブラリが利用可能であることを前提とします。

import logging
import json
import pathlib
import os
import sys
import time
import random
import re
from typing import (
    Dict, List, Any, Optional, Union, Tuple, Callable, Type, Final, TypeAlias
)
from datetime import datetime, timezone # Part 1から

# Part 1からNGGSConfigをインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig, JsonDict, GEMINI_AVAILABLE, OPENAI_AVAILABLE, google_exceptions, genai, ConfigurationError
# Part 2からResult, TemplateError, VocabularyError, LLMResponseをインポート
# from nggs_lite_part2 import Result, TemplateError, VocabularyError, LLMResponse, LLMError, GenerationError
# Part 3からSafeDict, safe_read_file, FileProcessingErrorをインポート
# from nggs_lite_part3 import SafeDict, safe_read_file, FileProcessingError

# --- Template & Vocabulary Utilities (Refined for v1.8) ---

# デフォルトのプロンプトテンプレート定義 (埋め込み文字列として)
# これらは理想的には外部ファイルからロードされるべきですが、単一ファイル配布のためにここに含めます。
# ユーザーガイドの評価項目を反映するように内容を更新。

DEFAULT_GENERATION_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体テキスト生成 (NGGS-Lite v1.8)

## 1. 目標設定
- **文体**: 新ゴシック文体（廃墟、影、内省、象徴性、存在論的震えを重視）
- **視点**: {perspective_mode}
- **目標文字数**: 約{target_length}字
- **位相焦点**: {phase_focus}
- **口語レベル**: {colloquial_level}
- **感情の弧 (指定があれば)**: {emotion_arc}

## 2. 参考語彙リスト (適宜活用、特に指定されたテーマや層に関連するもの)
{vocabulary_list_str}

## 3. 入力テキスト (コンテキストとして使用、または初期テキストとして拡張)
```text
{input_text}
```

## 4. 物語構成・流れ (指定があれば、NarrativeFlowFrameworkからの指示)
{narrative_flow_section}

## 5. 生成指示
上記の設定と入力テキストを踏まえ、指定された視点とスタイルで、目標文字数程度の新しいゴシック文体テキストを生成してください。以下の点を特に意識し、質の高いゴシック文体シーンを創出してください。

- **ゴシック的雰囲気の醸成**: 不安、神秘、退廃、美と恐怖の混在、存在論的震えを感じさせる雰囲気を追求する。
- **主観的描写の深化**: 指定された視点からの内的体験（思考、感情、五感を通じた知覚、記憶の断片）を深く、鮮明に描写する。
- **間接的表現と暗示**: 感情や状況を直接的に説明するのではなく、環境描写、行動、サブテキスト、象徴的要素を通じて暗示的に表現し、読者の解釈に委ねる余地を残す。
- **位相移行の自然さと効果**: セリフ、実況、モノローグ、ナレーション等の語りの位相（モード）を、物語の効果を最大限に高めるように、自然かつ効果的に移行させる。
- **四層構造の調和**: 物質層、感覚層、心理層、象徴層の描写をバランス良く、かつ有機的に連携させ、物語に深みと多層性を与える。
- **語彙選択の妙**: ゴシック的な響きを持ち、かつ文脈に適合した語彙を効果的に使用し、文体の質を高める。
- **読者体験の重視**: 常に読者がどのように感じ、どのように物語世界に没入できるかを最優先に考慮する。

出力は生成されたテキスト本文のみとしてください。説明や前置き、メタコメントは一切不要です。
"""

DEFAULT_EVALUATION_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体 テキスト評価 (NGGS-Lite v1.8)

## 評価対象テキスト
```text
{generated_text}
```

## 評価基準 (各項目1.0～5.0点で評価、0.1点刻み)
以下の各評価項目について、具体的なテキスト箇所を引用しつつ、その理由と共にスコアをJSON形式で出力してください。

### A. LLM基本評価項目
- **gothic_atmosphere (ゴシック的雰囲気)**: 廃墟、影、神秘、不安、退廃、美と恐怖の混在、存在論的震えなどの雰囲気がどの程度醸成されているか？ (1:希薄 - 5:濃厚)
- **stylistic_gravity (文体の荘重さ/質)**: ゴシック特有の重厚さ、格調高さ、あるいは独特の美意識が文体に現れているか？ (1:軽薄/稚拙 - 5:荘重/洗練)
- **indirect_emotion (感情表現の間接性)**: 感情が直接的な説明でなく、描写や行動、サブテキストを通じて効果的に暗示されているか？ (1:直接的すぎる - 5:極めて間接的/暗示的)
- **vocabulary_richness (語彙の質と豊かさ)**: ゴシック的な語彙が適切かつ効果的に使用され、文体に深みを与えているか？ (1:貧弱/不適切 - 5:豊か/効果的)

### B. ETI (存在論的震え指数) 関連評価
- **eti_overall (ETI総合評価)**: テキスト全体が読者に存在論的な不安や感覚の揺らぎをどの程度与えるか。境界性、両義性、超越侵犯、不確定性、内的変容の要素を総合的に評価。 (1:皆無 - 5:強烈)
- **eti_boundary (境界性の表現)**: 現実と非現実、生と死、内と外などの境界が曖昧になるような表現の巧みさ。 (1:不明瞭 - 5:巧み)
- **eti_ambivalence (両義性の表現)**: 相反する感情や概念（例：魅惑と恐怖）が共存する両義的な描写の度合い。 (1:単純 - 5:複雑/多層的)
- **eti_transgression (超越・侵犯の表現)**: 日常の規範や限界を超えるような（狂気、異形、禁忌など）表現のインパクト。 (1:平凡 - 5:衝撃的)
- **eti_uncertainty (不確定性の表現)**: 謎めいた雰囲気や解釈の多義性を生み出す表現の度合い。 (1:明確すぎる - 5:暗示的/多義的)
- **eti_transformation (内的変容の表現)**: 登場人物の認識や自己同一性が揺らぎ変容する過程の描写の深さ。 (1:表面的 - 5:深遠)

### C. RI (可読性指数) 関連評価
- **ri_clarity (構造的明瞭性)**: 文の構造が明確で、論理的な繋がりが理解しやすいか。 (1:難解 - 5:明瞭)
- **ri_visual_rhythm (視覚的リズム)**: 段落構成や文の長短のバランスが良く、視覚的に読みやすいか。 (1:単調/読みにくい - 5:リズミカル/読みやすい)
- **ri_emotional_flow (感情フローの自然さ)**: 感情の推移や雰囲気の変化が自然で、読者が感情移入しやすいか。 (1:不自然/唐突 - 5:極めて自然)
- **ri_cognitive_load (認知処理負荷)**: 専門用語や難解な表現が適切で、読者の理解を過度に妨げていないか。 (1:高負荷/難解 - 5:低負荷/平易)
- **ri_interpretive_resonance (解釈的余韻)**: 読後に解釈の幅や深い余韻を残すような工夫があるか。 (1:表面的 - 5:深い余韻)

### D. その他主要評価項目
- **subjective_depth (主観的語りの深さ)**: 登場人物の内的世界（思考、感情、感覚）への没入感、主観的体験の描写の深さ。 (1:浅い/客観的 - 5:深い/没入的)
- **phase_transition_quality (位相移行の質)**: セリフ、実況、モノローグ、ナレーション等の語りの位相間の移行が自然で効果的か？ (1:不自然/唐突 - 5:極めて自然/効果的)
- **colloquial_gothic_blend_quality (口語/ゴシック融合の質)**: 日常的な口語表現とゴシック的な文語表現が、意図したレベルで自然に融合しているか？ (1:不自然/乖離 - 5:極めて自然/融合)
- **layer_balance_quality (層バランスの質)**: 物質層、感覚層、心理層、象徴層の描写が、作品の意図に対して適切なバランスで効果的に含まれているか？ (1:偏り大/効果薄 - 5:理想的バランス/効果的)
- **emotion_arc_achievement (感情変容の達成度)**: テキスト全体を通じて描かれる感情の変化や流れ（指定があればその弧）が、説得力を持って自然に達成されているか？ (1:平板/未達成 - 5:説得力があり達成)

### E. 総合評価
- **overall_quality (全体的な質と完成度)**: 上記全要素を総合し、新ゴシック文体テキストとしての完成度、独自性、文学的価値、読者への訴求力を評価。 (1:低い - 5:極めて高い)

## 出力形式
必ず以下のJSON形式で、上記「A」～「E」の各評価項目（例: `gothic_atmosphere`, `eti_overall`など）のスコア(float)と、それぞれの評価理由(string)を記述してください。理由には具体的なテキスト箇所を引用すると尚良いです。

```json
{{
  "scores": {{
    "gothic_atmosphere": <float>, "stylistic_gravity": <float>, "indirect_emotion": <float>, "vocabulary_richness": <float>,
    "eti_overall": <float>, "eti_boundary": <float>, "eti_ambivalence": <float>, "eti_transgression": <float>, "eti_uncertainty": <float>, "eti_transformation": <float>,
    "ri_clarity": <float>, "ri_visual_rhythm": <float>, "ri_emotional_flow": <float>, "ri_cognitive_load": <float>, "ri_interpretive_resonance": <float>,
    "subjective_depth": <float>, "phase_transition_quality": <float>, "colloquial_gothic_blend_quality": <float>, "layer_balance_quality": <float>, "emotion_arc_achievement": <float>,
    "overall_quality": <float>
  }},
  "reasoning": {{
    "gothic_atmosphere": "<理由>", "stylistic_gravity": "<理由>", "indirect_emotion": "<理由>", "vocabulary_richness": "<理由>",
    "eti_overall": "<理由>", "eti_boundary": "<理由>", "eti_ambivalence": "<理由>", "eti_transgression": "<理由>", "eti_uncertainty": "<理由>", "eti_transformation": "<理由>",
    "ri_clarity": "<理由>", "ri_visual_rhythm": "<理由>", "ri_emotional_flow": "<理由>", "ri_cognitive_load": "<理由>", "ri_interpretive_resonance": "<理由>",
    "subjective_depth": "<理由>", "phase_transition_quality": "<理由>", "colloquial_gothic_blend_quality": "<理由>", "layer_balance_quality": "<理由>", "emotion_arc_achievement": "<理由>",
    "overall_quality": "<全体的な評価と改善点>"
  }}
}}
```
"""

DEFAULT_IMPROVEMENT_TEMPLATE: Final[str] = """
# 命令: 新ゴシック文体テキスト改善 (NGGS-Lite v1.8)

## 1. 改善対象テキスト (抜粋または全文)
```text
{original_text}
```

## 2. 前回の評価結果と改善指示の要約 (JSON形式)
```json
{evaluation_results_json} 
```
上記評価の要点:
- 特に評価が低かった項目: {low_score_items_str}
- 特に評価が高かった項目: {high_score_items_str}
- 層バランス分析からの示唆: {layer_distribution_analysis}
- 位相分布分析からの示唆: {phase_distribution_analysis}

## 3. 今回の改善における目標設定
- **文体**: 新ゴシック文体（存在論的震えの追求）
- **視点**: {perspective_mode}
- **位相焦点**: {phase_focus}
- **口語レベル**: {colloquial_level}
- **感情の弧 (維持または再設定)**: {emotion_arc}

## 4. 参考語彙リスト (特に低評価項目や目標とする雰囲気に合致するものを活用)
{vocabulary_list_str}

## 5. 今回の具体的改善指示 (システムまたは戦略に基づいて生成された指示)
{improvement_section}

## 6. 全体的な改善指示
上記の評価結果（特に低評価項目）、目標設定、および具体的改善指示を踏まえ、改善対象テキスト全体を全面的に書き直すか、あるいは効果的な部分修正・加筆を行ってください。より質の高い、読者の心に深く響く新ゴシック文体テキストへと改善することを目的とします。以下の点を特に意識してください。

- **弱点の克服**: 評価で指摘された弱点や、具体的改善指示で示された箇所を重点的に修正・改善する。
- **強みの維持・発展**: 評価で高かった点や、ゴシック文体としての魅力的な要素は維持しつつ、さらに洗練させる。
- **全体的な質の向上**: ゴシック的雰囲気、文体の質と荘重さ、感情表現の間接性と深み、語彙の選択と豊かさ、主観描写の没入感、位相移行の自然さと効果、四層構造のバランスと連携、口語と文語の融合度、感情変容の説得力など、全ての評価軸において総合的な向上を目指す。
- **設定と指示の遵守**: 視点、位相焦点、口語レベル、感情の弧などの設定、および具体的改善指示を正確に反映させる。
- **創造性と独自性**: 指示に従いつつも、AIとしての創造性を発揮し、ありきたりでない独自の表現を追求する。

出力は改善されたテキスト本文のみとしてください。説明やコメント、メタタグは一切不要です。
"""

# グローバル辞書: デフォルトテンプレート群
DEFAULT_TEMPLATES: Final[Dict[str, str]] = {
    "generation": DEFAULT_GENERATION_TEMPLATE,
    "evaluation": DEFAULT_EVALUATION_TEMPLATE,
    "improvement": DEFAULT_IMPROVEMENT_TEMPLATE,
}

# グローバル辞書: デフォルト語彙 (フォールバック用)
DEFAULT_LAYERED_VOCABULARY: Final[Dict[str, List[str]]] = {
    "physical": ["城壁", "石畳", "尖塔", "地下牢", "書物", "肖像画", "古時計", "燭台", "屋敷", "鏡"],
    "sensory": ["冷気", "黴臭さ", "月光", "影", "囁き", "鐘の音", "真紅の", "漆黒の", "霧雨", "無音"],
    "psychological": ["憂愁", "狂気", "不安", "孤独", "記憶", "忘却", "既視感", "罪悪感", "苦悶", "焦燥"],
    "symbolic": ["鏡（象徴）", "時計（象徴）", "螺旋階段", "迷宮", "仮面", "血（象徴）", "薔薇（象徴）", "鴉", "運命", "深淵"]
}


def load_template(
    file_path: Union[str, pathlib.Path], # ファイルからの読み込みを試みるパス
    template_name: str,                   # "generation", "evaluation", "improvement" のいずれか
    config: NGGSConfig                    # NGGSConfigインスタンス
) -> Result[str, TemplateError]:
    """
    指定された名前のプロンプトテンプレートをロードします。
    まずconfigで指定されたテンプレートディレクトリ内のファイルを試み、
    失敗した場合はグローバルなDEFAULT_TEMPLATESからフォールバックします。
    """
    template_loader_logger = logging.getLogger("NGGS-Lite.TemplateLoader")
    
    # 1. ファイルからの読み込み試行
    # config.DEFAULT_TEMPLATES_DIR は ./templates_v1.8 のような相対パスを想定
    # pathlib.Path(config.DEFAULT_TEMPLATES_DIR) で絶対パスに変換可能
    # BASE_DIR はスクリプトの場所を指すため、それと組み合わせる
    # NGGSConfig.get_default_paths() を使うのがより適切
    default_paths = config.get_default_paths() # BASE_DIR基準のパスを取得
    template_dir_from_config = default_paths.get("DEFAULT_TEMPLATES_DIR", BASE_DIR / config.DEFAULT_TEMPLATES_DIR)
    
    # file_path引数は、コマンドライン等で明示的に指定されたパスを優先する想定
    # ここでは、template_nameに基づいてファイルパスを構築する
    # file_path引数は実際には使用せず、template_nameとconfigからパスを決定する
    # 関数のシグネチャを見直し、configからテンプレートディレクトリを取得するようにする
    
    # 正しいファイルパスの構築
    # 引数 file_path は実際には template_name から構築されるべきファイル名の一部。
    # この関数の呼び出し側で、config.DEFAULT_TEMPLATES_DIR / f"{template_name}.txt" のような形で
    # file_path を渡すか、この関数内部でそのように構築する。
    # プランでは「load_templateが、NGGSConfig.DEFAULT_TEMPLATES_DIR からのファイル読み込みを最優先」とあるため、
    # file_path引数は実際には使用せず、template_nameとconfigからパスを決定する。
    
    # 修正: file_path引数は無視し、configとtemplate_nameからパスを構築
    actual_file_path_to_try = template_dir_from_config / f"{template_name}.txt"
    
    template_loader_logger.debug(f"テンプレート '{template_name}' をファイル '{actual_file_path_to_try}' からロード試行...")
    
    read_file_result = safe_read_file(actual_file_path_to_try)
    last_load_error: Optional[Exception] = None

    if read_file_result.is_ok:
        template_content = read_file_result.unwrap()
        if template_content and template_content.strip():
            validation_result = validate_template(template_content) # 期待キーはここでは指定しない
            if validation_result.is_ok:
                template_loader_logger.info(f"テンプレート '{template_name}' をファイル '{actual_file_path_to_try.name}' から正常にロードしました。")
                return Result.ok(template_content)
            else: # ファイル内容はあったが検証失敗
                last_load_error = validation_result.error
                template_loader_logger.error(f"ファイル '{actual_file_path_to_try.name}' のテンプレート検証に失敗: {last_load_error}")
        else: # ファイルは存在したが空だった
            last_load_error = TemplateError(f"テンプレートファイル '{actual_file_path_to_try.name}' が空です。")
            template_loader_logger.warning(str(last_load_error))
    elif isinstance(read_file_result.error, FileNotFoundError):
        last_load_error = read_file_result.error
        template_loader_logger.info(f"テンプレートファイル '{actual_file_path_to_try.name}' が見つかりませんでした。デフォルトテンプレートを確認します。")
    else: # その他のファイル読み込みエラー
        last_load_error = read_file_result.error
        template_loader_logger.error(f"テンプレートファイル '{actual_file_path_to_try.name}' の読み込みに失敗: {last_load_error}")

    # 2. デフォルトテンプレートへのフォールバック
    default_template_content = DEFAULT_TEMPLATES.get(template_name)
    if default_template_content:
        if last_load_error: # ファイルロード試行でエラーがあった場合
            template_loader_logger.warning(
                f"テンプレート '{template_name}' のファイルロードに失敗したため、埋め込みデフォルトテンプレートを使用します。"
                f"(ファイルエラー: {last_load_error})"
            )
        # デフォルトテンプレートも検証
        default_validation_result = validate_template(default_template_content)
        if default_validation_result.is_ok:
            template_loader_logger.info(f"テンプレート '{template_name}' に埋め込みデフォルトを使用しました。")
            return Result.ok(default_template_content)
        else: # 埋め込みデフォルトテンプレート自体が不正 (クリティカル)
            critical_error_msg = f"埋め込みデフォルトテンプレート '{template_name}' が不正です: {default_validation_result.error}"
            template_loader_logger.critical(critical_error_msg)
            return Result.fail(TemplateError(critical_error_msg, details=getattr(default_validation_result.error, 'details', None)))
    else: # ファイルにもなく、デフォルトにもない
        final_error_message = f"テンプレート '{template_name}' はファイル '{actual_file_path_to_try.name}' にも見つからず、埋め込みデフォルトも存在しません。"
        if last_load_error:
            final_error_message += f" (ファイルアクセス時のエラー: {last_load_error})"
        
        final_template_error = TemplateError(final_error_message, details=getattr(last_load_error, 'details', None))
        template_loader_logger.error(str(final_template_error))
        return Result.fail(final_template_error)


def validate_template(template_content: Optional[str], expected_placeholders: Optional[List[str]] = None) -> Result[bool, TemplateError]:
    """プロンプトテンプレート文字列の基本的な検証を行います。"""
    if not template_content or not isinstance(template_content, str) or not template_content.strip():
        return Result.fail(TemplateError("テンプレートは空でない文字列である必要があります。"))

    # 1. 中括弧のバランスチェック
    balance_check = 0
    for char_code in template_content:
        if char_code == '{':
            balance_check += 1
        elif char_code == '}':
            balance_check -= 1
        if balance_check < 0: # '}' が先行するケース
            return Result.fail(TemplateError("テンプレートのフォーマットエラー: '}' が先行しています。", details={"template_preview": truncate_text(template_content, 50)}))
    if balance_check != 0: # '{' が多いケース
        return Result.fail(TemplateError("テンプレートのフォーマットエラー: '{' が閉じていません。", details={"template_preview": truncate_text(template_content, 50)}))

    # 2. 期待されるプレースホルダーの存在チェック (指定されていれば)
    if expected_placeholders:
        try:
            # 単純な英数字とアンダースコアのプレースホルダーを検出
            # (例: {placeholder_name})
            found_placeholders = set(re.findall(r"\{([a-zA-Z0-9_]+)\}", template_content))
            missing_placeholders = [ph for ph in expected_placeholders if ph not in found_placeholders]
            if missing_placeholders:
                return Result.fail(TemplateError(
                    f"テンプレートに必要なプレースホルダーが欠落しています: {', '.join(missing_placeholders)}",
                    details={"missing": missing_placeholders, "found": list(found_placeholders)}
                ))
        except re.error as e_re_validate:
            # 正規表現エラーは警告に留め、検証は続行しない（キーチェックはオプション扱い）
            logging.getLogger("NGGS-Lite.Utils.Template").warning(
                f"テンプレートのプレースホルダーキーチェック中に正規表現エラー: {e_re_validate}"
            )

    # 3. ダミーデータでのフォーマット試行 (より厳密な構文チェック)
    try:
        # テンプレート内の全プレースホルダーを抽出
        all_placeholders_in_template = set(re.findall(r"\{([a-zA-Z0-9_]+)\}", template_content))
        dummy_data_for_format_check = {ph_key: "dummy_value" for ph_key in all_placeholders_in_template}
        # SafeDictを使用して、テンプレート内にあってもダミーデータにないキーを許容
        template_content.format_map(SafeDict(dummy_data_for_format_check))
        return Result.ok(True) # フォーマット成功
    except (ValueError, KeyError) as e_format_syntax: # format()の構文エラー
        return Result.fail(TemplateError(f"テンプレートのフォーマット構文エラー (ダミーフォーマット試行時): {e_format_syntax}", details={"template_preview": truncate_text(template_content, 70)}))
    except Exception as e_format_unexpected: # その他の予期せぬエラー
        return Result.fail(TemplateError(f"テンプレート検証中の予期せぬエラー (ダミーフォーマット試行時): {e_format_unexpected}", details={"template_preview": truncate_text(template_content, 70)}))


def load_vocabulary(
    config: NGGSConfig
) -> Result[Union[List[str], JsonDict], VocabularyError]:
    """
    NGGSConfigで定義された優先順位に基づいて語彙をロードします。
    GLCAIパス -> デフォルトNGGSパス -> 内部デフォルト の順で試行します。
    返される構造（リストまたは辞書）の一貫性を保証します。
    """
    vocab_loader_logger = logging.getLogger("NGGS-Lite.VocabLoader")
    last_encountered_error: Optional[Exception] = None # 最後に発生したロードエラーを保持

    # config.VOCAB_LOAD_PRIORITY (例: ['glcai', 'default']) に従ってロード試行
    for source_type_key in config.VOCAB_LOAD_PRIORITY:
        target_file_path_str: Optional[str] = None
        current_source_description: str = ""

        if source_type_key == 'glcai':
            target_file_path_str = config.GLCAI_VOCAB_PATH
            current_source_description = "GLCAI Vocabulary File"
        elif source_type_key == 'default':
            target_file_path_str = config.DEFAULT_VOCAB_PATH
            current_source_description = "Default NGGS Vocabulary File"
        else:
            vocab_loader_logger.warning(f"不明な語彙ソースタイプ '{source_type_key}' が優先順位リストに含まれています。スキップします。")
            continue

        resolved_file_path: Optional[pathlib.Path] = None
        if target_file_path_str:
            try:
                # BASE_DIR基準でパスを解決 (NGGSConfig.get_default_paths() のような挙動)
                # ただし、target_file_path_strが既に絶対パスの可能性も考慮
                path_candidate = pathlib.Path(target_file_path_str)
                if not path_candidate.is_absolute():
                    resolved_file_path = (BASE_DIR / path_candidate).resolve()
                else:
                    resolved_file_path = path_candidate.resolve()
            except Exception as e_path_resolve:
                vocab_loader_logger.warning(
                    f"{current_source_description} のパス文字列 '{target_file_path_str}' の解決中にエラー: {e_path_resolve}"
                )
                # resolved_file_path は None のまま

        if resolved_file_path:
            vocab_loader_logger.info(f"{current_source_description} から語彙のロードを試行: {resolved_file_path}")
            file_read_result = safe_read_file(resolved_file_path) # Part 3のユーティリティ
            
            if file_read_result.is_ok:
                content_str = file_read_result.unwrap()
                if content_str and content_str.strip():
                    try:
                        loaded_json_data = json.loads(content_str)
                        # 基本的な型検証: リストまたは辞書であること
                        if isinstance(loaded_json_data, (list, dict)):
                            vocab_loader_logger.info(
                                f"{current_source_description} ({resolved_file_path.name}) から語彙を正常にロード・解析しました。"
                            )
                            return Result.ok(loaded_json_data) # 成功、ここで終了
                        else:
                            last_encountered_error = VocabularyError(
                                f"{current_source_description} ({resolved_file_path.name}) のデータがリストまたは辞書形式ではありません。"
                            )
                            vocab_loader_logger.warning(str(last_encountered_error))
                    except json.JSONDecodeError as e_json_decode:
                        last_encountered_error = VocabularyError(
                            f"{current_source_description} ({resolved_file_path.name}) のJSON解析エラー: {e_json_decode}"
                        )
                        vocab_loader_logger.warning(str(last_encountered_error))
                    except Exception as e_load_processing: # json.loads以外のエラー
                        last_encountered_error = VocabularyError(
                            f"{current_source_description} ({resolved_file_path.name}) のロード処理中に予期せぬエラー: {e_load_processing}"
                        )
                        vocab_loader_logger.error(str(last_encountered_error), exc_info=True)
                else: # ファイルは空だった
                    last_encountered_error = VocabularyError(f"{current_source_description} ({resolved_file_path.name}) が空です。")
                    vocab_loader_logger.warning(str(last_encountered_error))
            else: # safe_read_fileがエラーを返した場合 (FileNotFoundErrorなど)
                last_encountered_error = file_read_result.error
                # FileNotFoundErrorはsafe_read_file内でログされるので、ここでは重複ログを避ける
                if not isinstance(last_encountered_error, FileNotFoundError):
                    vocab_loader_logger.warning(
                        f"{current_source_description} ({resolved_file_path.name}) の読み込みに失敗: {last_encountered_error}"
                    )
        else: # target_file_path_strがNoneまたは解決できなかった場合
            vocab_loader_logger.info(f"ソースタイプ '{source_type_key}' のパスが設定されていないか無効です。")
            if last_encountered_error is None: # まだエラーが記録されていなければ
                last_encountered_error = VocabularyError(f"ソースタイプ '{source_type_key}' のパスが設定されていません。")

    # ループが完了してもロードに成功しなかった場合
    vocab_loader_logger.warning("設定されたファイルパスからの語彙ロードに全て失敗しました。")
    
    # 内部デフォルト語彙へのフォールバック
    internal_default_vocab = DEFAULT_LAYERED_VOCABULARY # Part 4で定義されたグローバル定数
    if internal_default_vocab and isinstance(internal_default_vocab, dict):
        log_msg_fallback = "埋め込みデフォルト階層化語彙を使用します。"
        if last_encountered_error:
            log_msg_fallback += f" (直前のエラー: {last_encountered_error})"
        vocab_loader_logger.warning(log_msg_fallback)
        return Result.ok(internal_default_vocab)
    else: # 内部デフォルトも不正な場合 (通常ありえない)
        final_error_obj = last_encountered_error if isinstance(last_encountered_error, Exception) else VocabularyError(str(last_encountered_error))
        if not final_error_obj: # last_errorもNoneの場合
            final_error_obj = VocabularyError("有効な語彙ファイルが見つからず、かつ有効な内部デフォルト語彙もありません。")
        vocab_loader_logger.error(str(final_error_obj))
        return Result.fail(final_error_obj) # type: ignore


# --- Instruction Extraction Utility ---
def extract_improvement_instructions(instruction_text: str) -> Optional[str]:
    """
    LLMの応答から構造化された改善指示を抽出します。
    (v1.7からのロジックをベースに、v1.8でマイナーリファイン)
    """
    instruction_extractor_logger = logging.getLogger("NGGS-Lite.InstructionExtractor")
    if not instruction_text or not isinstance(instruction_text, str):
        return None # 入力が無効ならNone

    # 抽出パターンのリスト (特異性の高いものから順に試行: ヘッダー > キーワード句 > リスト)
    # 各パターンは、指示内容を含むキャプチャグループ(通常は最初のグループ)を持つことを期待
    regex_patterns_for_extraction: List[str] = [
        # 1. 特定のセクションヘッダー (例: ## 改善指示)
        r'(?:^|\n)\s*##\s*(?:改善(?:プロンプト|指示)?|具体的な改善指示|改善点|提案事項)\s*?\n([\s\S]+?)(?=\n\s*##\s*\w+|$)',
        # 2. 指示を示唆するキーワード句 (例: 以下の点を改善してください)
        r'(?:以下|次|下記)の(?:点|観点|項目|ポイント|指示)を(?:参考にして|踏まえて|考慮し|重点的に)?(?:改善|修正|強化|調整|変更|見直し|加筆|削除|検討).*?\n([\s\S]+?)(?=\n\n|\n\s*##|$)',
        # 3. 番号付きリスト (より堅牢なパターン)
        r'(?:^|\n)\s*((?:\d+[.\uFF0E\uff61]\s+[\s\S]+?)+)(?=\n\s*\d+[.\uFF0E\uff61]|\n\n|$)',
        # 4. 箇条書きリスト (より堅牢なパターン)
        r'(?:^|\n)\s*((?:[\*\u30FB・\-]\s+[\s\S]+?)+)(?=\n\s*[\*\u30FB・\-]\s+|\n\n|$)'
    ]
    
    extracted_instruction_blocks: List[str] = []
    search_target_text = instruction_text # 検索対象テキスト

    for pattern_index, regex_pattern_str in enumerate(regex_patterns_for_extraction):
        try:
            # re.finditer で重複しない全てのマッチを取得
            matches_found = list(re.finditer(regex_pattern_str, search_target_text, re.MULTILINE | re.IGNORECASE))
            if matches_found:
                instruction_extractor_logger.debug(
                    f"改善指示抽出パターン {pattern_index + 1} が {len(matches_found)} 回マッチしました。"
                )
                for match_obj in matches_found:
                    # 通常、最初のキャプチャグループ(group(1))が指示内容
                    if match_obj.lastindex is not None and match_obj.lastindex >= 1:
                        instruction_block_content = match_obj.group(1).strip()
                        # 空のマッチや単なるマーカーを避けるための基本的なチェック
                        # (例: "*" や "-" のみ、または非常に短い文字列)
                        cleaned_content_for_check = re.sub(r'[\*\u30FB・\-]', '', instruction_block_content).strip()
                        if cleaned_content_for_check and len(cleaned_content_for_check) > 10: # 最低10文字以上
                            extracted_instruction_blocks.append(instruction_block_content)
        except re.error as e_re_extract:
            instruction_extractor_logger.warning(
                f"改善指示抽出中の正規表現エラー (パターン {pattern_index + 1}): {e_re_extract}"
            )

    if extracted_instruction_blocks:
        # 後処理: 重複排除、クリーニング、最適なブロックの選択(ここでは結合)
        unique_cleaned_instructions: List[str] = []
        seen_normalized_instructions: Set[str] = set()
        for instruction_str in extracted_instruction_blocks:
            # 重複チェックのために空白を正規化し、小文字に変換
            normalized_instr_for_dedup = re.sub(r'\s+', ' ', instruction_str).strip().lower()
            if normalized_instr_for_dedup not in seen_normalized_instructions and len(normalized_instr_for_dedup) > 15: # 最低15文字
                unique_cleaned_instructions.append(instruction_str.strip()) # 元の形式で追加
                seen_normalized_instructions.add(normalized_instr_for_dedup)

        if len(unique_cleaned_instructions) > 1:
            instruction_extractor_logger.info(
                f"{len(unique_cleaned_instructions)}個のユニークな改善指示ブロックを抽出しました。これらを結合します。"
            )
            # 複数のブロックが見つかった場合は、区切り線を入れて結合
            final_extracted_instructions = "\n\n---\n[追加の改善指示]\n---\n\n".join(unique_cleaned_instructions)
        elif len(unique_cleaned_instructions) == 1:
            final_extracted_instructions = unique_cleaned_instructions[0]
            instruction_extractor_logger.info("1個のユニークな改善指示ブロックを抽出しました。")
        else: # 全てのブロックが短すぎたか重複していた場合
            final_extracted_instructions = None
            instruction_extractor_logger.warning("抽出された改善指示ブロックは重複しているか、内容が短すぎました。")
        
        return final_extracted_instructions if final_extracted_instructions else None # Noneや空文字列の場合はNoneを返す
    else:
        # パターンに一致しなかった場合のフォールバック: テキスト全体が指示のように見えるか？
        stripped_full_text = instruction_text.strip()
        # リスト形式（数字や箇条書きで始まる）か、ある程度の長さがあるか
        if len(stripped_full_text) > 70 and \
           (re.match(r'^\s*(\d+\.|[\*\u30FB・\-])', stripped_full_text) or len(stripped_full_text) > 200):
            instruction_extractor_logger.warning(
                "特定の改善指示パターンに一致しませんでした。応答全体をフォールバック改善指示として使用します。"
            )
            return stripped_full_text
        else:
            instruction_extractor_logger.warning("意味のある改善指示を抽出できませんでした。")
            return None


# --- Utility function for exponential backoff ---
def exponential_backoff(attempt: int, base_delay: float, max_delay: float, jitter: bool = True) -> float:
    """
    指数バックオフ遅延を計算します（オプションでジッター付き）。
    Args:
        attempt: 現在のリトライ試行回数 (0ベースを推奨、つまり初回は attempt=0)。
        base_delay: 基本遅延秒数。
        max_delay: 最大遅延秒数。
        jitter: Trueの場合、計算された遅延にランダムなジッターを加える。
    Returns:
        計算された遅延秒数。
    """
    if attempt < 0: attempt = 0 # 試行回数は非負

    # capped_exponent の計算を修正: attempt が 0 の場合、2**0 = 1 となるように
    # また、大きな attempt 数によるオーバーフローを防ぐため、指数をキャップ (例: 7-8程度)
    # (2**7 = 128, 2**8 = 256)
    capped_exponent_val = min(attempt, 8) # 0から始まる試行回数を想定
    
    # 基本遅延 * 2^attempt (ただし上限あり)
    calculated_delay = base_delay * (2 ** capped_exponent_val)
    
    # 最大遅延でクリップ
    delay_before_jitter = min(calculated_delay, max_delay)

    final_delay_seconds: float
    if jitter:
        # フルジッター: [0, delay_before_jitter] の範囲でランダムな値を加える
        # ただし、実用上は delay_before_jitter の一部（例: +/- 50%）にすることが多い
        # ここでは、[0.5 * delay_before_jitter, 1.5 * delay_before_jitter] の範囲とするか、
        # あるいは、[delay_before_jitter * (1-jitter_factor), delay_before_jitter * (1+jitter_factor)]
        # よりシンプルなのは、[0, delay_before_jitter] からランダムに選ぶ「フルジッター」
        # ここでは、元のコードの意図に近い「delay_before_jitterの近傍」とする
        # 例: +/- 25% のジッター
        jitter_range = delay_before_jitter * 0.25
        final_delay_seconds = random.uniform(delay_before_jitter - jitter_range, delay_before_jitter + jitter_range)
    else:
        final_delay_seconds = delay_before_jitter
    
    # 最終的な遅延が0未満にならないようにし、max_delayも超えないように再クリップ
    final_delay_seconds = max(0.0, min(final_delay_seconds, max_delay))
    
    return round(final_delay_seconds, 3) # ミリ秒精度で丸める


# =============================================================================
# Part 4: LLM Client (Initialization)
# =============================================================================
# ConfigurationErrorはPart 2で定義済みと仮定
# LLMResponseはPart 2で定義済みと仮定
# GEMINI_AVAILABLE, OPENAI_AVAILABLE, genai, google_exceptions はPart 1で定義/インポート済みと仮定

class LLMClient:
    """
    LLM APIとの通信を処理するクラス (v1.8 Refactored)。
    主にGoogle Gemini APIを使用し、堅牢なエラー処理とリトライロジックを備えます。
    """
    def __init__(self, config: NGGSConfig, use_mock: bool = False):
        """
        LLMクライアントを初期化します。

        Args:
            config: NGGSConfigオブジェクト。APIキー、モデル名、安全設定などを含む。
            use_mock: Trueの場合、APIキーやライブラリの有無に関わらずモックモードを強制。
        """
        self.config: NGGSConfig = config
        self.use_mock: bool = use_mock
        self.api_key_source: str = "N/A" # APIキーの取得元を示す
        self.call_count: int = 0         # API呼び出し回数のカウンタ
        self.last_call_time: float = 0.0 # RPM制限のための最終呼び出し時刻
        
        # モック生成用の内部関数 (use_mockがTrueの場合に設定される)
        self._mock_generate_internal: Optional[Callable[[str, float], LLMResponse]] = None
        
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.LLMClient")

        self.client: Optional[Any] = None # Geminiクライアントインスタンスなど
        self.current_engine: str = "mock" if use_mock else config.LLM_ENGINE

        if self.use_mock:
            self.logger.info("LLMClientは強制的にモックモードを使用します。")
            self._setup_mock() # モック用の設定を行う
            self.api_key_source = "Mock Forced"
            return # モックモードの場合はここで初期化終了

        # 選択されたエンジンに基づいて初期化
        if self.current_engine == "gemini":
            if not GEMINI_AVAILABLE:
                self.logger.critical("Geminiエンジンが選択されましたが、google-generativeaiライブラリが利用できません。")
                raise ConfigurationError("google-generativeaiライブラリがインストールされていません。")
            self._initialize_gemini()
        elif self.current_engine == "openai": # v1.8ではプレースホルダー
            if not OPENAI_AVAILABLE:
                self.logger.critical("OpenAIエンジンが選択されましたが、openaiライブラリが利用できません。")
                raise ConfigurationError("openaiライブラリがインストールされていません。")
            self._initialize_openai() # OpenAI初期化 (プレースホルダー)
        else:
            # サポートされていないエンジンが設定されている場合はエラー
            unsupported_engine_msg = (
                f"サポートされていないLLMエンジンが設定されています: {self.current_engine}. "
                f"サポートされているのは 'gemini' (および 'openai' はプレースホルダー) のみです。"
            )
            self.logger.error(unsupported_engine_msg)
            raise ConfigurationError(unsupported_engine_msg)

        # 初期化試行後、クライアントがNoneのまま (かつモックモードでない) 場合は、初期化失敗と判断
        if self.client is None and not self.use_mock:
            self.logger.warning(
                f"LLMエンジン '{self.current_engine}' の初期化に失敗しました "
                f"(APIキーソース/エラー: {self.api_key_source})。モックモードにフォールバックします。"
            )
            self.use_mock = True # モックモードを強制
            self._setup_mock()
            self.api_key_source = f"Initialization Error ({self.current_engine}) -> Fallback to Mock"
            self.current_engine = "mock" # エンジン状態を更新

        self.logger.info(
            f"LLMClient初期化完了。 Engine: {self.current_engine}, "
            f"Model: {self._get_model_name()}, APIキーソース: {self.api_key_source}"
        )

    def _get_model_name(self) -> str:
        """現在アクティブなエンジン用に設定されたモデル名を返します。"""
        if self.current_engine == "gemini":
            return self.config.GEMINI_MODEL_NAME
        elif self.current_engine == "openai":
            # OpenAIモデル名をconfigから取得するロジック (実装された場合)
            # return getattr(self.config, 'OPENAI_MODEL_NAME', "mock_openai_model_placeholder")
            return "mock_openai_model_placeholder" # v1.8ではプレースホルダー
        elif self.current_engine == "mock":
            return f"mock_model_v{self.config.VERSION}" # バージョン情報を含むモックモデル名
        else:
            # 通常、__init__の検証でこのケースには到達しないはず
            self.logger.error(f"不明なエンジンタイプ '{self.current_engine}' のためモデル名を取得できません。")
            return "unknown_model"

    def _initialize_gemini(self) -> None:
        """google.generativeaiライブラリを使用してGeminiクライアントを初期化します。"""
        self.logger.info(f"Geminiクライアント初期化中 (Model: {self.config.GEMINI_MODEL_NAME})...")
        
        api_key_from_config = self.config.GEMINI_API_KEY # NGGSConfigからAPIキーを取得
        if not api_key_from_config:
            self.logger.error(
                f"Gemini APIキー (環境変数 '{self.config.GEMINI_API_KEY_ENV}' または設定ファイル経由) が見つかりません。"
            )
            self.api_key_source = "Gemini Key Missing in Config/Env"
            self.client = None # クライアントがNoneであることを保証
            return # APIキーなしでは続行不可

        try:
            # genaiモジュールが正常にインポートされているか再確認 (Part 1のフラグとは別に)
            if genai is None or not GEMINI_AVAILABLE:
                # このケースは __init__ の GEMINI_AVAILABLE チェックで捕捉されるはずだが念のため
                self.logger.critical("Geminiライブラリ(genai)が利用できません。Geminiクライアントの初期化をスキップします。")
                self.api_key_source = "Gemini Library Not Loaded (genai is None)"
                self.client = None
                return

            # APIキーをgenaiライブラリに設定
            genai.configure(api_key=api_key_from_config)
            
            # GenerativeModelインスタンスを作成
            # NGGSConfigにGEMINI_SAFETY_SETTINGSフィールドを追加し、それを参照する
            gemini_safety_settings = getattr(self.config, 'GEMINI_SAFETY_SETTINGS', None)
            if gemini_safety_settings and isinstance(gemini_safety_settings, dict):
                # safety_settingsの形式をgoogle.generativeaiが期待する形式に変換する必要がある場合がある
                # 例: Dict[HarmCategory, HarmBlockThreshold]
                # ここでは、configの値が直接渡せる形式であると仮定
                self.logger.debug(f"Gemini Safety Settings適用: {gemini_safety_settings}")
                self.client = genai.GenerativeModel(
                    self.config.GEMINI_MODEL_NAME,
                    safety_settings=gemini_safety_settings # type: ignore
                )
            else:
                self.logger.debug("Gemini Safety Settings未指定。ライブラリデフォルトを使用。")
                self.client = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME)
            
            self.api_key_source = f"From Config (Env: {self.config.GEMINI_API_KEY_ENV} or direct)"
            self.logger.info("Geminiクライアントの初期化に成功しました。")

        except google_exceptions.PermissionDenied:
            self.logger.error("Gemini APIキーが無効であるか、必要な権限がありません。")
            self.api_key_source = "Gemini API Key Auth Error (PermissionDenied)"
            self.client = None
        except google_exceptions.InvalidArgument as e_invalid_arg:
            # モデル名が不正な場合などもこの例外が発生する可能性
            self.logger.error(f"Geminiクライアント初期化中の無効な引数エラー: {e_invalid_arg} (モデル名や設定を確認してください)")
            self.api_key_source = "Gemini Invalid Argument during Init"
            self.client = None
        except Exception as e_gemini_init_unexpected:
            # その他の予期せぬエラー
            self.logger.error(
                f"Geminiクライアント初期化中に予期せぬエラーが発生しました: {type(e_gemini_init_unexpected).__name__} - {e_gemini_init_unexpected}",
                exc_info=True
            )
            self.api_key_source = "Gemini Initialization Unexpected Error"
            self.client = None

    def _initialize_openai(self) -> None:
        """OpenAIクライアントを初期化します (v1.8ではプレースホルダー)。"""
        self.logger.warning(
            "OpenAIクライアントの初期化はNGGS-Lite v1.8ではプレースホルダー機能であり、"
            "実際には実装されていません。OpenAIエンジンを使用する設定は無視されます。"
        )
        # OpenAIサポートが追加された場合のロジック例:
        # openai_api_key = getattr(self.config, 'OPENAI_API_KEY', None)
        # if not openai_api_key:
        #     self.logger.error("OpenAI APIキーが見つかりません。")
        #     self.api_key_source = "OpenAI Key Missing"
        #     self.client = None
        #     return
        # try:
        #     if openai: # openaiモジュールがインポートされていれば
        #         self.client = openai.OpenAI(api_key=openai_api_key) # または AsyncOpenAI
        #         self.api_key_source = "OpenAI Key Provided (Placeholder)"
        #         self.logger.info("OpenAIクライアントの初期化に成功しました (プレースホルダー実装)。")
        #     else:
        #         self.logger.critical("OpenAIライブラリが利用できません。OpenAIクライアント初期化スキップ。")
        #         self.api_key_source = "OpenAI Library Not Loaded"
        #         self.client = None
        # except Exception as e_openai_init:
        #     self.logger.error(f"OpenAIクライアント初期化エラー: {e_openai_init}", exc_info=True)
        #     self.api_key_source = "OpenAI Init Error"
        #     self.client = None
        self.client = None # プレースホルダーなので常にNone
        # v1.8では正式サポートではないため、NotImplementedErrorを送出してもよい
        # raise NotImplementedError("OpenAIクライアント初期化はNGGS-Lite v1.8では実装されていません。")

# =============================================================================
# Part 4 End: Utilities (Continued), LLM Client Initialization
# =============================================================================
# =============================================================================
# Part 5: LLM Client (Generation Logic)
# (Continues LLMClient class from Part 4)
# =============================================================================
# Part 1, 2, 3, 4で定義された型やクラス (NGGSConfig, Result, LLMResponse,
# ConfigurationError, LLMError, GenerationError, SafeDict, truncate_text,
# GEMINI_AVAILABLE, OPENAI_AVAILABLE, genai, google_exceptions, exponential_backoffなど)
# および標準ライブラリが利用可能であることを前提とします。

import logging
import json
import time
import random
import re
from typing import (
    Dict, List, Any, Optional, Union, Tuple, Callable, Type, Final
)
# from nggs_lite_part1 import NGGSConfig, GEMINI_AVAILABLE, OPENAI_AVAILABLE, genai, google_exceptions, ConfigurationError # モジュール分割時
# from nggs_lite_part2 import Result, LLMResponse, LLMError, GenerationError # モジュール分割時
# from nggs_lite_part3 import truncate_text # モジュール分割時
# from nggs_lite_part4 import exponential_backoff # モジュール分割時 (Part 4で定義されていれば)

class LLMClient:
    # --- Methods from Part 4 (__init__, _get_model_name, _initialize_gemini, _initialize_openai) ---
    # これらのメソッドはPart 4で定義済みであると仮定し、ここでは再掲しません。
    # 完全なスクリプトでは、これらがこのクラス定義内に存在します。
    def __init__(self, config: NGGSConfig, use_mock: bool = False):
        self.config: NGGSConfig = config
        self.use_mock: bool = use_mock
        self.api_key_source: str = "N/A"
        self.call_count: int = 0
        self.last_call_time: float = 0.0
        self._mock_generate_internal: Optional[Callable[[str, float], LLMResponse]] = None
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.LLMClient")
        self.client: Optional[Any] = None
        self.current_engine: str = "mock" if use_mock else config.LLM_ENGINE

        if self.use_mock:
            self.logger.info("LLMClientは強制的にモックモードを使用します。")
            self._setup_mock()
            self.api_key_source = "Mock Forced"
            return

        if self.current_engine == "gemini":
            if not GEMINI_AVAILABLE:
                self.logger.critical("Geminiエンジンが選択されましたが、google-generativeaiライブラリが利用できません。")
                raise ConfigurationError("google-generativeaiライブラリがインストールされていません。")
            self._initialize_gemini()
        elif self.current_engine == "openai":
            if not OPENAI_AVAILABLE:
                self.logger.critical("OpenAIエンジンが選択されましたが、openaiライブラリが利用できません。")
                raise ConfigurationError("openaiライブラリがインストールされていません。")
            self._initialize_openai()
        else:
            unsupported_engine_msg = (
                f"サポートされていないLLMエンジンが設定されています: {self.current_engine}. "
                f"サポートされているのは 'gemini' (および 'openai' はプレースホルダー) のみです。"
            )
            self.logger.error(unsupported_engine_msg)
            raise ConfigurationError(unsupported_engine_msg)

        if self.client is None and not self.use_mock:
            self.logger.warning(
                f"LLMエンジン '{self.current_engine}' の初期化に失敗しました "
                f"(APIキーソース/エラー: {self.api_key_source})。モックモードにフォールバックします。"
            )
            self.use_mock = True
            self._setup_mock()
            self.api_key_source = f"Initialization Error ({self.current_engine}) -> Fallback to Mock"
            self.current_engine = "mock"

        self.logger.info(
            f"LLMClient初期化完了。 Engine: {self.current_engine}, "
            f"Model: {self._get_model_name()}, APIキーソース: {self.api_key_source}"
        )

    def _get_model_name(self) -> str:
        if self.current_engine == "gemini":
            return self.config.GEMINI_MODEL_NAME
        elif self.current_engine == "openai":
            return "mock_openai_model_placeholder"
        elif self.current_engine == "mock":
            return f"mock_model_v{self.config.VERSION}"
        else:
            self.logger.error(f"不明なエンジンタイプ '{self.current_engine}' のためモデル名を取得できません。")
            return "unknown_model"

    def _initialize_gemini(self) -> None:
        self.logger.info(f"Geminiクライアント初期化中 (Model: {self.config.GEMINI_MODEL_NAME})...")
        api_key_from_config = self.config.GEMINI_API_KEY
        if not api_key_from_config:
            self.logger.error(f"Gemini APIキー (環境変数 '{self.config.GEMINI_API_KEY_ENV}') が見つかりません。")
            self.api_key_source = "Gemini Key Missing in Config/Env"; self.client = None; return
        try:
            if genai is None or not GEMINI_AVAILABLE:
                self.logger.critical("Geminiライブラリ(genai)利用不可。初期化スキップ。"); self.api_key_source = "Gemini Lib Not Loaded"; self.client = None; return
            genai.configure(api_key=api_key_from_config)
            gemini_safety_settings = getattr(self.config, 'GEMINI_SAFETY_SETTINGS', None)
            if gemini_safety_settings and isinstance(gemini_safety_settings, dict):
                self.logger.debug(f"Gemini Safety Settings適用: {gemini_safety_settings}")
                self.client = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME, safety_settings=gemini_safety_settings)
            else:
                self.logger.debug("Gemini Safety Settings未指定。ライブラリデフォルト使用。")
                self.client = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME)
            self.api_key_source = f"From Config (Env: {self.config.GEMINI_API_KEY_ENV} or direct)"; self.logger.info("Geminiクライアント初期化成功。")
        except google_exceptions.PermissionDenied:
            self.logger.error("Gemini APIキー無効または権限不足。"); self.api_key_source = "Gemini API Key Auth Error"; self.client = None
        except google_exceptions.InvalidArgument as e_invalid_arg:
            self.logger.error(f"Gemini初期化中無効引数エラー: {e_invalid_arg}"); self.api_key_source = "Gemini Invalid Argument during Init"; self.client = None
        except Exception as e_gemini_init_unexpected:
            self.logger.error(f"Gemini初期化中予期せぬエラー: {type(e_gemini_init_unexpected).__name__} - {e_gemini_init_unexpected}", exc_info=True)
            self.api_key_source = "Gemini Initialization Unexpected Error"; self.client = None

    def _initialize_openai(self) -> None:
        self.logger.warning("OpenAIクライアント初期化はv1.8ではプレースホルダー。"); self.client = None
        # raise NotImplementedError("OpenAI client initialization is not implemented in NGGS-Lite v1.8.")
    # --- End of Methods from Part 4 ---

    def _setup_mock(self) -> None:
        """テストおよびAPIキーなしでの動作確認のためのモック生成関数をセットアップします。"""
        def mock_generate_internal_impl(prompt: str, temperature: float) -> LLMResponse:
            self.logger.debug(f"モック生成実行中 (温度: {temperature:.2f}). プロンプト先頭50文字: '{prompt[:50]}...'")
            time.sleep(random.uniform(0.01, 0.05))  # わずかな遅延をシミュレート
            
            response_text_content = f"Mocked Response for NGGS-Lite v{self.config.VERSION} (temperature={temperature:.2f})\n"
            prompt_lower_case = prompt.lower()
            metadata_for_response = {"mock": True, "engine": f"mock_v{self.config.VERSION}", "temperature": temperature}
            llm_response_obj = LLMResponse(text="", metadata=metadata_for_response) # 初期化

            if "評価" in prompt_lower_case or "evaluation" in prompt_lower_case:
                # ユーザーガイド1.2節の全評価項目を網羅するJSON構造を返す
                mock_scores: Dict[str, float] = {
                    # A. LLM基本評価項目
                    "gothic_atmosphere": round(random.uniform(2.5, 4.5), 1),
                    "stylistic_gravity": round(random.uniform(2.5, 4.8), 1),
                    "indirect_emotion": round(random.uniform(2.0, 4.0), 1),
                    "vocabulary_richness": round(random.uniform(2.8, 4.7), 1),
                    # B. ETI関連評価
                    "eti_overall": round(random.uniform(2.0, 4.5), 1),
                    "eti_boundary": round(random.uniform(1.5, 4.0), 1),
                    "eti_ambivalence": round(random.uniform(2.0, 4.2), 1),
                    "eti_transgression": round(random.uniform(1.8, 4.3), 1),
                    "eti_uncertainty": round(random.uniform(2.2, 4.6), 1),
                    "eti_transformation": round(random.uniform(2.0, 4.4), 1),
                    # C. RI関連評価
                    "ri_clarity": round(random.uniform(2.5, 4.8), 1),
                    "ri_visual_rhythm": round(random.uniform(2.0, 4.5), 1),
                    "ri_emotional_flow": round(random.uniform(2.2, 4.3), 1),
                    "ri_cognitive_load": round(random.uniform(2.8, 4.9), 1),
                    "ri_interpretive_resonance": round(random.uniform(1.5, 4.0), 1),
                    # D. その他主要評価項目
                    "subjective_depth": round(random.uniform(2.0, 4.7), 1),
                    "phase_transition_quality": round(random.uniform(2.2, 4.3), 1),
                    "colloquial_gothic_blend_quality": round(random.uniform(1.8, 4.1), 1),
                    "layer_balance_quality": round(random.uniform(2.0, 4.5), 1),
                    "emotion_arc_achievement": round(random.uniform(1.5, 4.0), 1),
                    # E. 総合評価
                    "overall_quality": round(random.uniform(2.5, 4.8), 1)
                }
                mock_reasoning: Dict[str, str] = {
                    key: f"これは項目「{key}」に対するモック評価の理由です。v1.8仕様。" for key in mock_scores
                }
                mock_eval_json_output = {"scores": mock_scores, "reasoning": mock_reasoning}
                try:
                    response_text_content += json.dumps(mock_eval_json_output, ensure_ascii=False, indent=2)
                except Exception:
                    response_text_content += '{"error": "モック評価JSONの生成に失敗しました。"}'
                llm_response_obj.text = response_text_content
            elif "改善指示" in prompt_lower_case or "improvement" in prompt_lower_case:
                response_text_content += (
                    "### モック改善指示 (NGGS-Lite v1.8)\n"
                    "- **ゴシック的雰囲気**: より深淵な恐怖と美のコントラストを追求してください。\n"
                    "- **主観描写**: 登場人物の内的葛藤や知覚の歪みを、具体的な感覚描写を通じて表現してください。\n"
                    "- **語彙**: 「退廃的」「荘厳」「両義性」といったキーワードを文脈に合わせて使用検討。\n"
                    "- **ETI向上**: 特に「境界性」と「不確定性」の要素を強化し、読者の現実感を揺さぶる描写を試みてください。"
                )
                llm_response_obj.text = response_text_content
            else: # 通常のテキスト生成
                response_text_content += f"これはモックモードで生成されたゴシック風テキストの断片です。プロンプトのテーマは「{truncate_text(prompt, 30)}」に関連する可能性があります。月影が古城の尖塔を照らし、長い影が中庭に伸びている。風が唸り声をあげ、不吉な予感を運んでくるかのようだ。"
                llm_response_obj.text = response_text_content
            
            # モックでもfinish_reasonを設定 (LLMResponseのフィールドに合わせる)
            if GEMINI_AVAILABLE and genai: # genai.typesが存在する場合
                 llm_response_obj.finish_reason = genai.types.Candidate.FinishReason.STOP # type: ignore
            else: # ダミーのfinish_reason
                 llm_response_obj.finish_reason = "STOP_MOCK"
            return llm_response_obj

        self._mock_generate_internal = mock_generate_internal_impl
        self.logger.info(f"モック生成関数 (v{self.config.VERSION}) のセットアップが完了しました。")

    def _adaptive_delay(self) -> None:
        """RPM制限に基づいてAPIリクエスト間の適応的遅延を管理します。"""
        rpm_limit_for_engine = 0
        if self.current_engine == "gemini":
            rpm_limit_for_engine = self.config.GEMINI_RPM_LIMIT
        elif self.current_engine == "openai": # v1.8ではプレースホルダー
            # openai_rpm_limit = getattr(self.config, 'OPENAI_RPM_LIMIT', 0)
            # rpm_limit_for_engine = openai_rpm_limit
            pass # OpenAIは現在対象外

        if rpm_limit_for_engine <= 0: # レート制限なし
            return

        min_interval_seconds = 60.0 / rpm_limit_for_engine
        current_monotonic_time = time.monotonic()

        if self.last_call_time > 0: # 前回の呼び出し時刻が記録されていれば
            elapsed_since_last_call = current_monotonic_time - self.last_call_time
            if elapsed_since_last_call < min_interval_seconds:
                wait_duration = min_interval_seconds - elapsed_since_last_call
                self.logger.debug(
                    f"RPM {rpm_limit_for_engine} limit ({self.current_engine}): "
                    f"{wait_duration:.3f}秒待機します..."
                )
                time.sleep(wait_duration)
                current_monotonic_time = time.monotonic() # 待機後の現在時刻を再取得

        self.last_call_time = current_monotonic_time # API呼び出し直前に最終呼び出し時刻を更新

    def _handle_api_error(self, exc: Exception, attempt_num: int) -> Tuple[bool, float]:
        """
        APIエラーを分類し、リトライの可否と遅延秒数を決定します。
        Retry-Afterヘッダー情報も考慮（将来的な拡張）。
        """
        should_retry_call: bool = False
        # exponential_backoffは0ベースのattemptを期待するため、attempt_num - 1 を渡す
        calculated_delay_seconds: float = exponential_backoff(
            attempt=attempt_num - 1, # 0-indexed attempt for backoff calculation
            base_delay=self.config.API_BASE_RETRY_DELAY,
            max_delay=self.config.API_MAX_RETRY_DELAY
        )
        error_type_name_str = type(exc).__name__
        error_message_str_lower = str(exc).lower()

        # Retry-Afterヘッダーの確認 (具体的な実装はライブラリ依存)
        # retry_after_header_seconds: Optional[float] = getattr(exc, 'retry_after', None)
        # if isinstance(retry_after_header_seconds, (int, float)):
        #     calculated_delay_seconds = max(calculated_delay_seconds, float(retry_after_header_seconds))
        #     self.logger.info(f"サーバーからRetry-Afterヘッダー値 ({retry_after_header_seconds}s) を検出。遅延に反映。")

        # Gemini APIエラーの分類
        if GEMINI_AVAILABLE and google_exceptions and isinstance(exc, google_exceptions.GoogleAPIError):
            if isinstance(exc, google_exceptions.ResourceExhausted): # レート制限など
                # API_RATE_LIMIT_DELAY を基本に、試行回数で増加させる
                rate_limit_specific_delay = self.config.API_RATE_LIMIT_DELAY * (1.8 ** (attempt_num - 1))
                calculated_delay_seconds = max(calculated_delay_seconds, rate_limit_specific_delay)
                # さらに大きな最大遅延を許容することも検討 (例: max_delay * 2)
                calculated_delay_seconds = min(self.config.API_MAX_RETRY_DELAY * 2.0, calculated_delay_seconds)
                self.logger.warning(
                    f"APIリソース枯渇エラー (試行 {attempt_num})。レート制限専用遅延適用。{calculated_delay_seconds:.1f}秒後にリトライします。"
                )
                should_retry_call = True
            elif isinstance(exc, (google_exceptions.InternalServerError,
                                  google_exceptions.ServiceUnavailable,
                                  google_exceptions.DeadlineExceeded)):
                self.logger.warning(
                    f"リトライ可能なGemini APIエラー ({error_type_name_str}, 試行 {attempt_num})。"
                    f"{calculated_delay_seconds:.1f}秒後にリトライします。"
                )
                should_retry_call = True
            elif isinstance(exc, (google_exceptions.InvalidArgument, google_exceptions.PermissionDenied,
                                  google_exceptions.Unauthenticated, google_exceptions.NotFound,
                                  google_exceptions.FailedPrecondition)):
                # BlockedByPolicyやStopCandidateExceptionもこの系統のエラーとして扱うか、
                # あるいはValueErrorとして別途処理するか検討。
                # ここでは、これらのGoogleAPIErrorはリトライ不可とする。
                self.logger.error(f"リトライ不能なGemini APIエラー ({error_type_name_str}): {exc}")
                should_retry_call = False
            else: # その他のGoogleAPIError
                self.logger.error(f"未分類のGoogle APIエラー ({error_type_name_str}): {exc}", exc_info=False)
                should_retry_call = False # デフォルトではリトライしない
        
        # コンテンツブロック関連のValueError (Gemini APIが送出する可能性)
        elif isinstance(exc, ValueError) and \
             ("block" in error_message_str_lower or "safety" in error_message_str_lower or "policy" in error_message_str_lower):
            self.logger.error(f"コンテンツ生成が安全設定またはポリシーによりブロックされました: {exc}")
            should_retry_call = False # 同一プロンプトでのリトライは無意味

        # OpenAI APIエラーの分類 (v1.8ではプレースホルダー)
        elif OPENAI_AVAILABLE and openai and hasattr(openai, 'APIError') and isinstance(exc, openai.APIError):
            # OpenAIのエラー処理ロジック (省略)
            self.logger.warning(f"OpenAI APIエラー ({error_type_name_str}) の処理はv1.8ではプレースホルダーです。")
            should_retry_call = False # フォールバックとしてリトライしない

        else: # その他の予期せぬエラー
            self.logger.error(f"API関連の予期せぬエラー ({error_type_name_str}): {exc}", exc_info=True) # トレースバック付きでログ
            should_retry_call = False # デフォルトではリトライしない

        return should_retry_call, calculated_delay_seconds

    def generate(self, prompt: str, temperature: Optional[float] = None) -> Result[str, LLMError]:
        """
        設定されたLLMエンジンを使用してテキストを生成します。
        堅牢なリトライメカニズムを備えています。
        """
        self.logger.debug(
            f"LLMClient.generate呼び出し開始。Engine: {self.current_engine}, "
            f"Temperature: {temperature if temperature is not None else 'Default (Config)'}, Mock: {self.use_mock}"
        )
        self.call_count += 1

        if self.use_mock: # モックモードの場合
            if self._mock_generate_internal:
                try:
                    effective_temp = temperature if temperature is not None \
                                     else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.80)
                    mock_llm_response_obj = self._mock_generate_internal(prompt, effective_temp)
                    return mock_llm_response_obj.to_result() # LLMResponse.to_result() を使用
                except Exception as e_mock_gen:
                    # LLMResponse, GenerationError は Part 2 で定義済み
                    err_obj = GenerationError(f"モック生成中に予期せぬエラー: {e_mock_gen}",
                                              details={"prompt_preview": truncate_text(prompt, 70)})
                    self.logger.error(str(err_obj), exc_info=True)
                    return Result.fail(err_obj)
            else:
                # ConfigurationError は Part 2 で定義済み
                return Result.fail(ConfigurationError("モックモードが有効ですが、内部モック関数が未設定です。"))

        # --- 実際のAPI呼び出し (リトライロジック付き) ---
        last_encountered_exception: Optional[Exception] = None
        
        for attempt_count in range(self.config.API_MAX_RETRIES + 1): # 最大リトライ回数 + 1 (初回試行)
            current_attempt_num = attempt_count + 1 # 1ベースの試行回数
            try:
                self._adaptive_delay() # APIレート制限のための遅延処理
                
                api_call_start_time = time.monotonic()
                # 温度設定: 引数 > configのデフォルト
                effective_temperature_for_call = temperature if temperature is not None \
                    else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.80)

                generated_text_content: Optional[str] = None
                llm_response_obj_from_call: Optional[LLMResponse] = None # 詳細応答用

                if self.current_engine == "gemini":
                    if not self.client or not (GEMINI_AVAILABLE and genai and isinstance(self.client, genai.GenerativeModel)):
                        # LLMError は Part 2 で定義済み
                        raise LLMError("Geminiクライアントが正しく初期化されていないか利用不可能です。")
                    # _call_gemini は LLMResponse オブジェクトを返すように変更
                    llm_response_obj_from_call = self._call_gemini(prompt, effective_temperature_for_call)
                elif self.current_engine == "openai": # v1.8ではプレースホルダー
                    llm_response_obj_from_call = self._call_openai(prompt, effective_temperature_for_call)
                else:
                    raise ConfigurationError(f"generate()呼び出し時に無効なエンジンが選択されました: {self.current_engine}")

                api_call_duration_seconds = time.monotonic() - api_call_start_time
                self.logger.info(
                    f"{self.current_engine} API呼び出し成功 ({api_call_duration_seconds:.2f}秒, 試行 {current_attempt_num}回目)."
                )

                if llm_response_obj_from_call:
                    # LLMResponse.to_result() でテキスト抽出と基本的な検証を行う
                    result_from_llm_response = llm_response_obj_from_call.to_result()
                    if result_from_llm_response.is_ok:
                        return Result.ok(result_from_llm_response.unwrap()) # 成功
                    else: # to_result()がエラーを返した場合 (例: 空テキスト、ブロック)
                        last_encountered_exception = result_from_llm_response.error
                        self.logger.warning(f"LLM応答の処理で問題発生 (試行 {current_attempt_num}): {last_encountered_exception}")
                        # このエラーがリトライ可能か確認
                        should_retry_after_proc_error, delay_for_proc_error = self._handle_api_error(
                            last_encountered_exception if last_encountered_exception else GenerationError("Unknown processing error"),
                            current_attempt_num
                        )
                        if not should_retry_after_proc_error or current_attempt_num > self.config.API_MAX_RETRIES:
                            break # リトライ不可または最大リトライ回数超過ならループ終了
                        # リトライする場合はループの最後で遅延
                else: # _call_gemini/_call_openai が None を返した場合 (通常は例外を送出すべき)
                    last_encountered_exception = GenerationError(f"{self.current_engine} API呼び出しがNone応答を返しました。")
                    self.logger.error(str(last_encountered_exception))
                    break # リトライせずにループ終了

            except Exception as e_api_call:
                last_encountered_exception = e_api_call
                should_retry_call, delay_seconds = self._handle_api_error(e_api_call, current_attempt_num)

                if not should_retry_call:
                    # リトライ不能なエラーの場合はループを抜ける
                    final_error_obj = GenerationError(
                        f"{self.current_engine} API呼び出し失敗 (リトライ不能): {last_encountered_exception}",
                        details={"engine": self.current_engine, "prompt_preview": truncate_text(prompt, 70)}
                    )
                    # エラーログは _handle_api_error 内で出力済みのはず
                    return Result.fail(final_error_obj)

                if current_attempt_num > self.config.API_MAX_RETRIES: # 最大リトライ回数超過
                    break # ループ終了

                self.logger.warning(
                    f"{self.current_engine} API呼び出しを {delay_seconds:.1f}秒後にリトライします... "
                    f"(試行 {current_attempt_num + 1}/{self.config.API_MAX_RETRIES + 1})"
                )
                time.sleep(delay_seconds)
        
        # ループが完了 (最大リトライ回数超過またはリトライ不能エラーでbreak)
        final_error_message_str = f"{self.current_engine} API呼び出しが {self.config.API_MAX_RETRIES + 1}回の試行後も失敗しました。"
        if last_encountered_exception:
            final_error_message_str += f" 最後のエラー: {type(last_encountered_exception).__name__} - {str(last_encountered_exception)}"
        else: # このケースは通常発生しないはず
            final_error_message_str += " 不明な理由または空応答。"

        final_generation_error_obj = GenerationError(
            final_error_message_str,
            details={"last_error_type": type(last_encountered_exception).__name__ if last_encountered_exception else "None",
                     "engine": self.current_engine, "prompt_preview": truncate_text(prompt, 70)}
        )
        self.logger.error(str(final_generation_error_obj))
        return Result.fail(final_generation_error_obj)

    def _call_gemini(self, prompt: str, temperature: float) -> Optional[LLMResponse]:
        """
        Gemini APIを呼び出し、応答をLLMResponseオブジェクトとして処理する内部メソッド。
        エラー発生時は例外を送出する。
        """
        if not self.client or not GEMINI_AVAILABLE or not genai or not isinstance(self.client, genai.GenerativeModel):
            # このエラーはgenerate()の呼び出し元で捕捉される
            raise LLMError("Geminiクライアントが利用できません (_call_gemini内部チェック)。")

        # GenerationConfigの準備 (Part 4の改善案を反映)
        generation_config_dict = self.config.GENERATION_CONFIG_DEFAULT.copy()
        generation_config_dict["temperature"] = temperature
        # Gemini APIはcandidate_count=1のみサポートすることが多いので、1に固定
        generation_config_dict["candidate_count"] = 1
        
        # genai.types.GenerationConfigが受け付けるキーのみを抽出
        valid_genai_config_keys = {
            "temperature", "top_p", "top_k", "candidate_count", "max_output_tokens", "stop_sequences"
        }
        config_dict_for_api = {
            k: v for k, v in generation_config_dict.items() if k in valid_genai_config_keys and v is not None
        }

        try:
            final_gemini_generation_config = genai.types.GenerationConfig(**config_dict_for_api)
        except TypeError as e_gen_cfg_type:
            self.logger.error(f"無効なGemini GenerationConfigパラメータ: {config_dict_for_api}, エラー: {e_gen_cfg_type}")
            # ConfigurationError は Part 2 で定義済み
            raise ConfigurationError(f"無効なGemini GenerationConfigパラメータ: {e_gen_cfg_type}") from e_gen_cfg_type

        self.logger.debug(
            f"Gemini API呼び出し実行中 (Temp: {temperature:.2f}, TopP: {config_dict_for_api.get('top_p')}, TopK: {config_dict_for_api.get('top_k')}). "
            f"Prompt長: {len(prompt)}"
        )
        if len(prompt) > 1500: # 長いプロンプトの場合はプレビューをログに出力
            self.logger.debug(f"プロンプトプレビュー (長文のため一部表示): {truncate_text(prompt, 250)}")

        response_from_gemini_api: Optional[Any] = None
        try:
            # Gemini API呼び出し
            response_from_gemini_api = self.client.generate_content(
                contents=[prompt], # リスト形式で渡す
                generation_config=final_gemini_generation_config,
                request_options={'timeout': self.config.API_TIMEOUT}
                # self.config.GEMINI_SAFETY_SETTINGS は _initialize_gemini でモデルに設定済み
            )
        except Exception as e_api_call_gemini:
            # API呼び出し中のエラーはgenerate()のリトライループで処理されるように再送出
            self.logger.error(
                f"Gemini generate_content呼び出しでエラー発生: {type(e_api_call_gemini).__name__} - {e_api_call_gemini}",
                exc_info=False # リトライ処理で詳細ログが出るのでここでは抑制
            )
            raise e_api_call_gemini

        # --- Gemini API応答の処理 ---
        if response_from_gemini_api is None:
            self.logger.warning("Gemini APIからの応答がNoneでした。")
            return None # またはエラーを送出

        # LLMResponseオブジェクトのフィールドを初期化
        response_text_content: str = ""
        response_metadata: JsonDict = {"model_used": self.config.GEMINI_MODEL_NAME}
        response_prompt_feedback: Optional[Any] = None
        response_finish_reason: Optional[Any] = None
        response_safety_ratings: Optional[List[Any]] = None
        
        try:
            # 1. プロンプトレベルのフィードバック確認 (コンテンツブロックなど)
            response_prompt_feedback = getattr(response_from_gemini_api, "prompt_feedback", None)
            if response_prompt_feedback and hasattr(response_prompt_feedback, "block_reason") and response_prompt_feedback.block_reason:
                block_reason_name = getattr(response_prompt_feedback.block_reason, 'name', str(response_prompt_feedback.block_reason))
                safety_ratings_list_prompt = getattr(response_prompt_feedback, "safety_ratings", [])
                ratings_str_prompt = ", ".join([
                    f"{getattr(r.category, 'name', 'UNK_CAT')}={getattr(r.probability, 'name', 'UNK_PROB')}"
                    for r in safety_ratings_list_prompt if hasattr(r, 'category') and hasattr(r, 'probability')
                ])
                # GenerationError は Part 2 で定義済み
                raise GenerationError(
                    f"コンテンツ生成がプロンプトレベルでブロックされました (Gemini): 理由='{block_reason_name}'",
                    details={"block_reason": block_reason_name, "safety_ratings_prompt": ratings_str_prompt or "N/A"}
                )

            # 2. 候補(candidates)の存在と内容確認
            if not hasattr(response_from_gemini_api, 'candidates') or not response_from_gemini_api.candidates:
                # 候補がない場合、応答レベルの終了理由を確認
                response_level_finish_reason_obj = getattr(response_from_gemini_api, 'finish_reason', None)
                response_level_finish_reason_str = getattr(response_level_finish_reason_obj, 'name', str(response_level_finish_reason_obj))
                prompt_feedback_str_log = str(response_prompt_feedback) if response_prompt_feedback else "N/A"
                self.logger.warning(
                    f"Gemini応答に候補(candidates)が含まれていません。応答レベル終了理由: {response_level_finish_reason_str}. "
                    f"プロンプトフィードバック: {prompt_feedback_str_log}"
                )
                # このケースもGenerationErrorとして扱うか、空のLLMResponseを返すか検討
                # ここでは、テキストがないので空のLLMResponseを返す
                return LLMResponse(text="", metadata=response_metadata, prompt_feedback=response_prompt_feedback, finish_reason=response_level_finish_reason_obj)


            # 3. 最初の候補を処理 (通常、candidate_count=1なので最初の要素のみ)
            first_candidate = response_from_gemini_api.candidates[0]
            response_finish_reason = getattr(first_candidate, "finish_reason", None)
            response_safety_ratings = getattr(first_candidate, "safety_ratings", [])

            # 4. 候補の終了理由(finish_reason)確認 (SAFETYなど)
            if response_finish_reason == genai.types.Candidate.FinishReason.SAFETY: # type: ignore
                ratings_str_candidate = ", ".join([
                    f"{getattr(r.category, 'name', 'UNK_CAT')}={getattr(r.probability, 'name', 'UNK_PROB')}"
                    for r in response_safety_ratings if hasattr(r, 'category') and hasattr(r, 'probability')
                ])
                raise GenerationError(
                    "コンテンツ生成が候補レベルでブロックされました (Gemini Candidate): 理由='SAFETY'",
                    details={"finish_reason": "SAFETY", "safety_ratings_candidate": ratings_str_candidate or "N/A"}
                )
            elif response_finish_reason is not None and response_finish_reason != genai.types.Candidate.FinishReason.STOP: # type: ignore
                # STOP以外の終了理由 (MAX_TOKENS, RECITATIONなど)
                finish_reason_name_log = getattr(response_finish_reason, 'name', str(response_finish_reason))
                self.logger.warning(f"Gemini生成候補の終了理由がSTOP以外です: {finish_reason_name_log}")
                # RECITATIONもGenerationErrorとして扱うことを検討
                if response_finish_reason == genai.types.Candidate.FinishReason.RECITATION: # type: ignore
                     raise GenerationError("コンテンツ生成が候補レベルで停止しました (Gemini Candidate): 理由='RECITATION'",
                                           details={"finish_reason": "RECITATION"})


            # 5. content partsからテキストを安全に抽出
            candidate_content_obj = getattr(first_candidate, "content", None)
            content_parts_list = getattr(candidate_content_obj, "parts", []) if candidate_content_obj else []

            if not content_parts_list:
                if response_finish_reason == genai.types.Candidate.FinishReason.STOP: # type: ignore
                    self.logger.warning("Gemini候補はSTOPで終了しましたが、content.partsが空です。")
                else:
                    finish_reason_name_log_empty = getattr(response_finish_reason, 'name', str(response_finish_reason))
                    self.logger.debug(f"Gemini候補のcontent.partsが空です。終了理由: {finish_reason_name_log_empty}")
                # テキストがないので空のLLMResponse
                return LLMResponse(text="", metadata=response_metadata, prompt_feedback=response_prompt_feedback,
                                   finish_reason=response_finish_reason, safety_ratings=response_safety_ratings)

            response_text_content = "".join(part.text for part in content_parts_list if hasattr(part, "text") and isinstance(part.text, str))
            
            # 応答メタデータにトークン数などの情報を追加 (可能であれば)
            # response_from_gemini_api.usage_metadata などから取得
            usage_metadata = getattr(response_from_gemini_api, 'usage_metadata', None)
            if usage_metadata:
                response_metadata['prompt_token_count'] = getattr(usage_metadata, 'prompt_token_count', None)
                response_metadata['candidates_token_count'] = getattr(usage_metadata, 'candidates_token_count', None)
                response_metadata['total_token_count'] = getattr(usage_metadata, 'total_token_count', None)

            return LLMResponse(
                text=response_text_content.strip(), # strip()して返す
                metadata=response_metadata,
                prompt_feedback=response_prompt_feedback,
                finish_reason=response_finish_reason,
                safety_ratings=response_safety_ratings
            )

        except AttributeError as e_attr_resp_proc: # 応答オブジェクトの構造が予期しない場合
            self.logger.error(f"Gemini応答の解析中に属性エラー: {e_attr_resp_proc}", exc_info=False)
            # LLMError は Part 2 で定義済み
            raise LLMError(f"Gemini応答の構造解析エラー: {e_attr_resp_proc}") from e_attr_resp_proc
        except GenerationError: # 内部で送出されたGenerationErrorはそのまま伝播
            raise
        except Exception as e_resp_proc_unexpected: # その他の予期せぬエラー
            self.logger.error(f"Gemini応答処理中に予期せぬエラー: {e_resp_proc_unexpected}", exc_info=True)
            raise LLMError(f"Gemini応答処理中の予期せぬエラー: {e_resp_proc_unexpected}") from e_resp_proc_unexpected

    def _call_openai(self, prompt: str, temperature: float) -> Optional[LLMResponse]:
        """OpenAI APIを呼び出す内部メソッド (v1.8ではプレースホルダー)。"""
        self.logger.warning("OpenAI API呼び出し関数はv1.8ではプレースホルダーであり、実装されていません。")
        # raise NotImplementedError("OpenAIクライアントとのインタラクションはNGGS-Lite v1.8では実装されていません。")
        # プレースホルダーとしてNoneを返すか、ダミーのLLMResponseを返す
        return LLMResponse(
            text=f"OpenAI Mock Response (prompt: {truncate_text(prompt,30)}, temp: {temperature})",
            metadata={"engine": "mock_openai", "model_used": "placeholder_openai_model"}
        )

# =============================================================================
# Part 5 End: LLM Client (Generation Logic)
# =============================================================================
# =============================================================================
# Part 6: Vocabulary Management (Initialization and Processing)
# =============================================================================
# Part 1, 2, 3, 4で定義された型やクラス (NGGSConfig, Result, VocabularyError,
# JsonDict, load_vocabulary, ConfigurationErrorなど)
# および標準ライブラリが利用可能であることを前提とします。

import logging
import pathlib
import re # _parse_structured_list での文字列処理に使用する可能性
from typing import (
    Dict, List, Any, Optional, Union, Set, Final, TypeAlias
)
from dataclasses import dataclass, field

# Part 1からNGGSConfig, JsonDict, ConfigurationErrorをインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# Part 2からResult, VocabularyErrorをインポート
# from nggs_lite_part2 import Result, VocabularyError
# Part 4からload_vocabularyをインポート
# from nggs_lite_part4 import load_vocabulary

# このモジュール専用のロガー
logger_vocab_manager = logging.getLogger("NGGS-Lite.VocabularyManager")

@dataclass
class VocabularyItem:
    """
    単一の語彙項目と関連メタデータを表現します。
    v1.8: 一貫性を確保し、レイヤー初期化を処理します。
    """
    word: str
    categories: List[str] = field(default_factory=list)  # 広範なグルーピング
    example: Optional[str] = None  # 使用法を示す例文
    tags: List[str] = field(default_factory=list)  # フィルタリング用タグ (例: 口語レベル、時代)
    layers: Set[str] = field(default_factory=set)  # 四層モデルに基づくレイヤー (VALID_LAYERSに対して検証)
    usage_count: int = 0  # 重み付きサンプリングのための使用回数
    source: Optional[str] = None  # 単語の出所 (例: 'glcai', 'default_file')

    def __post_init__(self):
        """初期化後にlayers属性が常にセットであることを保証します。"""
        if self.layers is None:  # default_factoryが正しく使用されなかった場合のNoneを処理
            self.layers = set()
        elif not isinstance(self.layers, set):
            try:
                # リストや他のイテラブルからの変換を試みる
                self.layers = set(self.layers)
            except TypeError:
                # 変換失敗時は警告ログを出し、空セットをデフォルトとする
                # グローバルなlogger_vocab_managerを参照
                logger_vocab_manager.warning(
                    f"単語 '{self.word}' のレイヤー '{self.layers}' をセットに変換できませんでした。"
                    f"空セットをデフォルトとします。"
                )
                self.layers = set()
        # VocabularyManager.VALID_LAYERSに対する検証はVocabularyManager側で行う


class VocabularyManager:
    """
    ゴシック語彙を管理し、様々なデータ形式とフィルタリングをサポートします。
    v1.8: 優先順位（GLCAI > デフォルトファイル > 内部）に基づくロードロジックの改良、
          解析とアイテム処理の改善。
    """
    # 有効なレイヤーをクラス定数として明確に定義
    VALID_LAYERS: Final[Set[str]] = {"physical", "sensory", "psychological", "symbolic"}

    def __init__(self, config: NGGSConfig):
        """
        NGGSConfigのパスと優先順位を使用してVocabularyManagerを初期化します。

        Args:
            config: 語彙パスと設定を含むNGGSConfigオブジェクト。
        """
        if not isinstance(config, NGGSConfig):
            err_msg = "VocabularyManagerには有効なNGGSConfigインスタンスが必要です。"
            # ロガーが完全に設定されていない可能性があるため、フォールバックとしてprintも使用
            try:
                logging.getLogger("NGGS-Lite.VocabularyManager").critical(err_msg)
            except Exception:
                print(f"CRITICAL (VocabularyManager): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg) # Part 1で定義されたConfigurationErrorを使用

        self.config: NGGSConfig = config
        self.logger: logging.Logger = logger_vocab_manager # 専用の語彙ロガーを使用
        self.items: List[VocabularyItem] = [] # 全語彙アイテムのリスト
        # レイヤー名をキーとし、そのレイヤーに属するVocabularyItemオブジェクトのリストを値とする辞書
        self.layered_vocabulary: Dict[str, List[VocabularyItem]] = {
            layer: [] for layer in self.VALID_LAYERS
        }
        self.all_words: Set[str] = set() # 既存単語の迅速なチェック用セット（大文字・小文字区別）
        self.loaded_source_description: str = "Unknown" # 実際にロードされた語彙ソースの説明

        self.logger.info("VocabularyManagerを初期化中...")
        # 設定された優先順位に基づいて語彙をロード (Part 4のユーティリティ関数を使用)
        load_result: Result[Union[List[str], JsonDict], VocabularyError] = load_vocabulary(self.config)

        if load_result.is_ok:
            vocab_data = load_result.unwrap()
            # 成功したロードソースに基づいて説明を決定
            # load_vocabularyは成功したソースを返さないため、ファイル存在確認で推測
            # (より正確にはload_vocabularyがソース情報を返すように変更するのが望ましい)
            # ここではプランに従い、ファイル存在で推測
            glcai_path_str = str(self.config.GLCAI_VOCAB_PATH) if self.config.GLCAI_VOCAB_PATH else ""
            default_vocab_path_str = str(self.config.DEFAULT_VOCAB_PATH) if self.config.DEFAULT_VOCAB_PATH else ""

            # BASE_DIR基準でパスを解決
            glcai_full_path = (BASE_DIR / glcai_path_str).resolve() if glcai_path_str and not pathlib.Path(glcai_path_str).is_absolute() else pathlib.Path(glcai_path_str)
            default_full_path = (BASE_DIR / default_vocab_path_str).resolve() if default_vocab_path_str and not pathlib.Path(default_vocab_path_str).is_absolute() else pathlib.Path(default_vocab_path_str)

            if self.config.VOCAB_LOAD_PRIORITY[0] == 'glcai' and glcai_path_str and glcai_full_path.exists():
                 self.loaded_source_description = f"GLCAI File ({glcai_full_path.name})"
            elif default_vocab_path_str and default_full_path.exists(): # GLCAIが優先でも存在しない場合、またはdefaultが優先の場合
                 self.loaded_source_description = f"Default NGGS File ({default_full_path.name})"
            else: # どのファイルも存在しないか、ロードに失敗したが内部デフォルトが使われた場合
                 self.loaded_source_description = "Internal Default Vocabulary"
            
            self._process_loaded_data(vocab_data, self.loaded_source_description)
        else:
            # エラーはload_vocabularyユーティリティによってログ記録済みのはず
            self.logger.error(f"VocabularyManagerの初期化に失敗しました: {load_result.error}")
            self.loaded_source_description = f"Failed to load ({load_result.error})"
            # ロード失敗時は空の語彙で初期化されるが、エラーソースを記録

        self.logger.info(
            f"VocabularyManagerは {len(self.items)} 個のユニークなアイテムで初期化されました。"
            f"ソース: {self.loaded_source_description}"
        )
        # デバッグ用に各レイヤーのアイテム数をログ記録
        for layer_key, items_in_layer_list in self.layered_vocabulary.items():
            self.logger.debug(f"  レイヤー '{layer_key}': {len(items_in_layer_list)} アイテム")

    def _process_loaded_data(self, loaded_data: Union[List[str], JsonDict], loaded_from_source: str) -> None:
        """ロードされた語彙データを処理します。"""
        if not loaded_data: # 空のリストや辞書も含む
            self.logger.error(f"語彙データが空または無効です ({loaded_from_source} からロード後)。")
            return

        self.logger.info(f"{loaded_from_source} からロードされた語彙データを処理中...")
        try:
            # データ型をチェックし、適切なパーサーを呼び出す
            if isinstance(loaded_data, dict) and any(key in self.VALID_LAYERS for key in loaded_data.keys()):
                # NGGSデフォルトの階層化形式（キーがレイヤー名）の可能性が高い
                self._parse_layered_dict(loaded_data)
            elif isinstance(loaded_data, list) and loaded_data: # リストが空でないことを確認
                first_item_in_list = loaded_data[0]
                if isinstance(first_item_in_list, dict):
                    # 構造化リスト形式（例：GLCAIエクスポート）の可能性が高い
                    self._parse_structured_list(loaded_data)
                elif isinstance(first_item_in_list, str):
                    # 単純なフラット単語リスト
                    self._parse_flat_list(loaded_data)
                else:
                    self.logger.warning(
                        f"{loaded_from_source} からの語彙データのリストアイテム形式が認識できません。"
                        f"最初のアイテムの型: {type(first_item_in_list)}"
                    )
            elif isinstance(loaded_data, dict): # 辞書だが、トップレベルキーがレイヤー名ではない場合
                self.logger.warning(
                    f"語彙データは辞書ですが、期待される階層化形式ではありません（キーがレイヤー名ではない）。"
                    f"未知の辞書形式（値が単語リストの可能性）として解析を試みます。ソース: {loaded_from_source}"
                )
                self._parse_unknown_dict_format(loaded_data)
            else:
                self.logger.error(
                    f"{loaded_from_source} からの語彙データを解析できません。予期しないトップレベル型: {type(loaded_data)}"
                )
        except Exception as e_parse_struct:
            # 解析プロセス自体のエラーをキャッチ
            self.logger.error(f"{loaded_from_source} からの語彙データ構造の処理中にエラー: {e_parse_struct}", exc_info=True)

    def _add_item(self, item_to_add: VocabularyItem) -> None:
        """
        検証済みの語彙アイテムをマネージャーのリストとセットに追加します。
        単語の一意性を保証し、レイヤーを検証/推論します。
        """
        if not item_to_add.word or not isinstance(item_to_add.word, str):
            self.logger.debug(f"無効なVocabularyItemをスキップ: wordが欠落または文字列ではありません ({item_to_add})")
            return

        word_stripped = item_to_add.word.strip()
        if not word_stripped:
            self.logger.debug("無効なVocabularyItemをスキップ: wordがトリミング後に空です。")
            return
        
        # 重複チェック (大文字・小文字を区別)
        if word_stripped in self.all_words:
            # self.logger.debug(f"重複する語彙 '{word_stripped}' をスキップします。") # 詳細ログは冗長な場合がある
            return

        item_to_add.word = word_stripped # 保存する単語はトリミング済みであることを保証

        # --- レイヤーの検証と推論 ---
        original_input_layers = item_to_add.layers.copy() # ログ記録用に元レイヤーを保持
        valid_layers_from_input = {layer for layer in item_to_add.layers if layer in self.VALID_LAYERS}

        if not valid_layers_from_input: # 有効なレイヤーが提供されなかった場合
            inferred_item_layers = self._infer_layers(word_stripped) # レイヤーを推論
            item_to_add.layers = {layer for layer in inferred_item_layers if layer in self.VALID_LAYERS} # 推論結果も検証
            # 入力レイヤーが無効だったために推論が必要だった場合にログ記録
            if original_input_layers: # ユーザーが何か無効なものを入力した場合のみログ
                invalid_provided_layers = original_input_layers - self.VALID_LAYERS
                if invalid_provided_layers:
                    self.logger.debug(
                        f"単語 '{word_stripped}': 無効な入力レイヤー {invalid_provided_layers}。"
                        f"推論されたレイヤー: {item_to_add.layers}"
                    )
        else: # 有効なレイヤーがユーザー/データソースから提供された場合
            item_to_add.layers = valid_layers_from_input

        # --- デフォルトレイヤーの割り当て (それでもレイヤーがない場合) ---
        if not item_to_add.layers:
            default_assigned_layer = "psychological" # 合理的なデフォルト
            self.logger.debug(
                f"単語 '{word_stripped}' は、推論も検証もできなかったため、デフォルトレイヤー '{default_assigned_layer}' に割り当てられました。"
            )
            item_to_add.layers = {default_assigned_layer}

        # --- カテゴリ推論 (必要かつ未提供の場合) ---
        if not item_to_add.categories: # categoriesリストが空の場合のみ推論
            item_to_add.categories = self._infer_categories(word_stripped)
            # self.logger.debug(f"単語 '{word_stripped}' の推論カテゴリ: {item_to_add.categories}")

        # 検証済みアイテムを追加
        self.items.append(item_to_add)
        self.all_words.add(word_stripped)

        # 階層化辞書に追加
        for layer_name_str in item_to_add.layers:
            # レイヤー検証は上記で行われたため、キーは存在するはず
            if layer_name_str in self.layered_vocabulary:
                self.layered_vocabulary[layer_name_str].append(item_to_add)
            else: # このケースはバグの可能性を示唆
                self.logger.error(
                    f"内部エラー: アイテム '{word_stripped}' を予期しないレイヤー '{layer_name_str}' "
                    f"(VALID_LAYERSに未定義) に追加しようとしました。このレイヤーへの割り当てをスキップします。"
                )
        # 追加アイテムのログは非常に冗長になる可能性があるため、デフォルトでは無効化
        # self.logger.debug(f"語彙アイテム追加: {item_to_add.word} (レイヤー: {item_to_add.layers}, カテゴリ: {item_to_add.categories})")

    def _parse_flat_list(self, vocab_string_list: List[str]) -> None:
        """単純な語彙単語リストを解析します。"""
        self.logger.debug(f"フラットリストから語彙を解析中 ({len(vocab_string_list)} 個の潜在的単語)。")
        num_added_successfully = 0
        for word_entry_str in vocab_string_list:
            if isinstance(word_entry_str, str) and word_entry_str.strip():
                # アイテムを作成、推論は_add_item内で行われる
                new_vocab_item = VocabularyItem(word=word_entry_str.strip())
                # 追加前に元の単語を保存し、追加されたか確認
                original_word_to_check = new_vocab_item.word
                self._add_item(new_vocab_item)
                # 実際に単語が追加されたか確認 (重複や空でない)
                if original_word_to_check in self.all_words and \
                   any(item.word == original_word_to_check for item in self.items):
                    num_added_successfully += 1
        self.logger.info(f"フラットリストから {num_added_successfully} 個のユニークな単語を解析しました。")

    def _parse_layered_dict(self, layered_vocab_dict: JsonDict) -> None:
        """レイヤーをキーとする辞書から語彙を解析します。"""
        self.logger.debug("階層化辞書から語彙を解析中。")
        num_added_successfully = 0
        num_total_words_processed = 0
        for layer_key_str, words_in_layer_list in layered_vocab_dict.items():
            if layer_key_str in self.VALID_LAYERS:
                if isinstance(words_in_layer_list, list):
                    num_total_words_processed += len(words_in_layer_list)
                    for word_entry_str in words_in_layer_list:
                        if isinstance(word_entry_str, str) and word_entry_str.strip():
                            # レイヤーを明示的に割り当て
                            new_vocab_item = VocabularyItem(word=word_entry_str.strip(), layers={layer_key_str})
                            original_word_to_check = new_vocab_item.word
                            self._add_item(new_vocab_item)
                            if original_word_to_check in self.all_words and \
                               any(item.word == original_word_to_check for item in self.items):
                                num_added_successfully += 1
                else:
                    self.logger.warning(f"有効なレイヤー '{layer_key_str}' の値がリストではありません。スキップします。")
            else:
                self.logger.warning(f"階層化辞書内の無効なレイヤーキー '{layer_key_str}' を無視します。")
        self.logger.info(
            f"階層化辞書内の {len(layered_vocab_dict)} 個の潜在的レイヤーから {num_added_successfully} 個のユニークな単語を解析しました。"
            f"(処理された総単語数: {num_total_words_processed})"
        )

    def _parse_structured_list(self, structured_item_list: List[JsonDict]) -> None:
        """構造化された辞書のリスト（例：GLCAI形式）から語彙を解析します。"""
        self.logger.debug(f"構造化リストから語彙を解析中 ({len(structured_item_list)} アイテム)。")
        num_added_successfully = 0
        num_skipped_items = 0
        for item_data_dict in structured_item_list:
            if not isinstance(item_data_dict, dict):
                num_skipped_items += 1
                self.logger.debug(f"構造化語彙リスト内の非辞書アイテムをスキップ: {item_data_dict}")
                continue

            word_val = item_data_dict.get("word")  # 標準キー
            if not word_val:
                word_val = item_data_dict.get("term")  # フォールバックキー

            if isinstance(word_val, str) and word_val.strip():
                # 他のフィールドを安全に抽出
                categories_val = item_data_dict.get("categories", [])
                # categoriesが空でtagsが存在しリストなら、tagsをcategoriesのフォールバックとして使用
                if not categories_val and isinstance(item_data_dict.get("tags"), list):
                    categories_val = item_data_dict.get("tags", [])
                
                tags_val = item_data_dict.get("tags", []) # tagsは別途取得
                layers_data_val = item_data_dict.get("layers", [])
                example_val = item_data_dict.get("example")
                usage_count_raw_val = item_data_dict.get("usage_count", 0)
                source_val = item_data_dict.get("source")

                # --- データ型の検証と変換 ---
                # categoriesとtagsは文字列リストであることを保証
                final_categories_list = [str(cat) for cat in categories_val if isinstance(cat, (str, int, float))] if isinstance(categories_val, list) else []
                final_tags_list = [str(tag) for tag in tags_val if isinstance(tag, (str, int, float))] if isinstance(tags_val, list) else []
                
                final_example_str = str(example_val) if example_val is not None and not isinstance(example_val, (list, dict)) else None
                final_source_str = str(source_val) if source_val is not None and not isinstance(source_val, (list, dict)) else None

                final_layers_set: Set[str] = set()
                if isinstance(layers_data_val, (list, set)):
                    final_layers_set = {str(layer_item).strip().lower() for layer_item in layers_data_val if isinstance(layer_item, str) and str(layer_item).strip()}
                elif isinstance(layers_data_val, str): # カンマ区切り文字列を処理
                    final_layers_set = {l.strip().lower() for l in layers_data_val.split(',') if l.strip()}
                
                final_usage_count = 0
                try:
                    final_usage_count = int(usage_count_raw_val) if usage_count_raw_val is not None else 0
                    if final_usage_count < 0: final_usage_count = 0 # 負数は0に
                except (ValueError, TypeError):
                    self.logger.debug(f"単語 '{word_val}' の無効なusage_count '{usage_count_raw_val}'。0をデフォルトとします。")

                new_vocab_item = VocabularyItem(
                    word=word_val.strip(),
                    categories=final_categories_list,
                    example=final_example_str,
                    tags=final_tags_list,
                    layers=final_layers_set,
                    usage_count=final_usage_count,
                    source=final_source_str
                )
                original_word_to_check = new_vocab_item.word
                self._add_item(new_vocab_item)
                if original_word_to_check in self.all_words and \
                   any(item.word == original_word_to_check for item in self.items):
                    num_added_successfully += 1
            else: # wordが無効
                num_skipped_items += 1
        self.logger.info(
            f"構造化リストから {num_added_successfully} 個のユニークな単語を解析しました "
            f"({len(structured_item_list)} アイテム処理、{num_skipped_items} スキップ)。"
        )

    def _parse_unknown_dict_format(self, unknown_format_dict: JsonDict) -> None:
        """未知のキーを持つ辞書を解析試行。値が単語リストであると仮定。"""
        self.logger.warning(
            "未知の構造を持つ辞書から語彙の解析を試みます。"
            "キーをカテゴリ/レイヤー、値を単語リストと仮定します。"
        )
        num_added_successfully = 0
        num_total_words_processed = 0
        num_processed_keys = 0

        for key_as_category_or_layer, value_as_word_list in unknown_format_dict.items():
            if isinstance(value_as_word_list, list):
                num_processed_keys += 1
                current_key_num_added = 0
                num_total_words_processed += len(value_as_word_list)
                
                # キーを潜在的なレイヤーまたはカテゴリとして扱う
                potential_layers_set = {key_as_category_or_layer} if key_as_category_or_layer in self.VALID_LAYERS else set()
                potential_categories_list = [key_as_category_or_layer] # キーをカテゴリとして使用

                for word_entry_str in value_as_word_list:
                    if isinstance(word_entry_str, str) and word_entry_str.strip():
                        new_vocab_item = VocabularyItem(
                            word=word_entry_str.strip(),
                            categories=potential_categories_list,
                            layers=potential_layers_set # _add_itemで推論/検証
                        )
                        original_word_to_check = new_vocab_item.word
                        self._add_item(new_vocab_item)
                        if original_word_to_check in self.all_words and \
                           any(item.word == original_word_to_check for item in self.items):
                            current_key_num_added += 1
                if current_key_num_added > 0:
                    self.logger.debug(f"未知のキー '{key_as_category_or_layer}' の下から {current_key_num_added} 個の単語を解析しました。")
                    num_added_successfully += current_key_num_added
            else:
                self.logger.debug(f"未知の辞書形式でキー '{key_as_category_or_layer}' に関連付けられた値がリストではありません。スキップします。")

        if num_added_successfully > 0:
            self.logger.info(
                f"未知の構造の辞書内の {num_processed_keys} 個のキーから {num_added_successfully} 個のユニークな単語を解析しました。"
                f"(処理された総単語数: {num_total_words_processed})"
            )
        else:
            self.logger.error("未知の構造の辞書から単語を一切解析できませんでした。")

    def _infer_categories(self, word_to_infer: str) -> List[str]:
        """キーワードに基づいて潜在的なカテゴリを推論します（単純なヒューリスティック）。"""
        # ETI/ゴシック概念に基づくマッピング
        # NGGSConfigから取得するか、ここで定義。プランではConfigからが示唆。
        # ここでは既存コードのパターンを維持し、config参照はコメントアウト。
        # category_patterns_map = getattr(self.config, 'CATEGORY_INFERENCE_PATTERNS', {})
        category_patterns_map: Dict[str, List[str]] = {
            "境界性": ["窓", "扉", "境", "鏡", "霧", "間", "辺", "狭間", "閾", "仮面", "境界", "門"],
            "両義性": ["美醜", "聖俗", "悦楽", "苦悩", "魅惑", "恐怖", "愛憎", "二重", "パラドックス"],
            "超越侵犯": ["禁忌", "背徳", "浸食", "侵犯", "越境", "変態", "狂気", "異形", "超越", "呪い", "冒涜"],
            "不確定性": ["幻", "影", "夢", "謎", "迷", "曖昧", "朧", "囁き", "不確定", "秘密", "幻惑", "蜃気楼"],
            "内的変容": ["変容", "崩壊", "消失", "変身", "覚醒", "分裂", "記憶", "忘却", "内的", "精神", "回想"],
            "退廃": ["退廃", "頽廃", "衰退", "廃墟", "墓地", "骸骨", "腐臭", "黴", "朽ちる", "古びた", "荒廃"],
            "神秘": ["神秘", "錬金術", "黒魔術", "聖遺物", "儀式", "詠唱", "タロット", "契約", "魂", "星", "秘儀"],
            "自然": ["月光", "霧", "嵐", "沼地", "森", "茨", "薔薇", "蝙蝠", "鴉", "夜", "雨", "風", "雷鳴"]
        }
        inferred_categories_set: Set[str] = set()
        word_lower_case = word_to_infer.lower() # マッチング用に単語を小文字化

        for category_name, patterns_list in category_patterns_map.items():
            if any(pattern_str.lower() in word_lower_case for pattern_str in patterns_list): # パターンも小文字化して比較
                inferred_categories_set.add(category_name)
        
        # 一致がなければデフォルトカテゴリ、あればソートして返す
        return sorted(list(inferred_categories_set)) if inferred_categories_set else ["一般"]

    def _infer_layers(self, word_to_infer: str) -> Set[str]:
        """
        configのキーワードに基づいて潜在的なレイヤーを推論します（単純なヒューリスティック）。
        推論されたレイヤー名のセットを返します。
        """
        inferred_layers_set: Set[str] = set()
        word_lower_case = word_to_infer.lower()
        # NGGSConfigで定義されたキーワードを使用
        layer_keywords_map = self.config.LAYER_KEYWORDS

        for layer_name, keywords_list in layer_keywords_map.items():
            # マネージャーで定義された有効なレイヤーのみを考慮
            if layer_name in self.VALID_LAYERS:
                # キーワード（全体または一部として）が存在するかチェック
                if any(keyword_str.lower() in word_lower_case for keyword_str in keywords_list): # キーワードも小文字化
                    inferred_layers_set.add(layer_name)
        
        return inferred_layers_set # 推論されたレイヤーのセット（空の可能性あり）

# =============================================================================
# Part 6 End: Vocabulary Management (Initialization and Processing)
# =============================================================================
# =============================================================================
# Part 7: Vocabulary Management (Getters and Usage Tracking)
# (Continues VocabularyManager class from Part 6)
# =============================================================================
# Part 1, 2, 3, 4, 6で定義された型やクラス (NGGSConfig, VocabularyItem, JsonDictなど)
# および標準ライブラリが利用可能であることを前提とします。

import logging
import random
from typing import (
    Dict, List, Any, Optional, Union, Set, Final, TypeAlias
)
# from nggs_lite_part1 import NGGSConfig, JsonDict # モジュール分割時
# from nggs_lite_part6 import VocabularyItem, logger_vocab_manager # モジュール分割時

# logger_vocab_manager はPart 6で定義済みと仮定
# logger_vocab_manager = logging.getLogger("NGGS-Lite.VocabularyManager")

class VocabularyManager:
    # --- Methods from Part 6 (__init__, _process_loaded_data, _parse_*, _add_item, _infer_*) ---
    # これらのメソッドはPart 6で定義済みであると仮定し、ここでは再掲しません。
    # 完全なスクリプトでは、これらがこのクラス定義内に存在します。
    VALID_LAYERS: Final[Set[str]] = {"physical", "sensory", "psychological", "symbolic"}

    def __init__(self, config: NGGSConfig):
        if not isinstance(config, NGGSConfig):
            err_msg = "VocabularyManagerには有効なNGGSConfigインスタンスが必要です。"
            try: logging.getLogger("NGGS-Lite.VocabularyManager").critical(err_msg)
            except Exception: print(f"CRITICAL (VocabularyManager): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg)

        self.config: NGGSConfig = config
        self.logger: logging.Logger = logger_vocab_manager
        self.items: List[VocabularyItem] = []
        self.layered_vocabulary: Dict[str, List[VocabularyItem]] = {
            layer: [] for layer in self.VALID_LAYERS
        }
        self.all_words: Set[str] = set()
        self.loaded_source_description: str = "Unknown"

        self.logger.info("VocabularyManagerを初期化中...")
        load_result: Result[Union[List[str], JsonDict], VocabularyError] = load_vocabulary(self.config)

        if load_result.is_ok:
            vocab_data = load_result.unwrap()
            glcai_path_str = str(self.config.GLCAI_VOCAB_PATH) if self.config.GLCAI_VOCAB_PATH else ""
            default_vocab_path_str = str(self.config.DEFAULT_VOCAB_PATH) if self.config.DEFAULT_VOCAB_PATH else ""
            glcai_full_path = (BASE_DIR / glcai_path_str).resolve() if glcai_path_str and not pathlib.Path(glcai_path_str).is_absolute() else pathlib.Path(glcai_path_str)
            default_full_path = (BASE_DIR / default_vocab_path_str).resolve() if default_vocab_path_str and not pathlib.Path(default_vocab_path_str).is_absolute() else pathlib.Path(default_vocab_path_str)

            if self.config.VOCAB_LOAD_PRIORITY and self.config.VOCAB_LOAD_PRIORITY[0] == 'glcai' and glcai_path_str and glcai_full_path.exists():
                 self.loaded_source_description = f"GLCAI File ({glcai_full_path.name})"
            elif default_vocab_path_str and default_full_path.exists():
                 self.loaded_source_description = f"Default NGGS File ({default_full_path.name})"
            else:
                 self.loaded_source_description = "Internal Default Vocabulary"
            self._process_loaded_data(vocab_data, self.loaded_source_description)
        else:
            self.logger.error(f"VocabularyManagerの初期化に失敗しました: {load_result.error}")
            self.loaded_source_description = f"Failed to load ({load_result.error})"

        self.logger.info(
            f"VocabularyManagerは {len(self.items)} 個のユニークなアイテムで初期化されました。ソース: {self.loaded_source_description}"
        )
        for layer_key, items_in_layer_list in self.layered_vocabulary.items():
            self.logger.debug(f"  レイヤー '{layer_key}': {len(items_in_layer_list)} アイテム")

    def _process_loaded_data(self, loaded_data: Union[List[str], JsonDict], loaded_from_source: str) -> None:
        if not loaded_data:
            self.logger.error(f"語彙データが空または無効です ({loaded_from_source} からロード後)。")
            return
        self.logger.info(f"{loaded_from_source} からロードされた語彙データを処理中...")
        try:
            if isinstance(loaded_data, dict) and any(key in self.VALID_LAYERS for key in loaded_data.keys()):
                self._parse_layered_dict(loaded_data)
            elif isinstance(loaded_data, list) and loaded_data:
                first_item_in_list = loaded_data[0]
                if isinstance(first_item_in_list, dict): self._parse_structured_list(loaded_data)
                elif isinstance(first_item_in_list, str): self._parse_flat_list(loaded_data)
                else: self.logger.warning(f"{loaded_from_source} リストアイテム形式認識不可。型: {type(first_item_in_list)}")
            elif isinstance(loaded_data, dict):
                self.logger.warning(f"語彙データ辞書だが階層化形式でない。未知辞書形式試行。ソース: {loaded_from_source}")
                self._parse_unknown_dict_format(loaded_data)
            else: self.logger.error(f"{loaded_from_source} 解析不可。予期せぬトップレベル型: {type(loaded_data)}")
        except Exception as e_parse_struct:
            self.logger.error(f"{loaded_from_source} 語彙データ構造処理中エラー: {e_parse_struct}", exc_info=True)

    def _add_item(self, item_to_add: VocabularyItem) -> None:
        if not item_to_add.word or not isinstance(item_to_add.word, str):
            self.logger.debug(f"無効なVocabularyItemスキップ: word欠落/非文字列 ({item_to_add})"); return
        word_stripped = item_to_add.word.strip()
        if not word_stripped: self.logger.debug("無効なVocabularyItemスキップ: word空。"); return
        if word_stripped in self.all_words: return
        item_to_add.word = word_stripped
        original_input_layers = item_to_add.layers.copy()
        valid_layers_from_input = {layer for layer in item_to_add.layers if layer in self.VALID_LAYERS}
        if not valid_layers_from_input:
            inferred_item_layers = self._infer_layers(word_stripped)
            item_to_add.layers = {layer for layer in inferred_item_layers if layer in self.VALID_LAYERS}
            if original_input_layers:
                invalid_provided_layers = original_input_layers - self.VALID_LAYERS
                if invalid_provided_layers: self.logger.debug(f"単語 '{word_stripped}': 無効入力レイヤー {invalid_provided_layers}。推論レイヤー: {item_to_add.layers}")
        else: item_to_add.layers = valid_layers_from_input
        if not item_to_add.layers:
            default_assigned_layer = "psychological"
            self.logger.debug(f"単語 '{word_stripped}' デフォルトレイヤー '{default_assigned_layer}' 割り当て。")
            item_to_add.layers = {default_assigned_layer}
        if not item_to_add.categories: item_to_add.categories = self._infer_categories(word_stripped)
        self.items.append(item_to_add); self.all_words.add(word_stripped)
        for layer_name_str in item_to_add.layers:
            if layer_name_str in self.layered_vocabulary: self.layered_vocabulary[layer_name_str].append(item_to_add)
            else: self.logger.error(f"内部エラー: アイテム '{word_stripped}' を予期せぬレイヤー '{layer_name_str}' に追加試行。")

    def _parse_flat_list(self, vocab_string_list: List[str]) -> None:
        self.logger.debug(f"フラットリストから語彙解析中 ({len(vocab_string_list)}語)。"); num_added = 0
        for word_entry_str in vocab_string_list:
            if isinstance(word_entry_str, str) and word_entry_str.strip():
                new_item = VocabularyItem(word=word_entry_str.strip()); orig_word = new_item.word; self._add_item(new_item)
                if orig_word in self.all_words and any(i.word == orig_word for i in self.items): num_added += 1
        self.logger.info(f"フラットリストから {num_added} 個のユニーク語彙解析完了。")

    def _parse_layered_dict(self, layered_vocab_dict: JsonDict) -> None:
        self.logger.debug("階層化辞書から語彙解析中。"); num_added = 0; num_processed = 0
        for layer_key, words_list in layered_vocab_dict.items():
            if layer_key in self.VALID_LAYERS:
                if isinstance(words_list, list):
                    num_processed += len(words_list)
                    for word_str in words_list:
                        if isinstance(word_str, str) and word_str.strip():
                            new_item = VocabularyItem(word=word_str.strip(), layers={layer_key}); orig_word = new_item.word; self._add_item(new_item)
                            if orig_word in self.all_words and any(i.word == orig_word for i in self.items): num_added += 1
                else: self.logger.warning(f"レイヤー '{layer_key}' 値非リスト。スキップ。")
            else: self.logger.warning(f"階層化辞書内無効レイヤーキー '{layer_key}' 無視。")
        self.logger.info(f"階層化辞書 {len(layered_vocab_dict)}レイヤーから {num_added} 個ユニーク語彙解析完了。(総処理単語数: {num_processed})")

    def _parse_structured_list(self, structured_item_list: List[JsonDict]) -> None:
        self.logger.debug(f"構造化リストから語彙解析中 ({len(structured_item_list)}アイテム)。"); num_added = 0; num_skipped = 0
        for item_dict in structured_item_list:
            if not isinstance(item_dict, dict): num_skipped += 1; self.logger.debug(f"構造化語彙リスト内非辞書アイテムスキップ: {item_dict}"); continue
            word_val = item_dict.get("word") or item_dict.get("term")
            if isinstance(word_val, str) and word_val.strip():
                cats_val = item_dict.get("categories",[]); tags_val = item_dict.get("tags",[]); layers_val = item_dict.get("layers",[])
                if not cats_val and isinstance(item_dict.get("tags"), list): cats_val = item_dict.get("tags", [])
                ex_val = item_dict.get("example"); usage_raw = item_dict.get("usage_count",0); src_val = item_dict.get("source")
                final_cats = [str(c) for c in cats_val if isinstance(c,(str,int,float))] if isinstance(cats_val,list) else []
                final_tags = [str(t) for t in tags_val if isinstance(t,(str,int,float))] if isinstance(tags_val,list) else []
                final_ex = str(ex_val) if ex_val is not None and not isinstance(ex_val,(list,dict)) else None
                final_src = str(src_val) if src_val is not None and not isinstance(src_val,(list,dict)) else None
                final_layers:Set[str]=set();
                if isinstance(layers_val,(list,set)): final_layers={str(l).strip().lower() for l in layers_val if isinstance(l,str) and str(l).strip()}
                elif isinstance(layers_val,str): final_layers={l.strip().lower() for l in layers_val.split(',') if l.strip()}
                final_usage=0
                try: final_usage=int(usage_raw) if usage_raw is not None else 0; final_usage=max(0,final_usage)
                except(ValueError,TypeError):self.logger.debug(f"単語'{word_val}'無効usage_count'{usage_raw}'。0使用。")
                new_item=VocabularyItem(word=word_val.strip(),categories=final_cats,example=final_ex,tags=final_tags,layers=final_layers,usage_count=final_usage,source=final_src)
                orig_word=new_item.word;self._add_item(new_item)
                if orig_word in self.all_words and any(i.word==orig_word for i in self.items):num_added+=1
            else: num_skipped+=1
        self.logger.info(f"構造化リストから{num_added}個ユニーク語彙解析完了({len(structured_item_list)}アイテム処理,{num_skipped}スキップ)。")

    def _parse_unknown_dict_format(self, unknown_format_dict: JsonDict) -> None:
        self.logger.warning("未知構造辞書から語彙解析試行。キーをカテゴリ/レイヤー、値を単語リストと仮定。"); num_added=0; num_processed_total=0; num_keys_processed=0
        for key_cat_layer, val_word_list in unknown_format_dict.items():
            if isinstance(val_word_list, list):
                num_keys_processed+=1; current_key_added_count=0; num_processed_total+=len(val_word_list)
                potential_layers = {key_cat_layer} if key_cat_layer in self.VALID_LAYERS else set()
                potential_cats = [key_cat_layer]
                for word_str_entry in val_word_list:
                    if isinstance(word_str_entry, str) and word_str_entry.strip():
                        new_item = VocabularyItem(word=word_str_entry.strip(),categories=potential_cats,layers=potential_layers)
                        orig_word=new_item.word; self._add_item(new_item)
                        if orig_word in self.all_words and any(i.word==orig_word for i in self.items): current_key_added_count+=1
                if current_key_added_count > 0: self.logger.debug(f"未知キー'{key_cat_layer}'下から{current_key_added_count}語解析。"); num_added+=current_key_added_count
            else: self.logger.debug(f"未知辞書形式キー'{key_cat_layer}'関連値非リスト。スキップ。")
        if num_added > 0: self.logger.info(f"未知構造辞書{num_keys_processed}キーから{num_added}個ユニーク語彙解析完了(総処理単語数:{num_processed_total})。")
        else: self.logger.error("未知構造辞書から単語解析不可。")

    def _infer_categories(self, word_to_infer: str) -> List[str]:
        category_patterns_map: Dict[str, List[str]] = {
            "境界性": ["窓","扉","境","鏡","霧","間","辺","狭間","閾","仮面","境界","門"],"両義性": ["美醜","聖俗","悦楽","苦悩","魅惑","恐怖","愛憎","二重","パラドックス"],
            "超越侵犯": ["禁忌","背徳","浸食","侵犯","越境","変態","狂気","異形","超越","呪い","冒涜"],"不確定性": ["幻","影","夢","謎","迷","曖昧","朧","囁き","不確定","秘密","幻惑","蜃気楼"],
            "内的変容": ["変容","崩壊","消失","変身","覚醒","分裂","記憶","忘却","内的","精神","回想"],"退廃": ["退廃","頽廃","衰退","廃墟","墓地","骸骨","腐臭","黴","朽ちる","古びた","荒廃"],
            "神秘": ["神秘","錬金術","黒魔術","聖遺物","儀式","詠唱","タロット","契約","魂","星","秘儀"],"自然": ["月光","霧","嵐","沼地","森","茨","薔薇","蝙蝠","鴉","夜","雨","風","雷鳴"]
        }; inferred_cats:Set[str]=set(); word_lc=word_to_infer.lower()
        for cat_name, patterns in category_patterns_map.items():
            if any(p.lower() in word_lc for p in patterns): inferred_cats.add(cat_name)
        return sorted(list(inferred_cats)) if inferred_cats else ["一般"]

    def _infer_layers(self, word_to_infer: str) -> Set[str]:
        inferred_layers:Set[str]=set(); word_lc=word_to_infer.lower(); layer_kw_map=self.config.LAYER_KEYWORDS
        for layer_name, keywords in layer_kw_map.items():
            if layer_name in self.VALID_LAYERS:
                if any(kw.lower() in word_lc for kw in keywords): inferred_layers.add(layer_name)
        return inferred_layers
    # --- End of Methods from Part 6 ---

    # --- Vocabulary Getters and Usage Tracking Methods (Part 7) ---

    def get_vocabulary_for_prompt(
        self,
        layer: Optional[str] = None,
        category: Optional[str] = None,
        tags: Optional[List[str]] = None,
        style: Optional[str] = None, # 例: "colloquial_high", "formal_low"
        count: int = 20,
        avoid_words: Optional[List[str]] = None # 大文字・小文字区別せずに除外
    ) -> str:
        """
        プロンプト用の語彙リストをカンマ区切り文字列で取得します。
        レイヤー、カテゴリ、タグ、スタイルによるフィルタリング、および特定単語の除外をサポートします。
        使用回数に基づいた重み付きサンプリングを行います（使用頻度の低い単語を優先）。
        """
        if not self.items:
            self.logger.warning("語彙リストが空のため、プロンプト用単語を提供できません。")
            return ""

        candidate_items_for_sampling: List[VocabularyItem] = list(self.items) # フィルタリング用にコピー

        # 1. レイヤーフィルター
        if layer and layer in self.VALID_LAYERS:
            candidate_items_for_sampling = [
                item for item in candidate_items_for_sampling if layer in item.layers
            ]
        elif layer: # 無効なレイヤー指定
            self.logger.warning(f"無効なレイヤー '{layer}' がフィルタリングに指定されました。レイヤーフィルターは無視されます。")

        # 2. カテゴリフィルター
        if category:
            candidate_items_for_sampling = [
                item for item in candidate_items_for_sampling if category in item.categories
            ]

        # 3. タグフィルター (指定された全タグを含むアイテムを選択 - AND条件)
        if tags:
            target_tags_set = set(tag.lower() for tag in tags if isinstance(tag, str)) # 比較用に小文字化
            candidate_items_for_sampling = [
                item for item in candidate_items_for_sampling
                if target_tags_set.issubset(set(t.lower() for t in item.tags)) # アイテムのタグも小文字化して比較
            ]
        
        # 4. 除外単語フィルター (大文字・小文字を区別しない)
        if avoid_words:
            avoid_words_set_lower = set(word.lower() for word in avoid_words if isinstance(word, str))
            candidate_items_for_sampling = [
                item for item in candidate_items_for_sampling if item.word.lower() not in avoid_words_set_lower
            ]

        if not candidate_items_for_sampling:
            self.logger.warning(
                f"フィルタリング後、候補となる語彙アイテムが見つかりませんでした。"
                f"(Layer: {layer}, Category: {category}, Tags: {tags}, Avoid: {avoid_words})"
            )
            return ""

        # 5. 重み計算とスタイル調整
        # 使用回数が少ない単語ほど高い重み (例: 1 / (usage_count + 1)^1.5)
        # スタイルに応じて重みを調整 (例: 口語度高スタイルなら口語度高タグを持つ単語の重みを上げる)
        weights_for_sampling: List[float] = []
        for item in candidate_items_for_sampling:
            base_weight = 1.0 / ((item.usage_count + 1.0) ** 1.5) # 使用頻度が低いほど高ウェイト
            style_multiplier = 1.0
            if style and item.tags:
                item_tags_lower = {t.lower() for t in item.tags}
                if style == "colloquial_high" and "口語度高" in item_tags_lower: style_multiplier = 1.8
                elif style == "colloquial_high" and "口語度低" in item_tags_lower: style_multiplier = 0.3
                elif style == "formal_low" and "口語度低" in item_tags_lower: style_multiplier = 1.8 # formal_low = 口語度低
                elif style == "formal_low" and "口語度高" in item_tags_lower: style_multiplier = 0.3
            weights_for_sampling.append(max(0.001, base_weight * style_multiplier)) # 重みが0にならないように

        # 6. 重み付きランダムサンプリング
        num_items_to_sample = min(count, len(candidate_items_for_sampling))
        sampled_items_list: List[VocabularyItem] = []
        
        if num_items_to_sample <= 0: return ""

        try:
            if sum(weights_for_sampling) > 1e-9: # 有効な重みがある場合
                sampled_items_list = random.choices(
                    candidate_items_for_sampling, weights=weights_for_sampling, k=num_items_to_sample
                )
            elif candidate_items_for_sampling: # 全ての重みがほぼゼロの場合 (均一サンプリングにフォールバック)
                self.logger.debug("全候補アイテムの有効な重みがゼロのため、均一サンプリングにフォールバックします。")
                sampled_items_list = random.sample(candidate_items_for_sampling, k=num_items_to_sample)
        except ValueError as e_choices: # k > population などのエラー
            self.logger.warning(f"語彙の重み付きサンプリング中にエラー: {e_choices}。単純サンプリングを試みます。")
            if candidate_items_for_sampling:
                try:
                    # サンプル数が候補数を超えないように調整
                    safe_k = min(num_items_to_sample, len(candidate_items_for_sampling))
                    if safe_k > 0 : sampled_items_list = random.sample(candidate_items_for_sampling, k=safe_k)
                except ValueError as e_sample_fallback: # フォールバックも失敗した場合
                    self.logger.error(f"フォールバックの単純サンプリングも失敗: {e_sample_fallback}。最初の{num_items_to_sample}個を使用します。")
                    sampled_items_list = candidate_items_for_sampling[:num_items_to_sample]
        except Exception as e_sampling_unexpected:
            self.logger.error(f"語彙サンプリング中に予期せぬエラー: {e_sampling_unexpected}", exc_info=True)
            sampled_items_list = candidate_items_for_sampling[:num_items_to_sample] # 最終手段

        sampled_word_strings = [item.word for item in sampled_items_list]
        if sampled_word_strings:
            self.increment_usage(sampled_word_strings) # サンプリングされた単語の使用回数を増やす

        return ", ".join(sampled_word_strings)

    def get_vocabulary_by_eti_category(
        self,
        eti_category_key: str, # "境界性", "両義性" など、NGGSConfig.EXTENDED_ETI_WEIGHTSのキーと一致
        count: int = 5,
        avoid_words: Optional[List[str]] = None
    ) -> str:
        """
        指定されたETIカテゴリに合致する語彙アイテムを取得します。
        ETIカテゴリとVocabularyItem.categoriesまたは.tagsとのマッピングが必要です。
        """
        if not self.items: return ""
        
        # ETIカテゴリとVocabularyItem.categories/tagsのマッピング定義
        # これはNGGSConfigに持つか、より洗練されたマッピングロジックが必要になる可能性あり
        # ここでは、ETIカテゴリ名が直接VocabularyItem.categoriesに含まれると仮定する
        # または、より具体的なマッピングを定義する
        # 例: "境界性" -> category "境界性" or tag "boundary"
        # ユーザーガイド1.2節のETI構成要素全てに対応する必要がある
        eti_category_map_to_vocab_criteria: Dict[str, Dict[str, Union[List[str], Set[str]]]] = {
            "境界性": {"categories": ["境界性", "境界"], "tags": ["boundary", "liminality"]},
            "両義性": {"categories": ["両義性", "矛盾"], "tags": ["ambivalence", "duality"]},
            "超越侵犯": {"categories": ["超越", "侵犯", "禁忌"], "tags": ["transgression", "taboo"]},
            "不確定性": {"categories": ["不確定性", "謎"], "tags": ["uncertainty", "mystery", "ambiguity"]},
            "内的変容": {"categories": ["変容", "内的変化", "精神"], "tags": ["transformation", "internal_change"]},
            "位相移行": {"categories": ["位相", "移行"], "tags": ["phase_shift", "transition"]}, # ETIの構成要素としての「位相移行」
            "主観性": {"categories": ["主観", "内面"], "tags": ["subjectivity", "introspection"]}  # ETIの構成要素としての「主観性」
        }
        
        criteria_for_category = eti_category_map_to_vocab_criteria.get(eti_category_key)
        if not criteria_for_category:
            self.logger.warning(f"ETIカテゴリ '{eti_category_key}' のための語彙検索基準が定義されていません。")
            # フォールバックとして、カテゴリ名自体で検索を試みる
            criteria_for_category = {"categories": [eti_category_key], "tags": [eti_category_key.lower()]}

        target_categories_set = set(cat.lower() for cat in criteria_for_category.get("categories", []) if isinstance(cat, str))
        target_tags_set = set(tag.lower() for tag in criteria_for_category.get("tags", []) if isinstance(tag, str))

        filtered_items_for_eti = [
            item for item in self.items
            if (target_categories_set and any(cat.lower() in target_categories_set for cat in item.categories)) or \
               (target_tags_set and any(tag.lower() in target_tags_set for tag in item.tags))
        ]

        if avoid_words:
            avoid_words_set_lower = set(word.lower() for word in avoid_words if isinstance(word, str))
            filtered_items_for_eti = [
                item for item in filtered_items_for_eti if item.word.lower() not in avoid_words_set_lower
            ]

        num_items_to_sample_eti = min(count, len(filtered_items_for_eti))
        if num_items_to_sample_eti <= 0: return ""
        
        weights_for_eti = [1.0 / ((item.usage_count + 1.0) ** 1.5) for item in filtered_items_for_eti]
        sampled_items_eti: List[VocabularyItem] = []
        try:
            if sum(weights_for_eti) > 1e-9:
                sampled_items_eti = random.choices(filtered_items_for_eti, weights=weights_for_eti, k=num_items_to_sample_eti)
            elif filtered_items_for_eti:
                sampled_items_eti = random.sample(filtered_items_for_eti, k=num_items_to_sample_eti)
        except ValueError: # k > population or other sampling error
            if filtered_items_for_eti:
                safe_k_eti = min(num_items_to_sample_eti, len(filtered_items_for_eti))
                if safe_k_eti > 0: sampled_items_eti = random.sample(filtered_items_for_eti, k=safe_k_eti)
        except Exception as e_eti_sampling:
            self.logger.error(f"ETIカテゴリ '{eti_category_key}' の語彙サンプリング中にエラー: {e_eti_sampling}")
            sampled_items_eti = filtered_items_for_eti[:num_items_to_sample_eti]

        sampled_word_strings_eti = [item.word for item in sampled_items_eti]
        if sampled_word_strings_eti:
            self.increment_usage(sampled_word_strings_eti)
        return ", ".join(sampled_word_strings_eti)

    def get_vocabulary_by_tags(
        self,
        tags_to_match: List[str],
        count: int = 5,
        match_all_tags: bool = True, # TrueならAND検索、FalseならOR検索
        avoid_words: Optional[List[str]] = None
    ) -> str:
        """指定されたタグに合致する語彙アイテムを取得します。"""
        if not tags_to_match or not self.items:
            return ""
        
        target_tags_set_lower = set(tag.lower() for tag in tags_to_match if isinstance(tag, str))
        if not target_tags_set_lower: return "" # 有効なタグがなければ空

        filtered_items_by_tag: List[VocabularyItem] = []
        for item in self.items:
            item_tags_lower_set = {t.lower() for t in item.tags}
            if match_all_tags: # AND条件
                if target_tags_set_lower.issubset(item_tags_lower_set):
                    filtered_items_by_tag.append(item)
            else: # OR条件
                if any(tag_to_find in item_tags_lower_set for tag_to_find in target_tags_set_lower):
                    filtered_items_by_tag.append(item)
        
        if avoid_words:
            avoid_words_set_lower = set(word.lower() for word in avoid_words if isinstance(word, str))
            filtered_items_by_tag = [
                item for item in filtered_items_by_tag if item.word.lower() not in avoid_words_set_lower
            ]

        num_items_to_sample_tags = min(count, len(filtered_items_by_tag))
        if num_items_to_sample_tags <= 0: return ""
        
        weights_for_tags = [1.0 / ((item.usage_count + 1.0) ** 1.5) for item in filtered_items_by_tag]
        sampled_items_tags: List[VocabularyItem] = []
        try:
            if sum(weights_for_tags) > 1e-9:
                sampled_items_tags = random.choices(filtered_items_by_tag, weights=weights_for_tags, k=num_items_to_sample_tags)
            elif filtered_items_by_tag:
                sampled_items_tags = random.sample(filtered_items_by_tag, k=num_items_to_sample_tags)
        except ValueError:
            if filtered_items_by_tag:
                safe_k_tags = min(num_items_to_sample_tags, len(filtered_items_by_tag))
                if safe_k_tags > 0: sampled_items_tags = random.sample(filtered_items_by_tag, k=safe_k_tags)
        except Exception as e_tags_sampling:
            self.logger.error(f"タグ '{tags_to_match}' での語彙サンプリング中にエラー: {e_tags_sampling}")
            sampled_items_tags = filtered_items_by_tag[:num_items_to_sample_tags]

        sampled_word_strings_tags = [item.word for item in sampled_items_tags]
        if sampled_word_strings_tags:
            self.increment_usage(sampled_word_strings_tags)
        return ", ".join(sampled_word_strings_tags)

    def increment_usage(self, words_used: Union[str, List[str]]) -> None:
        """
        指定された単語または単語リストの使用回数をインクリメントします。
        リスト入力に最適化されています。
        """
        words_to_increment_set: Set[str]
        if isinstance(words_used, str):
            words_to_increment_set = {words_used.strip()}
        elif isinstance(words_used, list):
            words_to_increment_set = {word.strip() for word in words_used if isinstance(word, str) and word.strip()}
        else:
            self.logger.warning(f"increment_usageへの無効な入力型: {type(words_used)}。処理をスキップします。")
            return

        if not words_to_increment_set: return # 空セットなら何もしない

        num_incremented = 0
        # パフォーマンスが問題になる場合は、self.itemsを単語をキーとする辞書に変換してルックアップを高速化
        # word_to_item_map = {item.word: item for item in self.items} # __init__で作成しておくなど
        for vocab_item_instance in self.items:
            if vocab_item_instance.word in words_to_increment_set:
                vocab_item_instance.usage_count += 1
                num_incremented += 1
                # 最適化: 見つかった単語をセットから削除し、セットが空になったらループを抜ける
                # words_to_increment_set.remove(vocab_item_instance.word)
                # if not words_to_increment_set: break
                # (ただし、1つの単語がリスト内に複数回現れるケースを考慮する場合は上記最適化は不適切)

        if num_incremented > 0:
            self.logger.debug(f"{num_incremented} 個の単語の使用回数をインクリメントしました。")
        
        # 存在しない単語の使用を試みた場合のログ（詳細デバッグ用、通常は不要）
        # all_known_words_set = {item.word for item in self.items}
        # unknown_words_increment_attempt = words_to_increment_set - all_known_words_set
        # if unknown_words_increment_attempt:
        #     self.logger.debug(f"未知の単語の使用回数をインクリメントしようとしました: {unknown_words_increment_attempt}")

    def has_vocabulary(self) -> bool:
        """語彙リストが読み込まれてアイテムが存在するかどうかを確認します。"""
        return bool(self.items)

# =============================================================================
# Part 7 End: Vocabulary Management (Getters and Usage Tracking)
# =============================================================================
# =============================================================================
# Part 8: Base Evaluator Structure and LLM Evaluator (v1.8 Refined)
# =============================================================================
# Part 1, 2, 3, 4, 5で定義された型やクラス (NGGSConfig, Result, JsonDict,
# ConfigurationError, TemplateError, LLMClient, LLMError, GenerationError,
# JsonParsingError, SafeDict, truncate_text, validate_template など)
# および標準ライブラリが利用可能であることを前提とします。

import logging
import json
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import (
    Dict, List, Any, Optional, Union, Tuple, Type
)

# Part 1からNGGSConfig, JsonDict, ConfigurationErrorをインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# Part 2からResult, NGGSError, LLMError, GenerationError, JsonParsingError, EvaluationErrorをインポート
# from nggs_lite_part2 import Result, NGGSError, LLMError, GenerationError, JsonParsingError, EvaluationError
# Part 3からSafeDict, truncate_textをインポート
# from nggs_lite_part3 import SafeDict, truncate_text
# Part 4からLLMClient, TemplateError, validate_templateをインポート
# from nggs_lite_part4 import LLMClient, TemplateError, validate_template # LLMClientはPart 5で完成

@dataclass
class EvaluationResult:
    """
    様々な評価者からの構造化された評価結果を保持します。
    v1.8: confidenceフィールドを追加。scoresとreasoningが辞書であることを保証。
    """
    # default_factoryを使用して辞書やリストのようなミュータブルな型を初期化
    scores: Dict[str, float] = field(default_factory=dict)  # キー: メトリック名, 値: スコア (0-5)
    reasoning: Dict[str, str] = field(default_factory=dict) # キー: メトリック名, 値: 評価理由テキスト
    analysis: Optional[str] = None  # 評価者からの一般的な分析テキスト
    # 特定の評価者からのオプションの構造化データ
    components: JsonDict = field(default_factory=dict)  # 詳細なサブスコア (例: 主観性評価の構成要素)
    distribution: JsonDict = field(default_factory=dict) # 位相/層の分布
    # ヒューリスティックベース評価の信頼度スコア (0.0-1.0)
    confidence: Dict[str, float] = field(default_factory=dict)

    def __post_init__(self):
        """
        scores, reasoning, components, distribution, confidenceが辞書であることを保証します。
        これはdataclassのdefault_factoryによって既に保証されていますが、
        万が一Noneなどで初期化された場合のフォールバックとして機能します。
        """
        if not isinstance(self.scores, dict):
            self.scores = {}
        if not isinstance(self.reasoning, dict):
            self.reasoning = {}
        if not isinstance(self.components, dict):
            self.components = {}
        if not isinstance(self.distribution, dict):
            self.distribution = {}
        if not isinstance(self.confidence, dict):
            self.confidence = {}

# 全ての評価者のための抽象基底クラス
class BaseEvaluator(ABC):
    """全ての評価者のための抽象基底クラス (v1.8)。"""
    def __init__(self, config: NGGSConfig):
        """
        ベース評価者を初期化します。

        Args:
            config: NGGSConfigオブジェクト。

        Raises:
            ConfigurationError: configが有効なNGGSConfigインスタンスでない場合。
        """
        if not isinstance(config, NGGSConfig):
            # ロガーが完全に設定される前である可能性があるため、基本的なprint/raiseを使用
            err_msg = "BaseEvaluatorには有効なNGGSConfigインスタンスが必要です。"
            try:
                # NGGS-Lite.BaseEvaluatorロガーが利用可能であれば使用
                base_eval_logger = logging.getLogger("NGGS-Lite.BaseEvaluator")
                if base_eval_logger.hasHandlers():
                    base_eval_logger.critical(err_msg)
                else:
                    print(f"CRITICAL (BaseEvaluator Init): {err_msg}", file=sys.stderr)
            except Exception: # ロギング自体でエラーが発生した場合
                print(f"CRITICAL (BaseEvaluator Init - Logging Error): {err_msg}", file=sys.stderr)
            raise ConfigurationError(err_msg) # Part 1で定義されたConfigurationError
        
        self.config: NGGSConfig = config
        # 継承するクラスの名前に基づいてロガー名を取得
        self.logger: logging.Logger = logging.getLogger(f"NGGS-Lite.{self.__class__.__name__}")
        self.logger.debug(f"{self.__class__.__name__} が初期化されました。")

    @abstractmethod
    def evaluate(self, text_to_evaluate: str, base_llm_evaluation_result: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        与えられたテキストを評価者の特定の基準に基づいて評価します。
        サブクラスで実装する必要があります。

        Args:
            text_to_evaluate: 評価するテキスト。
            base_llm_evaluation_result: プライマリLLM評価からの結果を含むオプションのEvaluationResult。
                                         サブクラスはこれをコンテキストとして使用したり、
                                         ベースLLMスコアにアクセスしたりできます。

        Returns:
            スコア、評価理由、および評価者固有の他の分析データを含むEvaluationResultオブジェクト。
        """
        pass  # サブクラスが評価ロジックを提供する必要がある

    def _get_llm_score(
        self,
        base_llm_evaluation_result: Optional[EvaluationResult],
        score_key: str,
        default_score: float = 0.0 # デフォルトスコアを0.0-5.0の範囲内にすることが望ましい
    ) -> float:
        """
        ベースLLM評価結果から数値スコアを安全に取得し、標準の0.0-5.0の範囲にクランプします。

        Args:
            base_llm_evaluation_result: プライマリLLM評価者からのEvaluationResult。
            score_key: 取得する特定のスコアキー (例: "overall_quality")。
            default_score: スコアが見つからないか無効な場合に返すデフォルト値。

        Returns:
            0.0から5.0の間のfloatとしてのスコア、または丸められたデフォルト値。
        """
        score_to_return = default_score # デフォルトで開始

        if base_llm_evaluation_result and isinstance(base_llm_evaluation_result.scores, dict):
            raw_score_value = base_llm_evaluation_result.scores.get(score_key)
            # 取得したスコアが有効な数値であるか確認
            if isinstance(raw_score_value, (int, float)):
                try:
                    # スコアを有効範囲 [0.0, 5.0] にクランプ
                    score_to_return = max(0.0, min(5.0, float(raw_score_value)))
                except (ValueError, TypeError):
                    # isinstanceチェックを通過した場合、このケースは稀だが安全のため
                    self.logger.warning(
                        f"スコアキー '{score_key}' の値は数値でしたがfloat変換に失敗しました。デフォルト値を使用します。"
                    )
                    score_to_return = default_score # 変換エラー時はデフォルトにフォールバック
            elif raw_score_value is not None: # スコアは存在するが数値ではない場合
                self.logger.debug(
                    f"LLMスコアキー '{score_key}' の型が無効です: {type(raw_score_value)}。"
                    f"デフォルト値 {default_score:.1f} を使用します。"
                )
        # スコアを小数点以下2桁に丸めて返す
        return round(score_to_return, 2)

    def _get_llm_reasoning(
        self,
        base_llm_evaluation_result: Optional[EvaluationResult],
        reasoning_key: str
    ) -> Optional[str]:
        """
        ベースLLM評価結果から特定のメトリックの評価理由テキスト（文字列）を安全に取得します。

        Args:
            base_llm_evaluation_result: プライマリLLM評価者からのEvaluationResult。
            reasoning_key: 取得する特定の評価理由キー (例: "overall_quality")。

        Returns:
            評価理由文字列、または見つからないか無効な場合はNone。
        """
        if base_llm_evaluation_result and isinstance(base_llm_evaluation_result.reasoning, dict):
            reason_text = base_llm_evaluation_result.reasoning.get(reasoning_key)
            if isinstance(reason_text, str) and reason_text.strip(): # 空でない文字列であることを保証
                return reason_text.strip()
        return None # 見つからないか無効な場合はNoneを返す


# --- Primary LLM-based Evaluator ---
class Evaluator(BaseEvaluator):
    """
    設定されたLLMを使用して、構造化されたプロンプトに基づいてテキストを評価します。
    LLM応答のJSON解析と基本的な検証を処理します。
    BaseEvaluatorを継承します。(v1.8 Refined Parsing and Error Handling)
    """
    def __init__(self, llm_client: LLMClient, config: NGGSConfig, evaluation_template_str: str):
        """
        LLMベースのEvaluatorを初期化します。

        Args:
            llm_client: 設定済みのLLMClientインスタンス。
            config: NGGSConfigオブジェクト。
            evaluation_template_str: 評価プロンプト用のテンプレート文字列。

        Raises:
            ConfigurationError: llm_clientが無効な場合。
            TemplateError: evaluation_template_strが無効な場合。
        """
        super().__init__(config)  # BaseEvaluatorのinitを呼び出し
        if not isinstance(llm_client, LLMClient): # LLMClientはPart 5で完成
            self.logger.critical("Evaluatorには有効なLLMClientインスタンスが必要です。")
            raise ConfigurationError("Evaluatorには有効なLLMClientインスタンスが必要です。")
        self.llm: LLMClient = llm_client

        # 提供された評価テンプレートを初期化時に検証
        # 少なくとも "generated_text" プレースホルダーを期待
        # validate_templateはPart 4で定義済み
        validation_result_template = validate_template(evaluation_template_str, expected_keys=["generated_text"])
        if validation_result_template.is_err:
            template_validation_error = validation_result_template.error
            self.logger.critical(f"提供されたevaluation_templateが無効です: {template_validation_error}")
            if isinstance(template_validation_error, TemplateError) and template_validation_error.details:
                self.logger.critical(f"テンプレートエラーのコンテキスト: {template_validation_error.details}")
            raise template_validation_error # テンプレートが無効なら初期化を中止

        self.evaluation_template: str = evaluation_template_str

        # テンプレートバージョンを判定 (既知のキーに基づいて簡易チェック)
        # これにより、パーサーが後で正しいキーを期待することを確認
        template_content_lower = self.evaluation_template.lower()
        # Part 4のDEFAULT_EVALUATION_TEMPLATEに合わせたキーで判定
        if all(key in template_content_lower for key in ["eti_overall", "ri_clarity", "layer_balance_quality"]):
            self.template_version: str = "v1.8_refined"
        elif "layer_balance" in template_content_lower and "emotion_arc_quality" in template_content_lower:
            self.template_version = "v1.7_compatible"
        else:
            self.template_version = "Unknown_or_Custom"
        self.logger.info(f"Evaluatorは評価テンプレートバージョン: {self.template_version} を使用して初期化されました。")
        self.call_count: int = 0 # API呼び出し回数

    def evaluate(self, text_to_evaluate: str, base_llm_evaluation_result: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        評価テンプレートに基づいてLLM呼び出しを行い、テキストをスコアリングします。
        BaseEvaluator.evaluateをオーバーライドします。解析と検証を処理します。

        Args:
            text_to_evaluate: 評価するテキスト。
            base_llm_evaluation_result: このプライマリエバリュエータでは使用されません。

        Returns:
            LLMからのスコアと評価理由を含むEvaluationResult、
            または評価失敗時はデフォルト値を含むEvaluationResult。
        """
        self.call_count += 1
        if not text_to_evaluate or not isinstance(text_to_evaluate, str) or not text_to_evaluate.strip():
            self.logger.warning("Evaluator.evaluateが空または無効なテキストで呼び出されました。デフォルトスコアを返します。")
            default_scores, default_reasons = self._get_default_scores_and_reasons()
            return EvaluationResult(scores=default_scores, reasoning=default_reasons, analysis="入力テキストが空または無効です。")

        self.logger.debug(f"LLM評価呼び出し {self.call_count} を開始します...")

        # --- 評価プロンプトの構築 ---
        try:
            # SafeDictを使用して、テキストが誤ってフォーマット類似の括弧を含んでいてもエラーを防ぐ
            # SafeDictはPart 3で定義済み
            evaluation_prompt_str = self.evaluation_template.format_map(SafeDict({"generated_text": text_to_evaluate}))
        except Exception as e_format: # format_map中の予期せぬエラー
            self.logger.critical(f"評価テンプレートのフォーマットが予期せず失敗しました: {e_format}", exc_info=True)
            default_scores, default_reasons = self._get_default_scores_and_reasons()
            critical_error_analysis = f"致命的エラー: 評価テンプレートのフォーマットエラー: {e_format}"
            return EvaluationResult(scores=default_scores, reasoning=default_reasons, analysis=critical_error_analysis)

        # --- LLMによる評価呼び出し ---
        # 設定された評価用温度を使用
        llm_call_result: Result[str, LLMError] = self.llm.generate(
            evaluation_prompt_str,
            temperature=self.config.EVALUATION_TEMPERATURE
        )

        if llm_call_result.is_err:
            # LLM呼び出し自体が失敗 (ネットワークエラー、リトライ後のAPIエラーなど)
            llm_evaluation_error = llm_call_result.error # LLMErrorインスタンス
            self.logger.error(f"LLM評価呼び出しに失敗しました: {llm_evaluation_error}")
            default_scores, default_reasons = self._get_default_scores_and_reasons()
            analysis_text = f"LLM評価API呼び出し失敗: {llm_evaluation_error}"
            if isinstance(llm_evaluation_error, NGGSError): # NGGSErrorはPart 2で定義
                analysis_text += f" Context: {llm_evaluation_error.get_context()}"
            return EvaluationResult(scores=default_scores, reasoning=default_reasons, analysis=analysis_text)

        llm_response_text = llm_call_result.unwrap() # is_errでなければunwrap可能
        # LLMResponse.to_result()で空応答は既にGenerationErrorとして処理されるはずだが、念のため
        if not llm_response_text or not llm_response_text.strip():
            self.logger.error("LLMが空の評価応答を返しました。")
            default_scores, default_reasons = self._get_default_scores_and_reasons()
            return EvaluationResult(scores=default_scores, reasoning=default_reasons, analysis="LLMが空の応答を返しました。")

        # --- LLM応答の解析 ---
        # _parse_evaluation_responseはJSON抽出と検証を行う
        parsing_result = self._parse_evaluation_response(llm_response_text)

        if parsing_result.is_err:
            json_parsing_error = parsing_result.error # JsonParsingErrorインスタンス
            self.logger.error(f"LLM評価応答のJSON解析に失敗しました: {json_parsing_error}")
            default_scores, default_reasons = self._get_default_scores_and_reasons()
            analysis_text = f"LLM応答のJSON解析失敗: {json_parsing_error}."
            if isinstance(json_parsing_error, NGGSError):
                analysis_text += f" Context: {json_parsing_error.get_context()}"
            # 解析失敗時は、デバッグ用に元の応答の抜粋をanalysisに含める
            # truncate_textはPart 3で定義済み
            analysis_text += f" 元の応答抜粋: {truncate_text(llm_response_text, 200)}"
            return EvaluationResult(scores=default_scores, reasoning=default_reasons, analysis=analysis_text)
        else:
            # 評価データの解析と検証に成功
            validated_evaluation_data = parsing_result.unwrap() # 'scores'と'reasoning'を含む辞書
            self.logger.info("LLM評価と応答解析に成功しました。")
            # 検証済みスコアと評価理由をEvaluationResultで返す
            return EvaluationResult(
                scores=validated_evaluation_data.get("scores", {}), # 存在するはず
                reasoning=validated_evaluation_data.get("reasoning", {}) # 存在するはず
                # analysis, components, distribution, confidenceはこのEvaluatorでは直接設定しない
            )

    def _get_default_scores_and_reasons(self) -> Tuple[Dict[str, float], Dict[str, str]]:
        """評価失敗時に使用するデフォルトスコアと評価理由を提供します。"""
        default_reason_text = "LLM評価失敗または応答解析不可のため、デフォルト値が使用されました。"
        # ユーザーガイド1.2節の全LLM評価項目を網羅
        expected_score_keys_list = [
            "gothic_atmosphere", "stylistic_gravity", "indirect_emotion", "vocabulary_richness",
            "eti_overall", "eti_boundary", "eti_ambivalence", "eti_transgression", "eti_uncertainty", "eti_transformation",
            "ri_clarity", "ri_visual_rhythm", "ri_emotional_flow", "ri_cognitive_load", "ri_interpretive_resonance",
            "subjective_depth", "phase_transition_quality", "colloquial_gothic_blend_quality",
            "layer_balance_quality", "emotion_arc_achievement",
            "overall_quality"
        ]
        # 平均的/低い評価を示す一貫したデフォルトスコア (例: 2.5)
        default_score_for_all_keys = 2.5
        default_scores_map = {key: default_score_for_all_keys for key in expected_score_keys_list}
        default_reasons_map = {key: default_reason_text for key in expected_score_keys_list}
        return default_scores_map, default_reasons_map

    def _parse_evaluation_response(self, llm_response_text: str) -> Result[JsonDict, JsonParsingError]:
        """
        LLMの評価応答（JSONを期待）を解析します。
        様々な潜在的フォーマット（生JSON、マークダウンブロック）を処理します。
        解析後に構造を検証します。(v1.8 Refined)

        Returns:
            'scores'と'reasoning'を含む辞書 (Ok)、またはJsonParsingError (Err)。
        """
        self.logger.debug(f"LLM評価応答からのJSON解析試行 (長さ {len(llm_response_text)})...")
        
        extracted_json_string: Optional[str] = None
        parsed_json_data: Optional[JsonDict] = None
        used_parsing_method: str = "N/A"
        parsing_error_log: List[str] = [] # 解析エラーの詳細を収集

        stripped_llm_response = llm_response_text.strip()

        # 試行1: 応答全体がJSONオブジェクトか確認
        if stripped_llm_response.startswith('{') and stripped_llm_response.endswith('}'):
            used_parsing_method = "応答全体をJSONとして解析"
            try:
                parsed_json_data = json.loads(stripped_llm_response)
                extracted_json_string = stripped_llm_response
            except json.JSONDecodeError as e_json_direct:
                parsing_error_log.append(f"メソッド '{used_parsing_method}' 失敗: {e_json_direct.msg} at pos {e_json_direct.pos}")
                parsed_json_data = None # 失敗時はリセット

        # 試行2: マークダウンのコードブロックからJSONを抽出 (```json ... ```)
        if parsed_json_data is None:
            # DOTALLでブロック内の改行にマッチ、言語タグはオプションで大文字小文字区別なし
            markdown_json_match = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", stripped_llm_response, re.IGNORECASE | re.DOTALL)
            if markdown_json_match:
                extracted_json_string = markdown_json_match.group(1).strip()
                used_parsing_method = "Markdownコードブロックから抽出"
                try:
                    parsed_json_data = json.loads(extracted_json_string)
                except json.JSONDecodeError as e_json_markdown:
                    parsing_error_log.append(f"メソッド '{used_parsing_method}' 失敗: {e_json_markdown.msg} at pos {e_json_markdown.pos}")
                    parsed_json_data = None
                    extracted_json_string = None # 解析失敗時はリセット
        
        # 試行3: "scores"と"reasoning"キーを含む厳密なJSON構造パターンで検索
        # (LLMがJSONの周囲に余分なテキストを追加した場合に有効)
        if parsed_json_data is None:
            # scoresとreasoningキーを含む最も外側のJSONオブジェクトを探す正規表現
            # (LLMの出力が多少変動しても対応できるよう、空白許容度を高める)
            strict_json_pattern_match = re.search(
                r'\{\s*"scores"\s*:\s*\{[\s\S]*?\}\s*,\s*"reasoning"\s*:\s*\{[\s\S]*?\}\s*\}',
                stripped_llm_response, re.DOTALL
            )
            if strict_json_pattern_match:
                extracted_json_string = strict_json_pattern_match.group(0).strip()
                used_parsing_method = "外部JSONオブジェクト('scores'/'reasoning'キー基準)を抽出"
                try:
                    parsed_json_data = json.loads(extracted_json_string)
                except json.JSONDecodeError as e_json_strict:
                    parsing_error_log.append(f"メソッド '{used_parsing_method}' 失敗: {e_json_strict.msg} at pos {e_json_strict.pos}")
                    parsed_json_data = None
                    extracted_json_string = None

        # 試行4: 最初の'{'と最後の'}'の間を部分文字列として抽出 (最終手段)
        if parsed_json_data is None:
            first_brace_index = stripped_llm_response.find('{')
            last_brace_index = stripped_llm_response.rfind('}')
            if first_brace_index != -1 and last_brace_index > first_brace_index:
                extracted_json_string = stripped_llm_response[first_brace_index : last_brace_index + 1]
                used_parsing_method = "最初と最後の'{' '}'間の部分文字列を抽出"
                try:
                    parsed_json_data = json.loads(extracted_json_string)
                except json.JSONDecodeError as e_json_substring:
                    parsing_error_log.append(f"メソッド '{used_parsing_method}' 失敗: {e_json_substring.msg} at pos {e_json_substring.pos}")
                    # オプション: JSON修復ライブラリの試行 (json_repairなど)
                    # try:
                    #     import json_repair # オプション依存
                    #     repaired_json_str = json_repair.repair_json(extracted_json_string)
                    #     parsed_json_data = json.loads(repaired_json_str)
                    #     extracted_json_string = repaired_json_str # 修復された文字列で更新
                    #     self.logger.info("JSON修復ライブラリによりJSON構造の修復に成功しました。")
                    # except ImportError:
                    #     self.logger.debug("json_repairライブラリが見つかりません。JSON修復はスキップされました。")
                    # except Exception as e_repair:
                    #     parsing_error_log.append(f"JSON修復試行失敗: {e_repair}")
                    #     parsed_json_data = None
                    #     extracted_json_string = None
                    # else: # No repair library, or repair failed
                    parsed_json_data = None
                    extracted_json_string = None


        # --- 最終チェックと検証 ---
        if parsed_json_data is None:
            # JsonParsingErrorはPart 2で定義済み
            json_parsing_err = JsonParsingError(
                "LLM応答から有効なJSON構造を見つけられませんでした。",
                details={"parsing_errors": parsing_error_log,
                         "response_preview": truncate_text(stripped_llm_response, 200), # Part 3のtruncate_text
                         "last_attempted_json_candidate": truncate_text(extracted_json_string, 200) if extracted_json_string else "N/A"}
            )
            return Result.fail(json_parsing_err)

        self.logger.info(f"JSON解析成功 (使用メソッド: '{used_parsing_method}')。構造を検証中...")
        # 解析されたデータ構造を検証し、スコアを正規化
        validation_and_structuring_result = self._validate_and_structure_parsed_data(parsed_json_data)

        if validation_and_structuring_result.is_err:
            # 検証失敗、特定のEvaluationErrorを返す
            validation_error_obj = validation_and_structuring_result.error
            if isinstance(validation_error_obj, EvaluationError) and extracted_json_string:
                # デバッグ用に抽出されたJSON文字列をエラー詳細に追加
                validation_error_obj.details["extracted_json_string_for_validation"] = truncate_text(extracted_json_string, 300)
            return Result.fail(validation_error_obj) # type: ignore
        else:
            # 'scores'と'reasoning'を含む検証済み辞書を返す
            return Result.ok(validation_and_structuring_result.unwrap())

    def _validate_and_structure_parsed_data(self, parsed_data_dict: Any) -> Result[JsonDict, EvaluationError]:
        """解析されたJSONデータ構造を検証し、スコア/理由を正規化します。"""
        if not isinstance(parsed_data_dict, dict):
            return Result.fail(EvaluationError(
                "解析された評価データが辞書形式ではありません。",
                details={"parsed_data_type": type(parsed_data_dict).__name__}
            ))

        # 必須のトップレベルキーの確認
        if "scores" not in parsed_data_dict or not isinstance(parsed_data_dict["scores"], dict):
            return Result.fail(EvaluationError(
                "解析データに'scores'辞書が含まれていないか、型が不正です。",
                details={"available_keys": list(parsed_data_dict.keys())}
            ))
        if "reasoning" not in parsed_data_dict or not isinstance(parsed_data_dict["reasoning"], dict):
            # reasoningは欠落していても警告に留め、空辞書を使用
            self.logger.warning("解析データに'reasoning'辞書が含まれていません。評価理由は空として処理を続行します。")
            parsed_data_dict["reasoning"] = {} # reasoningが存在することを保証

        validated_scores_map: Dict[str, float] = {}
        validated_reasons_map: Dict[str, str] = {}
        validation_warning_messages: List[str] = []
        
        # _get_default_scores_and_reasonsから期待されるスコアキーのリストを取得
        # (ユーザーガイド1.2節の全LLM評価項目を網羅するキー)
        expected_score_keys_defaults_map, _ = self._get_default_scores_and_reasons()

        raw_scores_from_json = parsed_data_dict["scores"]
        raw_reasons_from_json = parsed_data_dict["reasoning"]

        # 期待されるキーに基づいてスコアを検証
        for score_key_str in expected_score_keys_defaults_map:
            score_value_raw = raw_scores_from_json.get(score_key_str)
            reason_text_raw = raw_reasons_from_json.get(score_key_str, "(LLMからの理由提供なし)") # 理由欠落時のデフォルト

            # --- スコア検証 ---
            # デフォルトスコアをフォールバックとして使用
            final_validated_score = expected_score_keys_defaults_map[score_key_str]
            if score_value_raw is None:
                validation_warning_messages.append(
                    f"スコアキー '{score_key_str}' がJSON内に欠落しています。デフォルト値 {final_validated_score:.1f} を使用します。"
                )
            else:
                try:
                    score_as_float = float(score_value_raw)
                    # スコアを有効範囲 [0.0, 5.0] にクランプ
                    clamped_score_value = max(0.0, min(5.0, score_as_float))
                    if abs(clamped_score_value - score_as_float) > 1e-5: # クランプが発生したか確認
                        validation_warning_messages.append(
                            f"スコアキー '{score_key_str}' の値 ({score_as_float}) が範囲外 [0.0, 5.0] でした。"
                            f"{clamped_score_value:.1f} に調整しました。"
                        )
                    final_validated_score = clamped_score_value
                except (ValueError, TypeError):
                    validation_warning_messages.append(
                        f"スコアキー '{score_key_str}' の値 '{score_value_raw}' の形式が無効です。"
                        f"デフォルト値 {final_validated_score:.1f} を使用します。"
                    )
            
            validated_scores_map[score_key_str] = round(final_validated_score, 2) # 検証済みスコアを丸めて保存

            # --- 評価理由検証 ---
            if not isinstance(reason_text_raw, str):
                validation_warning_messages.append(
                    f"評価理由キー '{score_key_str}' の値が文字列ではありません (型: {type(reason_text_raw)})。文字列に変換します。"
                )
                try:
                    reason_text_raw = str(reason_text_raw) # 変換試行
                except Exception:
                    reason_text_raw = "(評価理由の文字列変換不可)" # 変換失敗時のフォールバック
            
            validated_reasons_map[score_key_str] = reason_text_raw.strip() or "(LLMからの理由提供なし)" # 空文字列も処理

        # 検証中に警告があればログ記録
        if validation_warning_messages:
            self.logger.warning("LLM評価JSONの検証中に以下の警告が発生しました:")
            for warning_msg in validation_warning_messages:
                self.logger.warning(f"  - {warning_msg}")

        # scores辞書内の余分なキーをチェック (オプションで警告)
        extra_score_keys_found = set(raw_scores_from_json.keys()) - set(expected_score_keys_defaults_map.keys())
        if extra_score_keys_found:
            self.logger.warning(f"LLM評価の'scores'辞書に予期しないキーが含まれています: {extra_score_keys}")
            # v1.8ではこれらの余分なスコアは無視する方針

        # reasoning辞書内の余分なキーは警告なしで許容

        return Result.ok({"scores": validated_scores_map, "reasoning": validated_reasons_map})

# =============================================================================
# Part 8 End: Base Evaluator Structure and LLM Evaluator
# =============================================================================
# =============================================================================
# Part 9: Evaluator Components (v1.8) - Specific Evaluators (ETI, RI)
# =============================================================================
# Part 1, 2, 3, 8で定義された型やクラス (NGGSConfig, EvaluationResult, BaseEvaluator,
# JsonDict, ConfigurationError, get_metric_display_nameなど)
# および標準ライブラリ (logging, re, sys, statistics, mathなど) が利用可能であることを前提とします。

import logging
import re
import sys # RIEvaluatorのstatisticsインポートチェックで使用
import statistics # RIEvaluatorのヒューリスティック計算で使用
import math # RIEvaluatorのヒューリスティック計算で使用
from typing import (
    Dict, List, Any, Optional, Set, Tuple # Part 1からの型も含む
)
from dataclasses import dataclass, field # EvaluationResultで使用 (Part 8で定義済み)

# Part 1からNGGSConfig, JsonDict, ConfigurationErrorをインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# Part 3からget_metric_display_nameをインポート
# from nggs_lite_part3 import get_metric_display_name
# Part 8からEvaluationResult, BaseEvaluatorをインポート
# from nggs_lite_part8 import EvaluationResult, BaseEvaluator

# NarrativeFlowFrameworkの前方宣言 (Part 10で定義されるクラス)
# 実際のスクリプトでは、NarrativeFlowFrameworkはこれより前に定義されている必要があります。
if 'NarrativeFlowFramework' not in globals():
    # 開発中のリンターエラーを避けるためのプレースホルダー
    # 実行時には実際のクラス定義が必要
    class NarrativeFlowFramework: # type: ignore
        """
        Placeholder for NarrativeFlowFramework.
        This class is expected to be fully defined in Part 10.
        It should have methods like analyze_phase_distribution and analyze_layer_distribution.
        """
        def __init__(self, vocab_manager: Any, config: NGGSConfig): # type: ignore
            self.vocab_manager = vocab_manager
            self.config = config
            self.logger = logging.getLogger("NGGS-Lite.NarrativeFlowFramework.Placeholder")
            self.logger.warning("Using placeholder NarrativeFlowFramework. Full functionality requires Part 10 implementation.")

        def analyze_phase_distribution(self, text: str) -> JsonDict:
            self.logger.debug(f"Placeholder analyze_phase_distribution called for text (len: {len(text)}). Returning empty dict.")
            return {}

        def analyze_layer_distribution(self, text: str) -> JsonDict:
            self.logger.debug(f"Placeholder analyze_layer_distribution called for text (len: {len(text)}). Returning empty dict.")
            return {}
    # pass


class ExtendedETIEvaluator(BaseEvaluator):
    """
    拡張ETIメトリクス（境界性、両義性など）および、
    位相移行と主観性のヒューリスティックに基づいてテキストを評価します。(v1.8 Refined)
    既存のヒューリスティックの検証と調整に焦点を当てます。
    """
    def __init__(self, config: NGGSConfig, narrative_flow_framework: NarrativeFlowFramework):
        """
        ExtendedETIEvaluatorを初期化します。

        Args:
            config: NGGSConfigオブジェクト。
            narrative_flow_framework: 位相分析のためのNarrativeFlowFrameworkのインスタンス。

        Raises:
            ConfigurationError: narrative_flow_frameworkが無効な場合。
        """
        super().__init__(config)
        if not isinstance(narrative_flow_framework, NarrativeFlowFramework):
            self.logger.error(
                "ExtendedETIEvaluatorには有効なNarrativeFlowFrameworkインスタンスが必要です。"
            )
            raise ConfigurationError("ExtendedETIEvaluatorには有効なNarrativeFlowFrameworkインスタンスが必要です。")
        
        self.narrative_flow: NarrativeFlowFramework = narrative_flow_framework
        # 設定からETIの重みを取得
        self.eti_weights: Dict[str, float] = self.config.EXTENDED_ETI_WEIGHTS.copy()
        
        # 重みの合計が約1.0であるか初期化時に検証（オプション）
        current_weights_sum = sum(self.eti_weights.values())
        if abs(current_weights_sum - 1.0) > 0.01: # 浮動小数点誤差を許容
            self.logger.warning(
                f"拡張ETIの重みの合計が1.0ではありません (合計: {current_weights_sum:.3f})。"
                f"スコアは重みだけでは0-5スケールに完全に正規化されない可能性があります。"
            )
            # 必要であればここで重みを正規化するロジックを追加
            # if current_weights_sum > 1e-6: # ゼロ除算を避ける
            #     self.eti_weights = {key: val / current_weights_sum for key, val in self.eti_weights.items()}
            #     self.logger.info("拡張ETIの重みを合計1.0になるように正規化しました。")
        self.logger.info("ExtendedETIEvaluatorがv1.8の設定重みで初期化されました。")

    def evaluate(self, text_to_evaluate: str, base_llm_evaluation_result: Optional[EvaluationResult] = None) -> EvaluationResult:
        """
        ヒューリスティックとベースLLMスコアに基づいてETIスコアを計算します。
        v1.8: 改良されたヒューリスティックを使用し、位相分布を統合します。
        """
        if not text_to_evaluate or not isinstance(text_to_evaluate, str) or not text_to_evaluate.strip():
            self.logger.warning("ExtendedETIEvaluatorが空または無効なテキストを受け取りました。")
            return EvaluationResult(analysis="入力テキストが空または無効です。", scores={"eti_total_calculated": 0.0})

        # --- ETI構成要素スコアの計算 ---
        component_scores: Dict[str, float] = {}
        
        # 1. LLM評価結果からのマッピング (ユーザーガイド1.2節のETI構成要素に対応)
        #    マッピングキーはNGGSConfig.EXTENDED_ETI_WEIGHTSのキーと一致させる
        #    デフォルト値は中間的なスコア (例: 2.5)
        #    LLM評価テンプレート(Part 4)のキーとここで使用するキーを整合させる必要がある
        component_scores["境界性"] = self._get_llm_score(base_llm_evaluation_result, "eti_boundary", default=2.5)
        component_scores["両義性"] = self._get_llm_score(base_llm_evaluation_result, "eti_ambivalence", default=2.5)
        component_scores["超越侵犯"] = self._get_llm_score(base_llm_evaluation_result, "eti_transgression", default=2.5)
        component_scores["不確定性"] = self._get_llm_score(base_llm_evaluation_result, "eti_uncertainty", default=2.5)
        component_scores["内的変容"] = self._get_llm_score(base_llm_evaluation_result, "eti_transformation", default=2.5)

        # 2. ヒューリスティック計算 (v1.8の焦点: これらの検証/調整)
        component_scores["位相移行"] = self._evaluate_phase_transitions_heuristic(text_to_evaluate)
        # ETI内の「主観性」は、Part 10のSubjectiveEvaluatorの簡易版か、その結果を利用
        # ここでは、独立したヒューリスティックとして定義
        component_scores["主観性"] = self._evaluate_subjectivity_heuristic_for_eti(text_to_evaluate)

        # --- 加重合計ETIの計算 ---
        weighted_score_sum = 0.0
        total_applied_weight_value = 0.0
        
        # NGGSConfigで定義された重みに基づいて反復処理
        for component_key, weight_value in self.eti_weights.items():
            score_value = component_scores.get(component_key)
            if score_value is not None and isinstance(score_value, (int, float)):
                clamped_score = max(0.0, min(5.0, score_value)) # スコアを0-5範囲にクランプ
                weighted_score_sum += clamped_score * weight_value
                total_applied_weight_value += weight_value
                component_scores[component_key] = clamped_score # 計算に使用した値を保存
            else:
                self.logger.warning(
                    f"ETI計算: キー '{component_key}' のスコアが見つからないか無効です。"
                    f"デフォルトスコア(2.5)を使用して計算に含めます。"
                )
                default_component_score_val = 2.5
                component_scores[component_key] = default_component_score_val # 完全性のためにデフォルトを保存
                weighted_score_sum += default_component_score_val * weight_value
                total_applied_weight_value += weight_value
        
        # 正規化スコア (ゼロ除算を回避)
        normalized_total_score: float
        if total_applied_weight_value <= 1e-6: # 小さなイプシロン値を使用
            self.logger.error("ETI計算の合計適用重みがゼロまたは負です。正規化できません。")
            normalized_total_score = 0.0
        else:
            # 実際に貢献した構成要素の重みの合計で正規化
            normalized_total_score = weighted_score_sum / total_applied_weight_value

        # 正規化後に最終スコアをクランプ
        final_eti_total_score = max(0.0, min(5.0, normalized_total_score))

        # --- 結果の準備 ---
        final_scores_map: Dict[str, float] = {k: round(v, 2) for k, v in component_scores.items() if isinstance(v, (int, float))}
        final_scores_map["eti_total_calculated"] = round(final_eti_total_score, 2)
        
        # LLMが直接評価したETIスコアも参照用に含める (もしあれば)
        llm_direct_eti_score = self._get_llm_score(base_llm_evaluation_result, "eti_overall", default=-1.0)
        if llm_direct_eti_score >= 0: # 有効なスコアが見つかった場合
            final_scores_map["eti_from_llm"] = round(llm_direct_eti_score, 2)

        analysis_text_content = self._generate_eti_analysis(final_scores_map)
        
        # NarrativeFlowFrameworkを使用して位相分布を推定
        phase_distribution_map: JsonDict = {}
        phase_analysis_confidence_score = 0.0 # デフォルト信頼度
        try:
            phase_data_from_framework = self.narrative_flow.analyze_phase_distribution(text_to_evaluate)
            if isinstance(phase_data_from_framework, dict):
                phase_distribution_map = phase_data_from_framework
                phase_analysis_confidence_score = 1.0 if phase_distribution_map else 0.5 # 単純な信頼度
            else:
                self.logger.warning(f"位相分布分析が辞書でない値を返しました: {type(phase_data_from_framework)}")
                analysis_text_content += " (位相分布分析結果形式不正)"
        except Exception as e_phase_analysis:
            self.logger.error(f"ETI評価中の位相分布分析でエラー: {e_phase_analysis}", exc_info=True)
            analysis_text_content += " (位相分布分析失敗)"

        confidence_scores_map = {
            "位相移行": 0.65, # ヒューリスティック計算の信頼度
            "主観性": 0.65, # ヒューリスティック計算の信頼度
            "phase_distribution_analysis": round(phase_analysis_confidence_score, 2)
        }

        return EvaluationResult(
            scores=final_scores_map,
            reasoning={}, # ETI評価者は構成要素ごとの詳細な理由は生成しない
            analysis=analysis_text_content,
            distribution={"phase": phase_distribution_map}, # 計算された位相分布を保存
            confidence=confidence_scores_map
        )

    def _evaluate_phase_transitions_heuristic(self, text_to_evaluate: str) -> float:
        """位相移行の自然さをヒューリスティックに評価します (v1.8 heuristic)。"""
        # v1.7からのロジックをベースに微調整
        # より高度な分析 (マルコフモデル、文脈考慮など) はv1.9以降
        serif_pattern_regex = r'[「『](?:(?![」』]).)*?[」』]'
        monologue_pattern_regex = r'[（](?:(?![）]).)*?[）]' # 括弧内の思考
        
        # 文末の句読点の後に空白/改行が続く箇所で分割
        sentences_list = re.split(r'(?<=[。！？?!\.])\s+', text_to_evaluate.strip())
        sentences_list = [s for s in sentences_list if s and s.strip()] # 空の文を除去
        if not sentences_list or len(sentences_list) < 2:
            return 3.0  # 移行を判断するには短すぎる場合は中間スコア

        num_transitions = 0
        last_identified_phase = "other" # デフォルトの非特定フェーズで開始
        phase_counts_map: Dict[str, int] = {"serif": 0, "monologue": 0, "other": 0}

        for sentence_str in sentences_list:
            current_sentence_phase = "other" # デフォルトは「その他」(ナレーション/実況)
            if re.search(serif_pattern_regex, sentence_str):
                current_sentence_phase = "serif"
            elif re.search(monologue_pattern_regex, sentence_str):
                current_sentence_phase = "monologue"
            
            phase_counts_map[current_sentence_phase] = phase_counts_map.get(current_sentence_phase, 0) + 1
            if current_sentence_phase != last_identified_phase:
                num_transitions += 1
            last_identified_phase = current_sentence_phase

        num_phase_types_present = len([phase for phase, count in phase_counts_map.items() if count > 0])
        num_sentences_total = len(sentences_list)
        transition_frequency_val = num_transitions / (num_sentences_total - 1) if num_sentences_total > 1 else 0

        # スコアリングロジック (調整可能)
        diversity_score_comp = min(2.5, num_phase_types_present * 0.85) # 多様性スコア (最大2.5)
        ideal_transition_freq = 0.35 # 理想的な遷移頻度
        freq_deviation_val = abs(transition_frequency_val - ideal_transition_freq)
        frequency_score_comp = max(0.0, 2.5 - (freq_deviation_val * 4.0)) # 頻度スコア (最大2.5)
        
        combined_score = 0.0 + diversity_score_comp + frequency_score_comp
        final_score = max(0.0, min(5.0, combined_score)) # 最終スコアを0-5にクランプ
        return round(final_score, 2)

    def _evaluate_subjectivity_heuristic_for_eti(self, text_to_evaluate: str) -> float:
        """ETI構成要素としての主観性の深さをヒューリスティックに評価します (v1.8 simple heuristic)。"""
        # configからキーワードリストを使用
        first_person_pronouns = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        internal_expression_keywords = self.config.SUBJECTIVE_INNER_KEYWORDS
        
        fp_pronoun_count = sum(text_to_evaluate.count(p) for p in first_person_pronouns)
        internal_keyword_count = sum(text_to_evaluate.lower().count(kw.lower()) for kw in internal_expression_keywords)

        text_length_normalized = max(0.1, len(text_to_evaluate) / 1000.0) # 1000文字あたりの頻度に正規化

        # 頻度スコア計算 (0-5スケールに調整する乗数)
        fp_freq_score = fp_pronoun_count / text_length_normalized
        fp_score_component = max(0.0, min(2.5, 2.5 - abs(fp_freq_score - 10.0) * 0.25)) # 10回/1k文字でピーク

        internal_kw_freq_score = internal_keyword_count / text_length_normalized
        internal_score_component = max(0.0, min(2.5, 2.5 - abs(internal_kw_freq_score - 30.0) * 0.125)) # 30回/1k文字でピーク
        
        combined_score = fp_score_component + internal_score_component
        return round(max(0.0, min(5.0, combined_score)), 2) # 0.0-5.0範囲にクランプして丸める

    def _generate_eti_analysis(self, scores_map: Dict[str, float]) -> str:
        """ETI構成要素スコアに基づいて簡単なテキスト分析を生成します。"""
        analysis_parts_list: List[str] = []
        # eti_weightsのキーに含まれる構成要素スコアのみを対象とする
        component_scores_tuples_list = [
            (key_str, val_float) for key_str, val_float in scores_map.items()
            if key_str in self.eti_weights and isinstance(val_float, (int, float))
        ]
        if not component_scores_tuples_list:
            eti_total_score_val = scores_map.get("eti_total_calculated", scores_map.get("eti_from_llm"))
            eti_total_str_val = f"{eti_total_score_val:.1f}" if eti_total_score_val is not None else "N/A"
            return f"ETI分析データなし。ETI総合: {eti_total_str_val}"

        sorted_component_scores = sorted(component_scores_tuples_list, key=lambda item_tuple: item_tuple[1])
        
        weakest_point = sorted_component_scores[0] if sorted_component_scores else None
        strongest_point = sorted_component_scores[-1] if sorted_component_scores else None

        if strongest_point and strongest_point[1] >= 4.0:
            analysis_parts_list.append(f"強み: {get_metric_display_name(strongest_point[0])}({strongest_point[1]:.1f})")
        if weakest_point and weakest_point[1] < 3.0: # 弱点の閾値を3.0未満とする
            analysis_parts_list.append(f"改善点: {get_metric_display_name(weakest_point[0])}({weakest_point[1]:.1f})")
        elif len(sorted_component_scores) > 1 and sorted_component_scores[1][1] < 3.5: # 2番目に低いものがやや低い場合
            second_weakest_point = sorted_component_scores[1]
            analysis_parts_list.append(f"要確認: {get_metric_display_name(second_weakest_point[0])}({second_weakest_point[1]:.1f})")
        
        eti_total_final_score = scores_map.get("eti_total_calculated", scores_map.get("eti_from_llm", 0.0))
        analysis_parts_list.append(f"ETI総合: {eti_total_final_score:.1f}")
        
        return " ".join(analysis_parts_list) if analysis_parts_list else "ETI要素は概ねバランスが取れています。"


class RIEvaluator(BaseEvaluator):
    """
    可読性指数(RI)メトリクスに基づいてテキストを評価します。(v1.8 Refined)
    明瞭さ、リズム、感情フロー、認知負荷、解釈的余韻をヒューリスティックに評価します。
    v1.8: ヒューリスティック計算とスコアリングカーブを調整。
    """
    def __init__(self, config: NGGSConfig):
        """RIEvaluatorを初期化します。"""
        super().__init__(config)
        # RIの重み (v1.8調整版)
        self.ri_weights: Dict[str, float] = {
            "clarity": 0.30,                # 構造的明瞭性、文長
            "visualRhythm": 0.20,           # 段落構成、改行
            "emotionalFlow": 0.15,          # LLMスコア + キーワードチェックによる一貫性
            "cognitiveLoad": 0.20,          # 文の複雑さ、漢字比率、読点頻度
            "interpretiveResonance": 0.15   # LLMスコア(語彙,雰囲気,ETI) + 象徴的密度
        }
        current_ri_weights_sum = sum(self.ri_weights.values())
        if abs(current_ri_weights_sum - 1.0) > 0.01:
            self.logger.warning(f"RIの重みの合計が1.0ではありません (合計: {current_ri_weights_sum:.3f})")
        self.logger.info("RIEvaluatorがv1.8ロジックで初期化されました。")

    def evaluate(self, text_to_evaluate: str, base_llm_evaluation_result: Optional[EvaluationResult] = None) -> EvaluationResult:
        """テキスト分析とベースLLMスコアに基づいてRIスコアを計算します。"""
        if not text_to_evaluate or not isinstance(text_to_evaluate, str) or not text_to_evaluate.strip():
            self.logger.warning("RIEvaluatorが空または無効なテキストを受け取りました。")
            return EvaluationResult(analysis="入力テキストが空または無効です。", scores={"ri_total_calculated": 0.0})

        # 個々のRI構成要素スコアをヘルパーメソッドで計算 (0-5スケールでクランプ済みを期待)
        component_scores_map: Dict[str, float] = {
            "clarity": self._calc_clarity_score(text_to_evaluate, base_llm_evaluation_result),
            "visualRhythm": self._calc_rhythm_score(text_to_evaluate),
            "emotionalFlow": self._calc_emotion_flow_score(text_to_evaluate, base_llm_evaluation_result),
            "cognitiveLoad": self._calc_cognitive_load_score(text_to_evaluate),
            "interpretiveResonance": self._calc_resonance_score(text_to_evaluate, base_llm_evaluation_result)
        }

        # 加重合計RIの計算
        weighted_ri_sum = 0.0
        total_applied_ri_weight = 0.0
        for component_key_ri, weight_val_ri in self.ri_weights.items():
            score_val_ri = component_scores_map.get(component_key_ri)
            if score_val_ri is not None and isinstance(score_val_ri, (int, float)):
                weighted_ri_sum += score_val_ri * weight_val_ri
                total_applied_ri_weight += weight_val_ri
            else:
                self.logger.warning(f"RI計算: キー '{component_key_ri}' のスコアが見つからないか無効です。デフォルト(2.5)を使用します。")
                default_ri_component_score = 2.5
                component_scores_map[component_key_ri] = default_ri_component_score # 完全性のために保存
                weighted_ri_sum += default_ri_component_score * weight_val_ri
                total_applied_ri_weight += weight_val_ri

        normalized_ri_total: float
        if total_applied_ri_weight <= 1e-6:
            self.logger.error("RI計算の合計適用重みがゼロまたは負です。")
            normalized_ri_total = 0.0
        else:
            normalized_ri_total = weighted_ri_sum / total_applied_ri_weight
        
        final_ri_total_score = max(0.0, min(5.0, normalized_ri_total)) # 最終スコアを0-5にクランプ

        final_scores_map_ri: Dict[str, float] = {
            k: round(v, 2) for k, v in component_scores_map.items() if isinstance(v, (int, float))
        }
        final_scores_map_ri["ri_total_calculated"] = round(final_ri_total_score, 2)

        analysis_text_content_ri = self._generate_ri_analysis(final_scores_map_ri)
        
        confidence_scores_map_ri = {
            "clarity": 0.70, "visualRhythm": 0.75, "emotionalFlow": 0.60,
            "cognitiveLoad": 0.70, "interpretiveResonance": 0.65
        }
        overall_ri_confidence = sum(
            confidence_scores_map_ri.get(k_conf, 0.5) * w_conf for k_conf, w_conf in self.ri_weights.items() if k_conf in confidence_scores_map_ri
        ) / max(0.01, sum(w_conf for k_conf, w_conf in self.ri_weights.items() if k_conf in confidence_scores_map_ri))
        confidence_scores_map_ri["ri_total_calculated"] = round(overall_ri_confidence, 2)

        return EvaluationResult(
            scores=final_scores_map_ri,
            reasoning={}, # RI評価者は主にヒューリスティックを使用
            analysis=analysis_text_content_ri,
            confidence=confidence_scores_map_ri
        )

    # --- RI Calculation Helper Methods (v1.8 Refined) ---
    def _get_sentences(self, text_content: str) -> List[str]:
        """テキストを文に分割します (改良版分割ロジック)。"""
        if not text_content: return []
        # 文末句読点の直後が空白、改行、またはテキスト終端である箇所で分割 (lookbehind使用)
        sentences_found = re.split(r'(?<=[。！？?!\.])(?:[\s\n]|$)+', text_content.strip())
        return [s.strip() for s in sentences_found if s and s.strip()] # 空文字列を除去

    def _get_paragraphs(self, text_content: str) -> List[str]:
        """1つ以上の空行に基づいてテキストを段落に分割します。"""
        if not text_content: return []
        # 1つ以上の改行（間にオプションの空白を許容）で分割
        paragraphs_found = re.split(r'\n\s*\n+', text_content.strip())
        return [p.strip() for p in paragraphs_found if p and p.strip()]

    def _calc_clarity_score(self, text_content: str, base_llm_evaluation_result: Optional[EvaluationResult]) -> float:
        """構造的明瞭性スコアを計算します (v1.8)。LLMスコアとヒューリスティックを組み合わせます。"""
        style_gravity_score = self._get_llm_score(base_llm_evaluation_result, "stylistic_gravity", default=3.0)
        sentences_list = self._get_sentences(text_content)
        if not sentences_list:
            return round(max(0.0, min(5.0, style_gravity_score * 0.5)), 2) # 0-5にクランプ

        sentence_lengths = [len(s) for s in sentences_list]
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0

        length_penalty_val = 0.0
        if avg_sentence_length > 100: length_penalty_val = 4.0
        elif avg_sentence_length > 45: length_penalty_val = (avg_sentence_length - 45) / (100 - 45) * 4.0
        sentence_length_score_comp = 5.0 - length_penalty_val

        variance_score_comp = 3.0 # 平均から開始
        if len(sentence_lengths) >= 2 and 'statistics' in sys.modules and statistics: # statisticsが利用可能か確認
            try:
                mean_val = statistics.mean(sentence_lengths)
                stdev_val = statistics.stdev(sentence_lengths)
                cv_val = stdev_val / mean_val if mean_val > 1e-6 else 0
                if 0.3 <= cv_val <= 0.7: variance_score_comp = min(5.0, 3.5 + (cv_val - 0.3) / 0.4 * 1.5)
                elif cv_val < 0.3: variance_score_comp = max(1.0, 3.5 - (0.3 - cv_val) * 5.0)
                else: variance_score_comp = max(1.0, 3.5 - (cv_val - 0.7) * 3.0)
            except statistics.StatisticsError: self.logger.debug("明瞭度: 文長分散計算のためのデータ不足。")
            except Exception as e_stat_clarity: self.logger.warning(f"明瞭度: 文分散計算エラー: {e_stat_clarity}")
        
        clarity_final_score = (sentence_length_score_comp * 0.60) + (variance_score_comp * 0.30) + (style_gravity_score * 0.10)
        return round(max(0.0, min(5.0, clarity_final_score)), 2)

    def _calc_rhythm_score(self, text_content: str) -> float:
        """段落構成と改行に基づいて視覚的リズムスコアを計算します (v1.8)。"""
        paragraphs_list = self._get_paragraphs(text_content)
        if not paragraphs_list or len(paragraphs_list) < 2: return 2.0 # 貧弱な段落構造の場合は低いデフォルトスコア

        paragraph_lengths = [len(p) for p in paragraphs_list]
        avg_paragraph_length = sum(paragraph_lengths) / len(paragraphs_list) if paragraphs_list else 0

        para_len_penalty_val = 0.0
        if avg_paragraph_length > 600: para_len_penalty_val = min(3.0, (avg_paragraph_length - 600) * 0.005)
        elif avg_paragraph_length < 100: para_len_penalty_val = min(2.5, (100 - avg_paragraph_length) * 0.03)
        paragraph_length_score_comp = 5.0 - para_len_penalty_val

        para_variance_score_comp = 3.0
        if len(paragraph_lengths) >= 2 and 'statistics' in sys.modules and statistics:
            try:
                mean_para_val = statistics.mean(paragraph_lengths)
                stdev_para_val = statistics.stdev(paragraph_lengths)
                cv_para_val = stdev_para_val / mean_para_val if mean_para_val > 1e-6 else 0
                if 0.4 <= cv_para_val <= 0.8: para_variance_score_comp = min(5.0, 3.5 + (cv_para_val - 0.4) / 0.4 * 1.5)
                elif cv_para_val < 0.4: para_variance_score_comp = max(1.0, 3.5 - (0.4 - cv_para_val) * 4.0)
                else: para_variance_score_comp = max(1.0, 3.5 - (cv_para_val - 0.8) * 2.5)
            except statistics.StatisticsError: self.logger.debug("リズム: 段落長分散計算のためのデータ不足。")
            except Exception as e_stat_rhythm: self.logger.warning(f"リズム: 段落分散計算エラー: {e_stat_rhythm}")
        
        rhythm_final_score = (paragraph_length_score_comp * 0.55) + (para_variance_score_comp * 0.45)
        return round(max(0.0, min(5.0, rhythm_final_score)), 2)

    def _calc_emotion_flow_score(self, text_content: str, base_llm_evaluation_result: Optional[EvaluationResult]) -> float:
        """LLMスコアと基本ヒューリスティックを使用して感情フロースコアを計算します (v1.8)。"""
        indirect_emotion_llm_score = self._get_llm_score(base_llm_evaluation_result, "indirect_emotion", default=2.5)
        emotion_arc_llm_score = self._get_llm_score(base_llm_evaluation_result, "emotion_arc_quality", default=2.5)

        emotion_keywords_list = self.config.SUBJECTIVE_INNER_KEYWORDS # 感情に関連するキーワード
        num_hits = sum(text_content.lower().count(kw.lower()) for kw in emotion_keywords_list)
        text_len_normalized = max(0.1, len(text_content) / 1000.0)
        
        # 感情語の密度が非常に低い、または非常に高い場合にペナルティ
        keyword_density = num_hits / text_len_normalized if text_len_normalized > 0 else 0.0
        density_penalty = 0.0
        if keyword_density < 5.0: # 低すぎる場合
            density_penalty = max(0.0, min(1.0, (5.0 - keyword_density) * 0.1))
        elif keyword_density > 60.0: # 高すぎる場合 (例)
            density_penalty = max(0.0, min(1.0, (keyword_density - 60.0) * 0.05))
        
        llm_score_average = (indirect_emotion_llm_score + emotion_arc_llm_score) / 2.0
        # LLMスコアに重きを置き、キーワード密度ペナルティを適用
        emotion_flow_final_score = llm_score_average * (1.0 - density_penalty * 0.2) # ペナルティ影響度を調整
        return round(max(0.0, min(5.0, emotion_flow_final_score)), 2)

    def _calc_cognitive_load_score(self, text_content: str) -> float:
        """複雑性ヒューリスティックに基づいて認知負荷スコアを計算します (v1.8)。"""
        sentences_list = self._get_sentences(text_content)
        if not sentences_list: return 3.0 # 文がなければ中間スコア

        num_total_sentences = len(sentences_list)
        num_total_chars_in_sentences = sum(len(s) for s in sentences_list)
        if num_total_chars_in_sentences == 0: return 3.0

        avg_sentence_len_val = num_total_chars_in_sentences / num_total_sentences if num_total_sentences > 0 else 0
        sentence_len_score_comp = max(0.0, min(5.0, 5.0 - max(0, avg_sentence_len_val - 45.0) * 0.08))

        num_kanji = 0; num_meaningful_chars = 0
        try:
            if 'unicodedata' in sys.modules and unicodedata: # unicodedata利用可能か確認
                for char_code in text_content:
                    category_char = unicodedata.category(char_code)
                    if category_char[0] not in ('P', 'S', 'Z', 'C'): # 句読点、記号、区切り文字、その他(制御文字等)以外
                        num_meaningful_chars += 1
                        # CJK統合漢字ブロックおよび拡張A, CJK互換漢字の範囲で判定
                        if ('\u4e00' <= char_code <= '\u9fff') or \
                           ('\u3400' <= char_code <= '\u4dbf') or \
                           ('\uF900' <= char_code <= '\uFAFF'):
                            num_kanji += 1
            else: # unicodedataが利用できない場合のフォールバック
                self.logger.warning("unicodedataモジュールが見つからないため、漢字比率計算は簡易版になります。")
                meaningful_chars_list = [c for c in text_content if not c.isspace() and c not in "。、！？「」『』（）"]
                num_meaningful_chars = len(meaningful_chars_list)
                num_kanji = sum(1 for c in meaningful_chars_list if '\u4e00' <= c <= '\u9fff')
        except Exception as e_char_analysis:
            self.logger.warning(f"認知負荷: 文字分析中にエラー: {e_char_analysis}")
            num_kanji = 0; num_meaningful_chars = len(text_content) # エラー時は総文字数を分母に

        kanji_ratio_val = num_kanji / num_meaningful_chars if num_meaningful_chars > 0 else 0.0
        complexity_score_comp = max(0.0, min(5.0, 5.0 - abs(kanji_ratio_val - 0.375) * 15.0))

        num_commas = text_content.count('、') + text_content.count(',')
        comma_freq_val = num_commas / num_total_sentences if num_total_sentences > 0 else 0
        comma_score_comp = max(1.0, min(5.0, 4.5 - abs(comma_freq_val - 1.5) * 1.2))

        cognitive_load_final_score = (sentence_len_score_comp * 0.50) + (complexity_score_comp * 0.30) + (comma_score_comp * 0.20)
        return round(max(0.0, min(5.0, cognitive_load_final_score)), 2)

    def _calc_resonance_score(self, text_content: str, base_llm_evaluation_result: Optional[EvaluationResult]) -> float:
        """LLMスコアとヒューリスティックを使用して解釈的余韻スコアを計算します (v1.8)。"""
        vocab_llm_score = self._get_llm_score(base_llm_evaluation_result, "vocabulary_richness", default=2.5)
        atmosphere_llm_score = self._get_llm_score(base_llm_evaluation_result, "gothic_atmosphere", default=2.5)
        eti_llm_score = self._get_llm_score(base_llm_evaluation_result, "eti_overall", default=2.5) # LLMが直接評価したETI

        symbolic_keywords_list = self.config.LAYER_KEYWORDS.get("symbolic", [])
        num_symbolic_hits = sum(text_content.lower().count(kw.lower()) for kw in symbolic_keywords_list)
        text_len_normalized = max(0.1, len(text_content) / 1000.0)
        
        symbolic_density_comp = min(2.5, max(-1.5, (num_symbolic_hits / text_len_normalized - 5.0) * 0.2))
        symbolic_heuristic_score = 2.5 + symbolic_density_comp # 1.0から4.0の範囲を想定

        resonance_final_score = (atmosphere_llm_score * 0.35) + (eti_llm_score * 0.35) + \
                                (symbolic_heuristic_score * 0.15) + (vocab_llm_score * 0.15)
        return round(max(0.0, min(5.0, resonance_final_score)), 2)

    def _generate_ri_analysis(self, scores_map: Dict[str, float]) -> str:
        """RIスコアに基づいて簡単なテキスト分析を生成します。"""
        analysis_parts_list_ri: List[str] = []
        component_scores_tuples_ri = [
            (k_ri, v_ri) for k_ri, v_ri in scores_map.items() if k_ri in self.ri_weights and isinstance(v_ri, (int, float))
        ]
        if not component_scores_tuples_ri:
            ri_total_val_str = scores_map.get("ri_total_calculated", "N/A")
            ri_total_display_str = f"{ri_total_val_str:.1f}" if isinstance(ri_total_val_str, float) else ri_total_val_str
            return f"RI分析データなし。RI総合: {ri_total_display_str}"

        sorted_scores_tuples_ri = sorted(component_scores_tuples_ri, key=lambda item_tuple_ri: item_tuple_ri[1])
        
        weakest_ri_point = sorted_scores_tuples_ri[0] if sorted_scores_tuples_ri else None
        strongest_ri_point = sorted_scores_tuples_ri[-1] if sorted_scores_tuples_ri else None

        if strongest_ri_point and strongest_ri_point[1] >= 4.0:
            analysis_parts_list_ri.append(f"読みやすさ強み: {get_metric_display_name(strongest_ri_point[0])}({strongest_ri_point[1]:.1f})")
        if weakest_ri_point and weakest_ri_point[1] < 3.0:
            analysis_parts_list_ri.append(f"読みやすさ改善点: {get_metric_display_name(weakest_ri_point[0])}({weakest_ri_point[1]:.1f})")
        
        ri_total_final_score = scores_map.get("ri_total_calculated", 0.0)
        analysis_parts_list_ri.append(f"RI総合: {ri_total_final_score:.1f}")
        
        return " ".join(analysis_parts_list_ri) if analysis_parts_list_ri else "読みやすさは概ね良好です。"

# =============================================================================
# Part 9 End: Specific Evaluator Implementations (ETI, RI)
# =============================================================================
# =============================================================================
# Part 10: Evaluator Components (v1.8) - NarrativeFlowFramework & SubjectiveEvaluator
# =============================================================================
# Part 1, 2, 3, 6, 8で定義された型やクラス (NGGSConfig, EvaluationResult, BaseEvaluator,
# VocabularyManager, JsonDict, ConfigurationError, get_metric_display_nameなど)
# および標準ライブラリ (logging, reなど) が利用可能であることを前提とします。

import logging
import re
from typing import (
    Dict, List, Any, Optional, Set, Tuple, Union # Part 1からの型も含む
)
from dataclasses import dataclass, field # EvaluationResultで使用 (Part 8で定義済み)

# Part 1からNGGSConfig, JsonDict, ConfigurationErrorをインポート (またはグローバルスコープで利用可能と仮定)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# Part 3からget_metric_display_nameをインポート
# from nggs_lite_part3 import get_metric_display_name
# Part 6からVocabularyManagerをインポート
# from nggs_lite_part6 import VocabularyManager
# Part 8からEvaluationResult, BaseEvaluatorをインポート
# from nggs_lite_part8 import EvaluationResult, BaseEvaluator

class NarrativeFlowFramework:
    """
    物語の位相と層の分布を分析し、物語構成のテンプレートを生成します。
    v1.8: 物語構造の理解と誘導のためのコアコンポーネント。
    """
    VALID_LAYERS: Final[Set[str]] = {"physical", "sensory", "psychological", "symbolic"} # VocabularyManagerと共通

    def __init__(self, vocab_manager: VocabularyManager, config: NGGSConfig):
        """
        NarrativeFlowFrameworkを初期化します。

        Args:
            vocab_manager: VocabularyManagerのインスタンス。
            config: NGGSConfigオブジェクト。
        """
        if not isinstance(vocab_manager, VocabularyManager): # VocabularyManagerはPart 6で定義
            raise ConfigurationError("NarrativeFlowFrameworkには有効なVocabularyManagerインスタンスが必要です。")
        if not isinstance(config, NGGSConfig): # NGGSConfigはPart 1で定義
            raise ConfigurationError("NarrativeFlowFrameworkには有効なNGGSConfigインスタンスが必要です。")

        self.vocab_manager: VocabularyManager = vocab_manager
        self.config: NGGSConfig = config
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.NarrativeFlowFramework")
        self.logger.info("NarrativeFlowFrameworkが初期化されました。")

        # 位相検出のための正規表現パターン (改善の余地あり)
        # ユーザーガイドの位相: セリフ、実況、モノローグ、ナレーション
        # NGGSConfigの目標位相: serif, live_report, monologue, narration, serif_prime
        self.phase_patterns: Dict[str, str] = {
            "serif": r"[「『](?:(?![」』]).)*?[」』]",  # 対話 (「」または『』で囲まれた部分)
            "monologue": r'[（](?:(?![）]).)*?[）]',  # 思考・独白 (（）で囲まれた部分)
            # "live_report" (実況) と "serif_prime" (変容後セリフ) の検出はより複雑な文脈理解が必要。
            # v1.8では、これらは主に "narration" に分類されるか、
            # あるいは特定のキーワードやマーカーで簡易的に識別する。
            # ここでは、serifとmonologue以外の部分をnarrationとして扱う基本方針。
        }
        # "narration" には "live_report" も含まれると解釈
        self.all_detectable_phases: Set[str] = {"serif", "monologue", "narration"}


    def analyze_phase_distribution(self, text_to_analyze: str) -> JsonDict:
        """
        テキストを分析し、物語の位相の分布を決定します。
        ユーザーガイドの位相（セリフ、実況、モノローグ、ナレーション）と
        NGGSConfig.PHASE_BALANCE_TARGETSのキーに対応します。

        Args:
            text_to_analyze: 分析するテキスト。

        Returns:
            位相名をキーとし、その比率(0.0-1.0)を値とする辞書。
            例: {"serif": 0.25, "monologue": 0.4, "narration": 0.35, "live_report": 0.0, "serif_prime": 0.0}
        """
        if not text_to_analyze or not isinstance(text_to_analyze, str) or not text_to_analyze.strip():
            self.logger.warning("analyze_phase_distribution: 入力テキストが空または無効です。")
            return {phase_key: 0.0 for phase_key in self.config.PHASE_BALANCE_TARGETS}

        total_text_length = len(text_to_analyze)
        if total_text_length == 0:
            return {phase_key: 0.0 for phase_key in self.config.PHASE_BALANCE_TARGETS}

        # 各位相の文字数を記録する辞書
        phase_character_counts: Dict[str, int] = {
            phase_key: 0 for phase_key in self.config.PHASE_BALANCE_TARGETS
        }
        # テキスト全体をカバーするビットマップ (Trueなら既に分類済み)
        character_classified_bitmap = [False] * total_text_length

        # 1. セリフ (serif) とモノローグ (monologue) を優先的に検出
        for phase_key, pattern_str in self.phase_patterns.items():
            if phase_key not in phase_character_counts: continue # 設定された目標位相のみ処理
            try:
                for match_obj in re.finditer(pattern_str, text_to_analyze):
                    start_index, end_index = match_obj.span()
                    # 重複カウントを避けるため、未分類の部分のみを加算
                    current_match_length = 0
                    for i in range(start_index, end_index):
                        if not character_classified_bitmap[i]:
                            character_classified_bitmap[i] = True
                            current_match_length += 1
                    phase_character_counts[phase_key] += current_match_length
            except re.error as e_re:
                self.logger.error(f"位相 '{phase_key}' の正規表現検索中にエラー: {e_re}")


        # 2. "live_report" (実況) と "serif_prime" (変容後セリフ) の簡易検出
        #    v1.8ではキーワードベースの簡易識別を試みる (より高度な識別は将来の課題)
        #    これらのキーワードはNGGSConfigに定義することを推奨
        live_report_keywords = getattr(self.config, "LIVE_REPORT_KEYWORDS", []) # 例: ["実況：", "速報："]
        serif_prime_keywords = getattr(self.config, "SERIF_PRIME_MARKERS", []) # 例: ["(変容)", "[異言]"]

        # live_reportの簡易検出 (キーワードが含まれる文を対象とするなど)
        if "live_report" in phase_character_counts and live_report_keywords:
            # ここでは単純なキーワード出現回数 * 平均文長で近似するなどのヒューリスティックが考えられる
            # より正確には文単位での分類が必要。今回は簡易的に0のまま。
            # (この部分は今後の改善ポイント)
            pass

        # serif_primeの簡易検出
        if "serif_prime" in phase_character_counts and serif_prime_keywords:
            # (この部分も今後の改善ポイント)
            pass
            # serif_primeがserifと重複しないように注意が必要。
            # もしserif_primeがserifの一種として扱われるなら、serifのカウントから一部を移すなど。
            # 今回は、serif_primeは独立して検出されなければ0とする。

        # 3. 残りの未分類部分をナレーション (narration) とする
        #    (実況や変容後セリフが別途カウントされていれば、それらも引く)
        classified_char_count = sum(phase_character_counts.get(pk, 0) for pk in ["serif", "monologue", "live_report", "serif_prime"] if pk in phase_character_counts)
        
        # narrationの文字数は、総文字数から他の分類済み位相の文字数を引いたもの
        # ただし、ビットマップで実際に分類されなかった文字数を集計する方が正確
        narration_char_count_from_bitmap = 0
        for i in range(total_text_length):
            if not character_classified_bitmap[i]:
                narration_char_count_from_bitmap +=1
        
        if "narration" in phase_character_counts:
            phase_character_counts["narration"] = narration_char_count_from_bitmap


        # 比率に正規化
        final_phase_distribution: JsonDict = {}
        for phase_key_target in self.config.PHASE_BALANCE_TARGETS:
            char_count_for_phase = phase_character_counts.get(phase_key_target, 0)
            final_phase_distribution[phase_key_target] = round(char_count_for_phase / total_text_length, 3) if total_text_length > 0 else 0.0
        
        self.logger.debug(f"位相分布分析結果: {final_phase_distribution}")
        return final_phase_distribution

    def analyze_layer_distribution(self, text_to_analyze: str) -> JsonDict:
        """
        四層モデル（物質、感覚、心理、象徴）に基づいてテキストを分析します。
        キーワードの出現回数に基づきます。

        Args:
            text_to_analyze: 分析するテキスト。

        Returns:
            レイヤー名をキーとし、その推定比率を値とする辞書。
        """
        if not text_to_analyze or not isinstance(text_to_analyze, str) or not text_to_analyze.strip():
            self.logger.warning("analyze_layer_distribution: 入力テキストが空または無効です。")
            return {layer_key: 0.0 for layer_key in self.VALID_LAYERS}

        layer_keyword_counts: Dict[str, int] = {layer_key: 0 for layer_key in self.VALID_LAYERS}
        text_content_lower = text_to_analyze.lower() # 検索用に小文字化
        total_keywords_found = 0

        # NGGSConfigからLAYER_KEYWORDSを取得
        layer_keywords_map = self.config.LAYER_KEYWORDS
        if not isinstance(layer_keywords_map, dict):
            self.logger.error("NGGSConfig.LAYER_KEYWORDSが辞書形式ではありません。層分布分析をスキップします。")
            return {layer_key: 0.0 for layer_key in self.VALID_LAYERS}

        for layer_name, keywords_list in layer_keywords_map.items():
            if layer_name in self.VALID_LAYERS: # 有効なレイヤーのみ処理
                if not isinstance(keywords_list, list):
                    self.logger.warning(f"レイヤー '{layer_name}' のキーワードリストが無効です。スキップします。")
                    continue
                for keyword_str in keywords_list:
                    if not isinstance(keyword_str, str): continue # キーワードは文字列のみ
                    # 単純な出現回数カウント。より高度な方法も検討可能。
                    num_hits = text_content_lower.count(keyword_str.lower())
                    if num_hits > 0:
                        layer_keyword_counts[layer_name] += num_hits
                        total_keywords_found += num_hits
            else:
                self.logger.warning(f"LAYER_KEYWORDS内に無効なレイヤーキー '{layer_name}' が含まれています。")
        
        final_layer_distribution: JsonDict = {}
        if total_keywords_found > 0:
            for layer_name_valid in self.VALID_LAYERS: # VALID_LAYERSを基準にキーを生成
                count_for_layer = layer_keyword_counts.get(layer_name_valid, 0)
                final_layer_distribution[layer_name_valid] = round(count_for_layer / total_keywords_found, 3)
        else: # キーワードが一つも見つからなかった場合
            for layer_name_valid in self.VALID_LAYERS:
                final_layer_distribution[layer_name_valid] = 0.0 # 全て0とする
        
        self.logger.debug(f"キーワードに基づく層分布分析結果: {final_layer_distribution}")
        return final_layer_distribution

    def generate_flow_template(
        self,
        theme: str, # 例: "記憶回帰型", "境界侵犯型"
        emotion_arc_str: Optional[str], # 例: "違和感->恐怖->啓示"
        perspective_mode_str: str # 例: "subjective_first_person"
    ) -> str:
        """
        テーマ、感情の弧、視点に基づいて物語構成のプロンプトセクションを生成します。
        ユーザーガイド1.3節「物語構成支援」に対応。
        """
        flow_instruction_parts: List[str] = []
        flow_instruction_parts.append("### 物語構成・流れに関する指示 (NarrativeFlowFramework生成):")

        # テーマに基づく指示
        if theme == "記憶回帰型":
            flow_instruction_parts.append(
                "- **物語構造（記憶回帰型）**: 現在の場面から始まり、過去の重要な記憶や出来事へと回帰し、"
                "最終的に現在の視点に戻り、記憶を通じて得られた気づきや変化が示唆されるように構成してください。"
            )
            flow_instruction_parts.append(
                "  - 過去のシーンは、現在の状況や主人公の心理状態と深く関連付け、対比や共鳴を生み出すように描写してください。"
            )
        elif theme == "境界侵犯型":
            flow_instruction_parts.append(
                "- **物語構造（境界侵犯型）**: 主人公が日常と非日常、あるいは現実と異界の境界に徐々に引き込まれ、"
                "最終的にはその境界を侵犯・超越する過程をスリリングに描写してください。"
            )
            flow_instruction_parts.append(
                "  - 境界を越える瞬間の描写と、その結果として生じる主人公の変容や認識の変化を物語のクライマックスとしてください。"
            )
        # 他のテーマパターンの追加が可能
        else: # 一般的なテーマ指示
            flow_instruction_parts.append(
                f"- **物語構造（テーマ: {theme}）**: 指定されたテーマ「{theme}」を物語の中心に据え、"
                "読者がテーマ性を感じ取れるような明確な起承転結、あるいは序破急の構造を持たせてください。"
            )

        # 感情の弧に基づく指示
        if emotion_arc_str:
            arc_steps_list = [step.strip() for step in emotion_arc_str.split("->") if step.strip()]
            if len(arc_steps_list) > 1:
                flow_instruction_parts.append(
                    f"- **感情の弧**: 「{emotion_arc_str}」に従い、物語を通じて主人公（または主要キャラクター）の感情が"
                    "段階的かつ説得力を持って変化していく様子を丁寧に描写してください。"
                )
                flow_instruction_parts.append(f"  - 開始時の感情状態: {arc_steps_list[0]}")
                if len(arc_steps_list) > 2:
                    middle_emotion_transitions = " -> ".join(arc_steps_list[1:-1])
                    flow_instruction_parts.append(f"  - 中間での感情の推移: {middle_emotion_transitions}")
                flow_instruction_parts.append(f"  - 終結時の感情状態: {arc_steps_list[-1]}")
                flow_instruction_parts.append("  - 各感情変化のきっかけとなる出来事や内的な気づきを明確に描写してください。")
        else:
            flow_instruction_parts.append(
                "- **感情のトーン**: 物語全体を通じて、テーマや雰囲気に合致した一貫性のある感情のトーンを維持するか、"
                "あるいは出来事に応じて自然で説得力のある感情の起伏を描写してください。"
            )

        # 視点モードに基づく指示
        if perspective_mode_str == "subjective_first_person":
            flow_instruction_parts.append(
                "- **視点と語り**: 物語の展開は、主人公（私、僕など）の主観的な体験、知覚、思考、感情、内省を通じて語られるようにしてください。"
                "読者が主人公と一体化して物語を体験できるような没入感を重視してください。"
            )
        elif perspective_mode_str == "perspective_shift": # ロードマップ「視点シフトによる二重性表現」
            flow_instruction_parts.append(
                "- **視点と語り（視点シフト）**: 物語の途中で効果的に視点人物を切り替えるか、あるいは同一人物の異なる時間軸や意識状態からの視点を混在させることで、"
                "多角的な描写、情報の非対称性、あるいは物語の二重性や深みを狙ってください。"
                "視点変更の際は、読者が混乱しないよう、明確な区切りや合図（例：章題、空行、三人称から一人称への移行など）を設けてください。"
            )
        # 他の視点モードへの対応も追加可能

        flow_instruction_parts.append(
            "- **クライマックスと転換点**: 物語のクライマックスや重要な転換点では、描写の密度を高め、サスペンスを盛り上げ、読者の感情移入を最大限に促すように演出してください。"
        )

        return "\n".join(flow_instruction_parts)


class SubjectiveEvaluator(BaseEvaluator):
    """
    主観的ナレーションの質と深さを評価します。(v1.8 Refined)
    ユーザーガイド1.2節の「主観性スコア」の項目に基づき、より詳細なヒューリスティックと構成要素スコアリングを使用します。
    """
    def __init__(self, config: NGGSConfig):
        """SubjectiveEvaluatorを初期化します。"""
        super().__init__(config)
        # キーワードは実行時にself.configからアクセス
        self.logger.info("SubjectiveEvaluatorがv1.8ロジックで初期化されました。")

    def evaluate(self, text_to_evaluate: str, base_llm_evaluation_result: Optional[EvaluationResult] = None) -> EvaluationResult:
        """複数のヒューリスティックに基づいて主観性を評価します。"""
        analysis_metrics_map: JsonDict = {} # 分析に使用した生メトリクスを保存
        component_scores_map: Dict[str, float] = {} # 各構成要素スコア (0-5)
        confidence_map: Dict[str, float] = {} # 各ヒューリスティックの信頼度 (0-1)

        text_length_val = len(text_to_evaluate)
        if text_length_val < 10: # 評価に十分なテキスト長が必要
            self.logger.warning("主観性評価スキップ: 入力テキストが短すぎます。")
            default_scores_map = {"subjective_score": 1.0} # 短いテキストの場合は低いデフォルト
            return EvaluationResult(
                scores=default_scores_map,
                analysis="入力テキストが短すぎるため主観性評価はスキップされました。",
                components={"first_person_score": 0.0, "inner_expression_score": 0.0,
                            "monologue_quality_score": 0.0, "consistency_score": 0.0},
                confidence={"subjective_score": 0.0} # 信頼度も0
            )

        # 計算用にテキスト長を正規化 (1000文字あたり)
        text_length_normalized = max(0.1, text_length_val / 1000.0) # ゼロ除算回避

        # --- 1. 一人称代名詞の使用 (頻度と分布) ---
        fp_score_val, fp_analysis_map = self._evaluate_first_person_usage(text_to_evaluate, text_length_normalized)
        component_scores_map['first_person_score'] = round(fp_score_val, 2)
        analysis_metrics_map.update(fp_analysis_map)
        confidence_map['first_person_score'] = 0.80 # 代名詞カウントは比較的高信頼度

        # --- 2. 内的表現の頻度と多様性 ---
        inner_expr_score_val, inner_expr_analysis_map = self._evaluate_internal_expression(text_to_evaluate, text_length_normalized)
        component_scores_map['inner_expression_score'] = round(inner_expr_score_val, 2)
        analysis_metrics_map.update(inner_expr_analysis_map)
        confidence_map['inner_expression_score'] = 0.70 # キーワードマッチングは中程度の信頼度

        # --- 3. モノローグの質 (ヒューリスティック) ---
        monologue_score_val, monologue_analysis_map = self._evaluate_monologue_quality(text_to_evaluate)
        component_scores_map['monologue_quality_score'] = round(monologue_score_val, 2)
        analysis_metrics_map.update(monologue_analysis_map)
        confidence_map['monologue_quality_score'] = 0.60 # 品質ヒューリスティックはやや低めの信頼度

        # --- 4. 視点の一貫性 (簡易チェックとLLM評価の参照) ---
        # perspective_modeはTextProcessorから渡される想定だが、ここではbase_llm_evalから推測
        # または、この評価者はperspective_modeを引数に取るように変更も検討
        perspective_mode_from_llm_eval = base_llm_evaluation_result.analysis if base_llm_evaluation_result and "perspective_mode" in (base_llm_evaluation_result.analysis or "") else None

        consistency_score_val, consistency_analysis_map = self._evaluate_perspective_consistency(
            text_to_evaluate,
            perspective_mode_from_llm_eval # LLM評価の視点情報も参照
        )
        component_scores_map['consistency_score'] = round(consistency_score_val, 2)
        analysis_metrics_map.update(consistency_analysis_map)
        confidence_map['consistency_score'] = 0.75 # 簡易チェックとLLM参照で中～高信頼度

        # --- 最終的な主観性スコア (加重平均) ---
        # 重みは内的表現と一貫性を重視 (調整可能)
        subjectivity_component_weights: Dict[str, float] = {
            'first_person_score': 0.15,
            'inner_expression_score': 0.35, # 内的表現の多様性と頻度を重視
            'monologue_quality_score': 0.25,  # モノローグの質も重要
            'consistency_score': 0.25      # 視点の一貫性
        }
        weighted_subjectivity_sum = 0.0
        total_applied_subjectivity_weight = 0.0
        component_confidence_values: List[float] = [] # 加重平均のための信頼度リスト

        for component_key, component_weight in subjectivity_component_weights.items():
            component_score = component_scores_map.get(component_key)
            component_confidence = confidence_map.get(component_key, 0.5) # 不明な場合は0.5
            if component_score is not None:
                # スコアを0-5範囲にクランプしてから加重
                clamped_component_score = max(0.0, min(5.0, component_score))
                weighted_subjectivity_sum += clamped_component_score * component_weight
                total_applied_subjectivity_weight += component_weight
                component_confidence_values.append(component_confidence * component_weight) # 信頼度も加重
                component_scores_map[component_key] = clamped_component_score # クランプ値を保存
            else: # スコアがない場合は警告 (通常は発生しないはず)
                self.logger.warning(f"主観性評価: 構成要素 '{component_key}' のスコアがありません。")
                # デフォルトスコア(例:2.5)と重みを加算
                default_comp_score = 2.5
                component_scores_map[component_key] = default_comp_score
                weighted_subjectivity_sum += default_comp_score * component_weight
                total_applied_subjectivity_weight += component_weight
                component_confidence_values.append(0.5 * component_weight) # 平均的信頼度

        final_subjectivity_score: float
        overall_subjectivity_confidence: float
        if total_applied_subjectivity_weight <= 1e-6: # ゼロ除算回避
            self.logger.error("主観性評価の合計重みがゼロです。")
            final_subjectivity_score = 0.0
            overall_subjectivity_confidence = 0.0
        else:
            final_subjectivity_score = weighted_subjectivity_sum / total_applied_subjectivity_weight
            overall_subjectivity_confidence = sum(component_confidence_values) / total_applied_subjectivity_weight if total_applied_subjectivity_weight > 1e-6 else 0.0
        
        final_subjectivity_score = max(0.0, min(5.0, final_subjectivity_score)) # 0-5にクランプ

        # --- 分析テキスト生成 ---
        analysis_text_parts: List[str] = [
            f"一人称使用({component_scores_map.get('first_person_score', 0.0):.1f}, 頻度: {analysis_metrics_map.get('first_person_freq', 0.0):.1f}/1k)",
            f"内的表現({component_scores_map.get('inner_expression_score', 0.0):.1f}, 多様性: {analysis_metrics_map.get('inner_expression_diversity_ratio', 0.0):.2f})",
            f"モノローグ質({component_scores_map.get('monologue_quality_score', 0.0):.1f}, 数: {analysis_metrics_map.get('monologue_count', 0)})",
            f"視点一貫性({component_scores_map.get('consistency_score', 0.0):.1f})"
        ]
        if analysis_metrics_map.get('perspective_consistency_warning'):
            analysis_text_parts.append(f"警告: {analysis_metrics_map['perspective_consistency_warning']}")
        final_analysis_text = f"主観性評価詳細: {', '.join(analysis_text_parts)}."

        final_scores_output_map = {"subjective_score": round(final_subjectivity_score, 2)}
        final_confidence_output_map = confidence_map.copy() # 個別信頼度
        final_confidence_output_map["subjective_score"] = round(overall_subjectivity_confidence, 2) # 総合信頼度

        return EvaluationResult(
            scores=final_scores_output_map, # 主要スコア
            reasoning={"subjective_score": f"総合的な主観描写の質 (構成要素スコア参照)。最終スコア: {final_subjectivity_score:.1f}"},
            analysis=final_analysis_text,
            components=component_scores_map, # 詳細な構成要素スコア
            confidence=final_confidence_output_map # 信頼度スコア
        )

    # --- Subjectivity Evaluator Helper Methods (v1.8 Heuristics) ---
    def _evaluate_first_person_usage(self, text_content: str, text_len_normalized: float) -> Tuple[float, JsonDict]:
        """一人称代名詞の使用頻度と分布を評価します。"""
        fp_pronouns_list = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        # 単語境界を考慮した正規表現でカウント (より正確)
        # re.escapeで特殊文字をエスケープ
        fp_regex_pattern = r'\b(?:' + '|'.join(re.escape(p) for p in fp_pronouns_list) + r')\b'
        num_fp_pronoun_hits = 0
        fp_match_objects: List[Any] = [] # re.Matchオブジェクトを格納
        try:
            fp_match_objects = list(re.finditer(fp_regex_pattern, text_content))
            num_fp_pronoun_hits = len(fp_match_objects)
        except re.error as e_re_fp:
            self.logger.warning(f"一人称代名詞の正規表現検索エラー: {e_re_fp}。単純カウントにフォールバックします。")
            num_fp_pronoun_hits = sum(text_content.count(p) for p in fp_pronouns_list) # フォールバック
        
        fp_pronoun_frequency = num_fp_pronoun_hits / text_len_normalized if text_len_normalized > 0 else 0.0

        # 分布ペナルティ (簡易版): 全出現箇所がテキストの前半または後半に集中している場合にペナルティ
        distribution_penalty_score = 0.0
        text_total_length = len(text_content)
        if num_fp_pronoun_hits >= 4 and text_total_length > 0 and fp_match_objects: # 判断に十分な出現数がある場合
            pronoun_positions_normalized = sorted([match.start() / text_total_length for match in fp_match_objects])
            if all(pos < 0.5 for pos in pronoun_positions_normalized) or \
               all(pos >= 0.5 for pos in pronoun_positions_normalized):
                distribution_penalty_score = 1.5  # 集中に対する大きなペナルティ
        
        # 頻度に基づくスコアリング (目標範囲: ~5-15回/1k文字、ピーク約10回/k)
        target_fp_frequency = 10.0
        ideal_score_at_target_freq = 4.0 # 頻度からの最大スコア要素
        # 目標から離れるほど二次関数的にペナルティ
        frequency_deviation_penalty = ((fp_pronoun_frequency - target_fp_frequency) / max(1.0, target_fp_frequency))**2 * 3.0
        frequency_score_component = max(0.0, ideal_score_at_target_freq - frequency_deviation_penalty)
        
        # 頻度スコアと分布ペナルティを組み合わせ (ベーススコア1.0)
        final_fp_score = max(0.0, min(5.0, frequency_score_component + 1.0 - distribution_penalty_score))
        
        analysis_output_map = {
            'first_person_freq_per_1k': round(fp_pronoun_frequency, 1),
            'first_person_total_count': num_fp_pronoun_hits,
            'first_person_distribution_penalty': round(distribution_penalty_score, 2)
        }
        return final_fp_score, analysis_output_map

    def _evaluate_internal_expression(self, text_content: str, text_len_normalized: float) -> Tuple[float, JsonDict]:
        """内的表現の頻度と多様性を評価します。"""
        internal_keywords_list = self.config.SUBJECTIVE_INNER_KEYWORDS
        text_content_lower = text_content.lower() # キーワードマッチング用に小文字化
        
        internal_keyword_hit_counts: Dict[str, int] = {
            keyword: text_content_lower.count(keyword.lower()) for keyword in internal_keywords_list
        }
        total_internal_keyword_hits = sum(internal_keyword_hit_counts.values())
        internal_keyword_frequency = total_internal_keyword_hits / text_len_normalized if text_len_normalized > 0 else 0.0

        # 多様性: 使用されたユニークなキーワードの割合
        num_unique_keywords_used = len([kw for kw, count in internal_keyword_hit_counts.items() if count > 0])
        total_available_keywords = len(internal_keywords_list)
        diversity_ratio_val = num_unique_keywords_used / total_available_keywords if total_available_keywords > 0 else 0.0
        diversity_score_comp = diversity_ratio_val * 5.0 # 多様性スコアを0-5に線形マッピング

        # 頻度スコア要素 (目標: ~20-45回/k文字、ピーク約30回/k)
        target_internal_freq = 30.0
        # 頻度からの最大スコア3.0、逸脱でペナルティ
        frequency_score_comp = max(0.0, min(3.0, 3.0 - abs(internal_keyword_frequency - target_internal_freq) * 0.1))
        
        # 組み合わせ (重み: 多様性60%, 頻度40%)
        final_inner_expr_score = max(0.0, min(5.0, (diversity_score_comp * 0.6) + (frequency_score_comp * 0.4)))
        
        analysis_output_map = {
            'inner_expression_freq_per_1k': round(internal_keyword_frequency, 1),
            'inner_expression_diversity_ratio': round(diversity_ratio_val, 2),
            'inner_expressions_unique_used_count': num_unique_keywords_used,
            'inner_expression_total_hits': total_internal_keyword_hits
        }
        return final_inner_expr_score, analysis_output_map

    def _evaluate_monologue_quality(self, text_content: str) -> Tuple[float, JsonDict]:
        """モノローグの質をヒューリスティックに評価します（長さ、内容マーカー基準）。"""
        # 日本語の括弧（）内の思考を表す正規表現 (空でない内容を期待)
        monologue_regex_pattern = r'[（](?:(?![）]).)+?[）]'
        # 内省や思考を示唆する可能性のある文末パターンを持つ15文字以上の文
        introspection_regex_pattern = r'[^。！？「『（\)）]{15,}(?:だろうか|かもしれない|気がする|ように思う|なのか|とは|理由は|なぜだろう)[。！？?]'
        
        monologue_segments: List[str] = []
        introspection_segments: List[str] = []
        try:
            monologue_segments = re.findall(monologue_regex_pattern, text_content)
            introspection_segments = re.findall(introspection_regex_pattern, text_content)
        except re.error as e_re_mono:
            self.logger.warning(f"モノローグ/内省パターンの正規表現検索エラー: {e_re_mono}")

        all_identified_monologues = monologue_segments + introspection_segments
        num_total_monologues = len(all_identified_monologues)
        analysis_output_map: JsonDict = {'monologue_count': num_total_monologues, 'avg_monologue_length': 0.0}
        if num_total_monologues == 0:
            return 1.5, analysis_output_map # 内的思考が検出されなければ低いスコア

        monologue_lengths = [len(m_seg) for m_seg in all_identified_monologues]
        avg_monologue_length_val = sum(monologue_lengths) / num_total_monologues if num_total_monologues > 0 else 0.0
        analysis_output_map['avg_monologue_length'] = round(avg_monologue_length_val, 1)
        
        monologue_quality_base_score = 2.0 # やや平均以下から開始

        # 理想的な長さ (例: 25-90文字) に対するボーナス
        if 25 < avg_monologue_length_val < 90:
            monologue_quality_base_score += min(1.5, (avg_monologue_length_val - 25) / (90 - 25) * 1.5)
        else: # 理想範囲外へのペナルティ
            deviation_from_ideal = min(abs(avg_monologue_length_val - 25), abs(avg_monologue_length_val - 90)) if avg_monologue_length_val > 0 else 25
            monologue_quality_base_score -= min(1.0, deviation_from_ideal * 0.02)

        # モノローグ/内省内の内的キーワードに対するボーナス
        internal_keywords = self.config.SUBJECTIVE_INNER_KEYWORDS
        num_internal_keywords_in_monologues = sum(
            any(kw.lower() in m_seg.lower() for kw in internal_keywords) for m_seg in all_identified_monologues
        )
        internal_keyword_ratio = num_internal_keywords_in_monologues / num_total_monologues if num_total_monologues > 0 else 0.0
        monologue_quality_base_score += min(1.0, internal_keyword_ratio * 1.0) # 最大1.0のボーナス

        # 疑問/不確かさマーカーに対するボーナス
        uncertainty_marker_list = ['?', '？', 'だろうか', 'なのか', 'かもしれない', 'なぜ']
        num_uncertainty_markers_in_monologues = sum(
            any(um_marker in m_seg for um_marker in uncertainty_marker_list) for m_seg in all_identified_monologues
        )
        uncertainty_marker_ratio = num_uncertainty_markers_in_monologues / num_total_monologues if num_total_monologues > 0 else 0.0
        monologue_quality_base_score += min(0.5, uncertainty_marker_ratio * 0.5) # 最大0.5のボーナス
        
        final_monologue_score = max(0.0, min(5.0, monologue_quality_base_score))
        return final_monologue_score, analysis_output_map

    def _evaluate_perspective_consistency(self, text_content: str, perspective_mode_hint: Optional[str] = None) -> Tuple[float, JsonDict]:
        """視点の一貫性を評価します（一人称と三人称代名詞の混在を簡易チェック）。"""
        fp_pronouns = self.config.SUBJECTIVE_FIRST_PERSON_PRONOUNS
        tp_pronouns = self.config.SUBJECTIVE_THIRD_PERSON_PRONOUNS
        
        # 単語境界を考慮し、エスケープ処理
        fp_regex = r'\b(?:' + '|'.join(re.escape(p) for p in fp_pronouns) + r')\b'
        tp_regex = r'\b(?:' + '|'.join(re.escape(p) for p in tp_pronouns) + r')\b'
        num_fp_hits = 0; num_tp_hits = 0
        try:
            num_fp_hits = len(re.findall(fp_regex, text_content))
            num_tp_hits = len(re.findall(tp_regex, text_content))
        except re.error as e_re_consistency:
            self.logger.warning(f"視点一貫性チェック中の正規表現エラー: {e_re_consistency}")

        consistency_score = 5.0 # デフォルトは一貫性あり
        warning_message: Optional[str] = None

        # perspective_mode_hintの解釈（例："subjective_first_person", "perspective_shift"など）
        is_intended_first_person = perspective_mode_hint and "first_person" in perspective_mode_hint.lower()
        allow_perspective_shift = perspective_mode_hint and "shift" in perspective_mode_hint.lower()

        if num_fp_hits > 0 and num_tp_hits > 0:
            # 一人称と三人称が混在
            if not allow_perspective_shift: # 視点シフトが意図されていない場合
                consistency_score = 1.0 # 大幅なペナルティ
                warning_message = "一人称と三人称の代名詞が意図せず混在している可能性があります。"
            else: # 視点シフトが意図されている場合
                consistency_score = 3.5 # 混在は許容されるが、自然さは別途評価が必要
                warning_message = "一人称と三人称の代名詞が混在しています（視点シフトモード）。移行の明確さを確認してください。"
        elif is_intended_first_person and num_fp_hits == 0 and num_tp_hits > 0:
            # 一人称視点のはずが三人称のみ出現
            consistency_score = 2.0
            warning_message = "一人称視点が期待されましたが、三人称代名詞のみが検出されました。"
        elif not is_intended_first_person and num_fp_hits > 0 and num_tp_hits == 0:
            # 三人称視点（または客観）のはずが一人称のみ出現
            consistency_score = 2.0
            warning_message = "一人称以外の視点が期待されましたが、一人称代名詞のみが検出されました。"
        elif num_fp_hits == 0 and num_tp_hits == 0:
            # どちらの明確な視点マーカーも見つからない場合
            consistency_score = 3.0 # 中間的なスコア
            # warning_message = "明確な一人称または三人称の代名詞が見つかりませんでした。視点が曖昧な可能性があります。"
        
        analysis_output_map: JsonDict = {
            'first_person_pronoun_count': num_fp_hits,
            'third_person_pronoun_count': num_tp_hits,
        }
        if warning_message:
            analysis_output_map['perspective_consistency_warning'] = warning_message

        return consistency_score, analysis_output_map

# =============================================================================
# Part 10 End: Evaluator Components (NarrativeFlowFramework & SubjectiveEvaluator)
# =============================================================================
# =============================================================================
# Part 11: TextProcessor Initialization and Core Evaluation Helper Methods
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, LLMClient, EvaluationResult, BaseEvaluator, Evaluator,
# ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator, NarrativeFlowFramework,
# VocabularyManager, Result, NGGSError, ConfigurationError, TemplateError,
# JsonDict, validate_template, truncate_text, get_metric_display_name など。

import logging
import math # _calculate_phase_score などで使用
from typing import (
    Dict, List, Any, Optional, Tuple, Callable # Part 1からの型も含む
)

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# from nggs_lite_part2 import Result, NGGSError, LLMError, EvaluationError
# from nggs_lite_part4 import LLMClient, TemplateError, validate_template # LLMClientはPart 5で完成
# from nggs_lite_part6 import VocabularyManager
# from nggs_lite_part8 import EvaluationResult, BaseEvaluator, Evaluator
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part3 import truncate_text, get_metric_display_name


logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")

class TextProcessor:
    """
    テキスト処理ワークフロー（生成、評価、改善）を統括します。
    v1.8では、Gemini連携、改良された評価者、GLCAI語彙取り込み、強化されたフィードバックループ、
    NDGS基本連携パーサー概念、改善されたエラー処理/レポーティングを統合します。
    複数のメトリクスに導かれた反復的改善による品質向上に焦点を当てます。
    """
    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,  # 基本LLM評価者
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        # オプション: 必要に応じてNDGSIntegrationパーサーを注入
        # NDGSIntegrationクラスはPart 15で定義される予定
        ndgs_parser: Optional[Any] = None # 現時点ではAnyを使用、後で具体的な型に
    ):
        """
        TextProcessorを初期化します。

        Args:
            config: NGGSConfigオブジェクト。
            llm_client: 初期化済みLLMClient。
            evaluator: 初期化済み基本Evaluator (LLM呼び出し用)。
            vocab_manager: 初期化済みVocabularyManager。
            narrative_flow: 初期化済みNarrativeFlowFramework。
            eti_evaluator: 初期化済みExtendedETIEvaluator。
            ri_evaluator: 初期化済みRIEvaluator。
            subjective_evaluator: 初期化済みSubjectiveEvaluator。
            generation_template: 初期生成用テンプレート文字列。
            improvement_template: 改善指示生成用テンプレート文字列。
            ndgs_parser: NDGS入力データ解析用のオプションオブジェクト。

        Raises:
            ConfigurationError: 必須コンポーネントが欠落または無効な場合。
            TemplateError: テンプレートが無効な場合。
        """
        self.logger: logging.Logger = logger_text_processor # 専用プロセッサーロガーを使用

        # --- コアコンポーネントの検証と格納 ---
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TextProcessorには有効なNGGSConfigインスタンスが必要です。")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TextProcessorには有効なLLMClientインスタンスが必要です。")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TextProcessorには有効な基本Evaluatorインスタンスが必要です。")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TextProcessorには有効なVocabularyManagerインスタンスが必要です。")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TextProcessorには有効なNarrativeFlowFrameworkインスタンスが必要です。")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TextProcessorには有効なExtendedETIEvaluatorインスタンスが必要です。")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TextProcessorには有効なRIEvaluatorインスタンスが必要です。")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TextProcessorには有効なSubjectiveEvaluatorインスタンスが必要です。")
        
        # NDGSパーサーの検証はオプション、期待されるインターフェースに依存
        if ndgs_parser is not None and not hasattr(ndgs_parser, 'parse'): # 基本的なチェック
            self.logger.warning("提供されたndgs_parserオブジェクトは、直接使用する場合に必要な 'parse' メソッドを欠いている可能性があります。")
            # ndgs_parserの使用方法によっては、ここでConfigurationErrorを送出することも検討

        self.config: NGGSConfig = config
        self.llm: LLMClient = llm_client
        self.evaluator: Evaluator = evaluator
        self.vocab_manager: VocabularyManager = vocab_manager
        self.narrative_flow: NarrativeFlowFramework = narrative_flow
        self.eti_evaluator: ExtendedETIEvaluator = eti_evaluator
        self.ri_evaluator: RIEvaluator = ri_evaluator
        self.subjective_evaluator: SubjectiveEvaluator = subjective_evaluator
        self.ndgs_parser: Optional[Any] = ndgs_parser # NDGSIntegrationインスタンスを想定

        # --- テンプレートの検証と格納 ---
        # 基本検証: 文字列であり空でなく、期待されるプレースホルダーを持つか
        # validate_templateはPart 4で定義
        gen_template_validation_result = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_validation_result.is_err:
            gen_template_error = gen_template_validation_result.error
            self.logger.critical(f"無効なgeneration_template: {gen_template_error}")
            raise gen_template_error # TemplateErrorを送出
        self.generation_template: str = generation_template

        imp_template_validation_result = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_validation_result.is_err:
            imp_template_error = imp_template_validation_result.error
            self.logger.critical(f"無効なimprovement_template: {imp_template_error}")
            raise imp_template_error # TemplateErrorを送出
        self.improvement_template: str = improvement_template

        self.logger.info(
            f"TextProcessorが初期化されました (v{config.VERSION})。エンジン: {self.llm.current_engine}, "
            f"最大ループ数: {config.DEFAULT_MAX_LOOPS}, 改善閾値: {config.DEFAULT_IMPROVEMENT_THRESHOLD:.1f}"
        )

    def _get_default_aggregated_scores(self) -> JsonDict:
        """
        全ての集約スコアキーに対するデフォルト値を持つ辞書を返します。
        評価ステップが致命的に失敗した場合に使用されます。
        """
        default_score_value = 0.0  # または、より中間的な2.5など、望ましい挙動に応じて
        # ユーザーガイド1.2節の主要評価指標を網羅
        return {
            # LLM基本評価からの直接的な主要項目
            "overall_quality": default_score_value,
            "gothic_atmosphere": default_score_value,
            "stylistic_gravity": default_score_value,
            "indirect_emotion": default_score_value,
            "vocabulary_richness": default_score_value,
            "subjective_depth": default_score_value, # LLM評価の主観性
            "phase_transition_quality": default_score_value, # LLM評価の位相移行
            "colloquial_gothic_blend_quality": default_score_value,
            "layer_balance_quality": default_score_value, # LLM評価の層バランス
            "emotion_arc_quality": default_score_value,
            "eti_overall": default_score_value, # LLM評価のETI総合

            # 計算済み/ヒューリスティック評価の主要項目
            "eti_total_calculated": default_score_value, # ETI評価者の計算結果
            "ri_total_calculated": default_score_value,  # RI評価者の計算結果
            "subjective_score": default_score_value, # 主観性評価者の計算結果

            # 派生スコア
            "phase_score": default_score_value,
            "layer_balance_score": default_score_value, # こちらは派生スコアの層バランス
            "emotion_arc_score": default_score_value,   # こちらは派生スコアの感情変容
            "colloquial_score": default_score_value     # こちらは派生スコアの口語/ゴシック
        }

    def _perform_initial_generation(
        self, initial_context_text: str, target_text_length: int, perspective_mode_str: str,
        phase_focus_str: str, colloquial_level_str: str, emotion_arc_str: Optional[str],
        narrative_flow_prompt_str: Optional[str]
    ) -> Result[str, LLMError]: # LLMErrorはPart 2で定義
        """
        主要な _generate_text ヘルパーを使用して初期テキスト生成呼び出しを処理します。
        (現時点ではプレースホルダー、生成ロジックに焦点を当てた後のパートで完全実装)
        """
        self.logger.info("初期テキスト生成を実行中...")
        # これはコアなテキスト生成メソッドを呼び出します。
        # Part 11では、評価ヘルパーに焦点を当てます。生成ロジックはTextProcessor.process内にあります。
        # 現時点では、プレースホルダーを返すか、定義されていれば簡略化された_generate_textを呼び出します。
        # _generate_textは後で実装されると仮定します。
        # return self._generate_text(
        #     input_text_or_context=initial_context_text,
        #     is_improvement=False, # 初期生成
        #     # 初期生成用の他のパラメータ...
        # )
        self.logger.warning("_perform_initial_generationはPart 11ではプレースホルダーです。")
        # 評価フローテスト用にダミーの成功を返す
        # Result, LLMErrorはPart 2で定義
        return Result.ok(f"プレースホルダー初期生成テキスト (コンテキスト先頭: {truncate_text(initial_context_text, 50)})")


    def _perform_full_evaluation(
        self, text_to_evaluate: str, phase_focus_param: str,
        colloquial_level_param: str, emotion_arc_param: Optional[str]
    ) -> Result[JsonDict, EvaluationError]: # EvaluationErrorはPart 2で定義
        """
        全ての評価（LLM、ETI、RI、主観性、分布、派生）を実行し、
        結果を単一の辞書に集約します。

        Args:
            text_to_evaluate: 評価するテキスト。
            phase_focus_param: 位相焦点パラメータ（派生スコアリング用）。
            colloquial_level_param: 口語レベルパラメータ（派生スコアリング用）。
            emotion_arc_param: 感情の弧パラメータ（派生スコアリング用）。

        Returns:
            全ての評価データを含む辞書 (Ok)、
            または致命的な評価ステップ失敗時のEvaluationError (Err)。
        """
        self.logger.info(f"完全評価を実行中 (テキスト長: {len(text_to_evaluate)})...")
        # full_results_dictは、各評価者のEvaluationResultオブジェクトや分析結果を格納
        full_results_dict: JsonDict = {
            "llm_eval_result": None,    # EvaluationResult オブジェクト
            "eti_result": None,         # EvaluationResult オブジェクト
            "ri_result": None,          # EvaluationResult オブジェクト
            "subjective_result": None,  # EvaluationResult オブジェクト
            "phase_distribution": {},   # NarrativeFlowFrameworkからの分析結果
            "layer_distribution": {},   # NarrativeFlowFrameworkからの分析結果
            "derived_scores": {},       # 計算された派生スコアの辞書
            "aggregated_scores": self._get_default_aggregated_scores(), # 最終的な集約スコア
            "errors": [],               # 評価中の回復可能エラーメッセージリスト
            "llm_eval_success_flag": False # 基本LLM評価がスコアを生成したかのフラグ
        }
        # aggregated_scores_mapは、意思決定やレポートに使用される主要スコアを集約
        aggregated_scores_map: Dict[str, float] = self._get_default_aggregated_scores().copy()
        base_llm_eval_context: Optional[EvaluationResult] = None

        # 1. 基本LLM評価 (最も重要)
        base_llm_eval_context = self._run_llm_eval(text_to_evaluate, full_results_dict, aggregated_scores_map)
        # llm_eval_success_flagは、ベースLLM評価が有効なスコアを返したかを示す
        full_results_dict["llm_eval_success_flag"] = (
            base_llm_eval_context is not None and
            isinstance(base_llm_eval_context.scores, dict) and
            "overall_quality" in base_llm_eval_context.scores and # 代表的なキーで確認
            isinstance(base_llm_eval_context.scores["overall_quality"], (int, float))
        )

        # 2. 特定のヒューリスティック評価者
        #    ベースLLM評価の結果をコンテキストとして渡す
        self._run_eti_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)
        self._run_ri_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)
        self._run_subjective_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)

        # 3. 分布分析 (NarrativeFlowFrameworkを使用)
        phase_distribution_data, layer_distribution_data = self._run_distribution_analysis(text_to_evaluate, full_results_dict)

        # 4. 派生スコアの計算と格納
        self._calculate_and_store_derived_scores(
            text_to_evaluate, phase_distribution_data, layer_distribution_data,
            phase_focus_param, colloquial_level_param, emotion_arc_param,
            full_results_dict, aggregated_scores_map
        )

        # 最終的な集約スコアをfull_results_dictに格納 (丸め処理も含む)
        full_results_dict["aggregated_scores"] = {
            key_str: round(val_float, 2) for key_str, val_float in aggregated_scores_map.items()
            if isinstance(val_float, (int, float))
        }

        if full_results_dict["errors"]:
            self.logger.warning(f"完全評価完了。ただし {len(full_results_dict['errors'])} 件の回復可能エラーが発生しました。")
        else:
            self.logger.info("完全評価が正常に完了しました。")
        
        return Result.ok(full_results_dict) # 成功時は評価結果辞書を返す

    def _run_llm_eval(self, text_to_eval: str, results_container: JsonDict, agg_scores_container: Dict[str, float]) -> Optional[EvaluationResult]:
        """基本LLM評価者を実行し、results_containerとagg_scores_containerを更新します。"""
        self.logger.debug("基本LLM評価を実行中...")
        default_llm_scores, _ = self.evaluator._get_default_scores_and_reasons() # Evaluatorのprotectedメソッドアクセス
        evaluation_result_obj: Optional[EvaluationResult] = None
        try:
            evaluation_result_obj = self.evaluator.evaluate(text_to_eval) # EvaluatorはPart 8で定義
            results_container["llm_eval_result"] = evaluation_result_obj # EvaluationResultオブジェクトを格納

            if evaluation_result_obj.scores and "overall_quality" in evaluation_result_obj.scores:
                # 全てのスコアが有効なfloatであることを確認して集約スコアに追加
                valid_llm_scores = {
                    key_str: float(val_float) for key_str, val_float in evaluation_result_obj.scores.items()
                    if isinstance(val_float, (int, float))
                }
                agg_scores_container.update(valid_llm_scores)
                self.logger.info("基本LLM評価が成功しました。")
            else: # スコアがないか、主要キーがない場合
                error_message = evaluation_result_obj.analysis or "基本LLM評価が有効なスコアを生成しませんでした。"
                results_container["errors"].append(f"LLM Eval Error: {error_message}")
                self.logger.error(f"基本LLM評価失敗または無効データ: {error_message}")
                agg_scores_container.update(default_llm_scores) # デフォルトスコアで集約を更新
        except Exception as e_llm_eval_fatal:
            critical_error_message = f"基本LLM評価中に致命的エラー: {e_llm_eval_fatal}"
            # EvaluationErrorはPart 2で定義
            evaluation_fatal_error = EvaluationError(critical_error_message, details={"original_exception": str(e_llm_eval_fatal)})
            self.logger.critical(str(evaluation_fatal_error), exc_info=True)
            results_container["errors"].append(str(evaluation_fatal_error))
            agg_scores_container.update(default_llm_scores) # デフォルトスコアで集約を更新
            # 失敗時もEvaluationResultオブジェクトを生成して格納
            evaluation_result_obj = EvaluationResult(analysis=f"LLM評価エラー: {e_llm_eval_fatal}", scores=default_llm_scores.copy())
            results_container["llm_eval_result"] = evaluation_result_obj
        return evaluation_result_obj

    def _run_eti_eval(self, text_to_eval: str, base_eval_context: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None:
        """ETI評価者を実行し、結果を更新します。"""
        self.logger.debug("ETI評価を実行中...")
        default_eti_score = 2.5 # ETIのデフォルトスコア
        try:
            # eti_evaluatorはPart 9で定義
            eti_evaluation_result = self.eti_evaluator.evaluate(text_to_eval, base_eval_context)
            results_container["eti_result"] = eti_evaluation_result # EvaluationResultオブジェクトを格納
            main_eti_score = eti_evaluation_result.scores.get("eti_total_calculated")
            if isinstance(main_eti_score, (int, float)):
                agg_scores_container["eti_total_calculated"] = float(main_eti_score)
            else:
                agg_scores_container["eti_total_calculated"] = default_eti_score
                warning_msg_eti = "ETI評価が数値スコア 'eti_total_calculated' を返しませんでした。"
                self.logger.warning(warning_msg_eti)
                results_container["errors"].append(f"ETI Eval Error: {warning_msg_eti}")
        except Exception as e_eti_eval:
            error_msg_eti = f"ETI評価中にエラー: {e_eti_eval}"
            self.logger.error(error_msg_eti, exc_info=True)
            results_container["errors"].append(error_msg_eti)
            results_container["eti_result"] = EvaluationResult(analysis=f"ETIエラー: {e_eti_eval}", scores={"eti_total_calculated": default_eti_score})
            agg_scores_container["eti_total_calculated"] = default_eti_score

    def _run_ri_eval(self, text_to_eval: str, base_eval_context: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None:
        """RI評価者を実行し、結果を更新します。"""
        self.logger.debug("RI評価を実行中...")
        default_ri_score = 2.5
        try:
            # ri_evaluatorはPart 9で定義
            ri_evaluation_result = self.ri_evaluator.evaluate(text_to_eval, base_eval_context)
            results_container["ri_result"] = ri_evaluation_result
            main_ri_score = ri_evaluation_result.scores.get("ri_total_calculated")
            if isinstance(main_ri_score, (int, float)):
                agg_scores_container["ri_total_calculated"] = float(main_ri_score)
            else:
                agg_scores_container["ri_total_calculated"] = default_ri_score
                warning_msg_ri = "RI評価が数値スコア 'ri_total_calculated' を返しませんでした。"
                self.logger.warning(warning_msg_ri)
                results_container["errors"].append(f"RI Eval Error: {warning_msg_ri}")
        except Exception as e_ri_eval:
            error_msg_ri = f"RI評価中にエラー: {e_ri_eval}"
            self.logger.error(error_msg_ri, exc_info=True)
            results_container["errors"].append(error_msg_ri)
            results_container["ri_result"] = EvaluationResult(analysis=f"RIエラー: {e_ri_eval}", scores={"ri_total_calculated": default_ri_score})
            agg_scores_container["ri_total_calculated"] = default_ri_score

    def _run_subjective_eval(self, text_to_eval: str, base_eval_context: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None:
        """主観性評価者を実行し、結果を更新します。"""
        self.logger.debug("主観性評価を実行中...")
        default_subjective_score = 2.5
        try:
            # subjective_evaluatorはPart 10で定義
            subjective_evaluation_result = self.subjective_evaluator.evaluate(text_to_eval, base_eval_context)
            results_container["subjective_result"] = subjective_evaluation_result
            main_subjective_score = subjective_evaluation_result.scores.get("subjective_score")
            if isinstance(main_subjective_score, (int, float)):
                agg_scores_container["subjective_score"] = float(main_subjective_score)
            else:
                agg_scores_container["subjective_score"] = default_subjective_score
                warning_msg_subj = "主観性評価が数値スコア 'subjective_score' を返しませんでした。"
                self.logger.warning(warning_msg_subj)
                results_container["errors"].append(f"Subjective Eval Error: {warning_msg_subj}")
        except Exception as e_subj_eval:
            error_msg_subj = f"主観性評価中にエラー: {e_subj_eval}"
            self.logger.error(error_msg_subj, exc_info=True)
            results_container["errors"].append(error_msg_subj)
            results_container["subjective_result"] = EvaluationResult(analysis=f"主観性エラー: {e_subj_eval}", scores={"subjective_score": default_subjective_score})
            agg_scores_container["subjective_score"] = default_subjective_score

    def _run_distribution_analysis(self, text_to_analyze: str, results_container: JsonDict) -> Tuple[JsonDict, JsonDict]:
        """NarrativeFlowFrameworkを使用して位相と層の分布分析を実行します。"""
        self.logger.debug("分布分析（位相・層）を実行中...")
        phase_distribution_data: JsonDict = {}
        layer_distribution_data: JsonDict = {}
        # narrative_flowはPart 10で定義
        if self.narrative_flow:
            try:
                phase_distribution_data = self.narrative_flow.analyze_phase_distribution(text_to_analyze)
                results_container["phase_distribution"] = phase_distribution_data
                self.logger.debug(f"位相分布分析結果: {phase_distribution_data}")
            except Exception as e_phase_dist:
                error_msg_phase = f"位相分布分析中にエラー: {e_phase_dist}"
                self.logger.error(error_msg_phase, exc_info=True)
                results_container["errors"].append(error_msg_phase)
                results_container["phase_distribution"] = {} # エラー時は空
            try:
                layer_distribution_data = self.narrative_flow.analyze_layer_distribution(text_to_analyze)
                results_container["layer_distribution"] = layer_distribution_data
                self.logger.debug(f"層分布分析結果: {layer_distribution_data}")
            except Exception as e_layer_dist:
                error_msg_layer = f"層分布分析中にエラー: {e_layer_dist}"
                self.logger.error(error_msg_layer, exc_info=True)
                results_container["errors"].append(error_msg_layer)
                results_container["layer_distribution"] = {} # エラー時は空
        else: # NarrativeFlowFrameworkが利用不可の場合 (通常は__init__でエラーになるはず)
            error_msg_nff_missing = "NarrativeFlowFrameworkが利用不可のため、分布分析をスキップしました。"
            self.logger.warning(error_msg_nff_missing)
            results_container["errors"].append(error_msg_nff_missing)
            results_container["phase_distribution"] = {}
            results_container["layer_distribution"] = {}
        return phase_distribution_data, layer_distribution_data

    def _calculate_and_store_derived_scores(
        self, text_content: str, phase_distribution_data: JsonDict, layer_distribution_data: JsonDict,
        phase_focus_param: str, colloquial_level_param: str, emotion_arc_param: Optional[str],
        results_container: JsonDict, agg_scores_container: Dict[str, float]
    ) -> None:
        """分析結果に基づいて派生スコア（位相、層、感情、口語）を計算し格納します。"""
        self.logger.debug("派生スコア（位相、層、感情、口語）を計算中...")

        def safe_score_calculation(calculation_func: Callable[..., float], *args: Any, score_name_log: str) -> float:
            """派生スコア計算を安全に実行し、エラー時はデフォルト値を返すヘルパー。"""
            default_derived_score = 2.5 # 派生スコアのデフォルト
            try:
                calculated_score = calculation_func(*args)
                if not isinstance(calculated_score, (int, float)):
                    self.logger.warning(
                        f"{score_name_log} の計算が数値でない値を返しました: {type(calculated_score)}。デフォルト値を使用します。"
                    )
                    results_container["errors"].append(f"{score_name_log} Score Error: Invalid type {type(calculated_score)}")
                    return default_derived_score
                # スコアを0-5範囲にクランプ
                return max(0.0, min(5.0, float(calculated_score)))
            except Exception as e_derived_calc:
                error_msg_derived = f"{score_name_log} スコア計算中にエラー: {e_derived_calc}"
                self.logger.warning(error_msg_derived, exc_info=False) # 頻発する可能性があるのでトレースバックは抑制
                results_container["errors"].append(error_msg_derived)
                return default_derived_score

        phase_score_val = safe_score_calculation(self._calculate_phase_score, phase_distribution_data, phase_focus_param, score_name_log="Phase Balance")
        layer_score_val = safe_score_calculation(self._calculate_layer_balance_score, layer_distribution_data, score_name_log="Layer Balance (Derived)")
        emotion_score_val = safe_score_calculation(self._calculate_emotion_arc_score, text_content, emotion_arc_param, score_name_log="Emotion Arc (Derived)")
        colloquial_score_val = safe_score_calculation(self._calculate_colloquial_score, text_content, colloquial_level_param, score_name_log="Colloquial/Gothic Blend (Derived)")

        # results_container["derived_scores"] に格納
        derived_scores_map = {
            "phase_score": round(phase_score_val, 2),
            "layer_balance_score": round(layer_score_val, 2),
            "emotion_arc_score": round(emotion_score_val, 2),
            "colloquial_score": round(colloquial_score_val, 2)
        }
        results_container["derived_scores"] = derived_scores_map
        
        # agg_scores_container にも主要な派生スコア名で格納
        agg_scores_container["phase_score"] = derived_scores_map["phase_score"]
        agg_scores_container["layer_balance_score"] = derived_scores_map["layer_balance_score"] # LLM評価の "layer_balance_quality" とは別キー
        agg_scores_container["emotion_arc_score"] = derived_scores_map["emotion_arc_score"] # LLM評価の "emotion_arc_quality" とは別キー
        agg_scores_container["colloquial_score"] = derived_scores_map["colloquial_score"] # LLM評価の "colloquial_gothic_blend_quality" とは別キー

        self.logger.debug(
            f"計算された派生スコア: Phase={phase_score_val:.1f}, LayerBalance={layer_score_val:.1f}, "
            f"EmotionArc={emotion_score_val:.1f}, ColloquialBlend={colloquial_score_val:.1f}"
        )

    def _calculate_phase_score(self, phase_distribution_map: JsonDict, phase_focus_param: str) -> float:
        """位相分布のバランスと焦点に基づいて位相スコアを計算します。"""
        if not phase_distribution_map: return 2.0 # 分布データがなければ中間スコア
        
        target_distribution_map = self.config.PHASE_BALANCE_TARGETS
        deviation_tolerance = self.config.PHASE_DEVIATION_TOLERANCE
        focus_target_ratio_val = self.config.PHASE_SCORE_FOCUS_TARGET_RATIO
        balance_penalty_multiplier = self.config.PHASE_SCORE_BALANCE_PENALTY_FACTOR
        diversity_bonus_per_phase = self.config.PHASE_SCORE_DIVERSITY_BONUS / len(target_distribution_map) # 1位相あたりのボーナス

        total_weighted_deviation = 0.0
        num_valid_phases_in_target = 0
        
        for phase_key, target_ratio_val in target_distribution_map.items():
            if phase_key in phase_distribution_map: # 目標位相が分析結果に存在する場合のみ
                num_valid_phases_in_target +=1
                actual_ratio_val = phase_distribution_map.get(phase_key, 0.0)
                deviation_val = abs(actual_ratio_val - target_ratio_val)
                # 許容誤差を超えた逸脱に対してペナルティ
                penalty_for_phase = max(0, deviation_val - deviation_tolerance) * balance_penalty_multiplier
                total_weighted_deviation += penalty_for_phase * (target_ratio_val * 2) # 目標比率が高いほどペナルティ影響大

        # バランススコア要素 (5.0点満点から逸脱ペナルティを引く)
        # 最大ペナルティは、全位相が最大逸脱した場合を想定 (例: penalty_factor * num_valid_phases)
        max_possible_total_penalty = balance_penalty_multiplier * num_valid_phases_in_target if num_valid_phases_in_target > 0 else 1.0
        balance_score_component = max(0.0, 5.0 - (total_weighted_deviation / max(0.1, max_possible_total_penalty) * 5.0))

        # 多様性スコア要素 (存在する位相の数に応じてボーナス)
        num_present_phases = len([
            phase_k for phase_k, ratio_v in phase_distribution_map.items()
            if isinstance(ratio_v, (int, float)) and ratio_v > 0.01 and phase_k in target_distribution_map
        ])
        diversity_score_component = min(5.0, 1.0 + num_present_phases * diversity_bonus_per_phase * 2.5) # スケーリング調整

        # 焦点スコア要素
        focus_score_component = 3.0 # デフォルトは中間
        if phase_focus_param != "balanced" and phase_focus_param in phase_distribution_map:
            actual_focus_ratio = phase_distribution_map.get(phase_focus_param, 0.0)
            # 焦点位相の比率が目標値に近いほど高スコア
            focus_score_component = min(5.0, max(1.0, 3.0 + (actual_focus_ratio - focus_target_ratio_val) * 10.0)) # 変化を大きく

        # 最終スコアの組み合わせ
        final_phase_score: float
        if phase_focus_param != "balanced": # 焦点がある場合
            final_phase_score = (focus_score_component * 0.5) + (diversity_score_component * 0.3) + (balance_score_component * 0.2)
        else: # バランス重視の場合
            final_phase_score = (balance_score_component * 0.45) + (diversity_score_component * 0.45) + (focus_score_component * 0.1)
        
        return max(0.0, min(5.0, final_phase_score)) # 0-5にクランプ

    def _calculate_layer_balance_score(self, layer_distribution_map: JsonDict) -> float:
        """層分布のバランスに基づいて層バランススコアを計算します。"""
        if not layer_distribution_map: return 2.0

        target_distribution_map = self.config.LAYER_BALANCE_TARGETS
        deviation_tolerance = self.config.LAYER_DEVIATION_TOLERANCE
        balance_penalty_multiplier = self.config.LAYER_BALANCE_PENALTY_FACTOR

        total_weighted_deviation_penalty = 0.0
        num_valid_layers_in_target = 0
        
        for layer_key, target_ratio_val in target_distribution_map.items():
            if layer_key in layer_distribution_map: # 目標層が分析結果に存在
                num_valid_layers_in_target += 1
                actual_ratio_val = layer_distribution_map.get(layer_key, 0.0)
                deviation_val = abs(actual_ratio_val - target_ratio_val)
                penalty_for_layer = max(0, deviation_val - deviation_tolerance) * balance_penalty_multiplier
                total_weighted_deviation_penalty += penalty_for_layer * (target_ratio_val * 2) # 目標比率で重み付け

        # 最大ペナルティで正規化し、5.0から減点
        max_possible_total_penalty_layer = balance_penalty_multiplier * num_valid_layers_in_target if num_valid_layers_in_target > 0 else 1.0
        normalized_total_penalty = (total_weighted_deviation_penalty / max(0.1, max_possible_total_penalty_layer)) * 5.0
        layer_balance_final_score = 5.0 - normalized_total_penalty
        
        return max(0.0, min(5.0, layer_balance_final_score))

    def _calculate_emotion_arc_score(self, text_content: str, emotion_arc_target_str: Optional[str]) -> float:
        """感情変容スコアを計算します (v1.8 プレースホルダー/基本ヒューリスティック)。"""
        # v1.8では、これはプレースホルダーまたは非常に基本的なヒューリスティックのまま。
        # 真の感情変容追跡はより高度なNLPが必要。
        # LLM評価の`emotion_arc_quality`を参照するか、ここで簡易チェック。
        # ここでは、ターゲットが指定されていれば中間スコア、なければやや低いスコアとする。
        if emotion_arc_target_str:
            self.logger.debug(f"感情変容スコアリング: 目標 '{emotion_arc_target_str}' が提供されました。プレースホルダースコア 3.0。")
            # 今後の改善: テキスト内の感情語の変化を追跡し、目標の弧と照合する。
            # 例: 感情辞書を使用し、テキストをセグメントに分割して各セグメントの感情価を計算。
            return 3.0
        else:
            self.logger.debug("感情変容スコアリング: 目標指定なし。プレースホルダースコア 2.5。")
            return 2.5

    def _calculate_colloquial_score(self, text_content: str, colloquial_target_level_str: str) -> float:
        """口語/ゴシック融合スコアを計算します (v1.8 プレースホルダー/基本ヒューリスティック)。"""
        self.logger.debug(f"口語/ゴシック融合スコア: 目標レベル '{colloquial_target_level_str}'。プレースホルダースコア 3.0。")
        # これも複雑な言語学的タスク。v1.8では簡易ヒューリスティックまたはプレースホルダー。
        # 例: colloquial_target_level_str が "high" で、テキストに古風な表現が多い場合は減点。
        #     "low" (つまりゴシック寄り) で、現代的な口語が多い場合は減点。
        # 現時点では中間的なスコアを返す。
        
        calculated_score = 3.0 # 中間点
        text_content_lower = text_content.lower()
        
        # configから口語・文語マーカーリストを取得することを推奨
        # ここでは簡易的なマーカーリストを使用
        informal_markers_list = getattr(self.config, "COLLOQUIAL_MARKERS_INFORMAL", ["だよね", "ってか", "まじ", "ウケる", "みたいな", "じゃん"])
        formal_archaic_markers_list = getattr(self.config, "COLLOQUIAL_MARKERS_FORMAL", ["である。", "なり。", "御座います", "申し上げます", "～奉る", "候", "ぬれば"])

        num_informal_hits = sum(marker.lower() in text_content_lower for marker in informal_markers_list)
        num_formal_hits = sum(marker in text_content for marker in formal_archaic_markers_list) # 一部は文体的に大文字小文字区別も

        # テキスト長に対するヒット数の比率で調整も検討 (例: / (len(text_content)/1000 + 1))
        # ヒット数が多いほど影響大とする
        informal_factor = math.log1p(num_informal_hits) # log(1+x) で急激な変化を抑制
        formal_factor = math.log1p(num_formal_hits)

        if colloquial_target_level_str == "high": # 口語度高が目標
            if formal_factor > informal_factor * 1.5: calculated_score -= 1.5 # フォーマルすぎ
            elif informal_factor < 0.1: calculated_score -= 0.5 # 口語度が不足
            else: calculated_score += 0.5 # ある程度口語的であればボーナス
        elif colloquial_target_level_str == "low": # 口語度低（ゴシック寄り）が目標
            if informal_factor > formal_factor * 1.5: calculated_score -= 1.5 # 口語的すぎ
            elif formal_factor < 0.1: calculated_score -= 0.5 # フォーマル度/ゴシック度が不足
            else: calculated_score += 0.5
        elif colloquial_target_level_str == "medium": # 中間が目標
            # どちらかに極端に偏っている場合にペナルティ
            if informal_factor > formal_factor * 2.5 and formal_factor < 0.1: calculated_score -= 1.0
            if formal_factor > informal_factor * 2.5 and informal_factor < 0.1: calculated_score -= 1.0
        
        return max(1.0, min(4.5, calculated_score)) # 1.0-4.5の範囲にクランプ（5.0は達成困難なため）

    # processメソッドおよびその他のTextProcessorロジックは後続のパートで実装
    # ... (processメソッドおよび他のTextProcessorロジックのプレースホルダー)

# =============================================================================
# Part 11 End: TextProcessor Initialization and Core Evaluation Helper Methods
# =============================================================================
# =============================================================================
# Part 12: TextProcessor Main Logic (Process Method Skeleton & Loop 0)
# (Continues TextProcessor class from Part 11)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, LLMClient, EvaluationResult, BaseEvaluator, Evaluator,
# ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator, NarrativeFlowFramework,
# VocabularyManager, Result, NGGSError, ConfigurationError, TemplateError, LLMError,
# JsonDict, validate_template, truncate_text, get_metric_display_name, ERROR_SEVERITY_MAP, ErrorSeverityなど。

import logging
import time # ループ時間計測用
from datetime import datetime, timezone # 結果辞書のタイムスタンプ用
from typing import (
    Dict, List, Any, Optional, Tuple, Callable # Part 1からの型も含む
)

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# from nggs_lite_part2 import Result, NGGSError, LLMError, EvaluationError, ERROR_SEVERITY_MAP, ErrorSeverity
# from nggs_lite_part4 import LLMClient, TemplateError, validate_template # LLMClientはPart 5で完成
# from nggs_lite_part6 import VocabularyManager
# from nggs_lite_part8 import EvaluationResult, BaseEvaluator, Evaluator
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part3 import truncate_text, get_metric_display_name
# from nggs_lite_part11 import logger_text_processor # TextProcessorのロガー

# logger_text_processor はPart 11で定義済みと仮定
# logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")

class TextProcessor:
    # --- __init__ and core evaluation helpers from Part 11 ---
    # これらのメソッドはPart 11で定義済みであると仮定し、ここでは簡略化して再掲または省略します。
    # 完全なスクリプトでは、これらがこのクラス定義内に存在します。
    def __init__(
        self, config: NGGSConfig, llm_client: LLMClient, evaluator: Evaluator,
        vocab_manager: VocabularyManager, narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator, ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator, generation_template: str,
        improvement_template: str, ndgs_parser: Optional[Any] = None
    ):
        self.logger = logger_text_processor # Part 11で初期化済み
        # --- コアコンポーネントの検証と格納 (Part 11で実装済み) ---
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TextProcessor: NGGSConfig無効")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TextProcessor: LLMClient無効")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TextProcessor: Evaluator無効")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TextProcessor: VocabularyManager無効")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TextProcessor: NarrativeFlowFramework無効")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TextProcessor: ExtendedETIEvaluator無効")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TextProcessor: RIEvaluator無効")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TextProcessor: SubjectiveEvaluator無効")
        if ndgs_parser is not None and not hasattr(ndgs_parser, 'parse'):
            self.logger.warning("TextProcessor: ndgs_parserに 'parse' メソッドがない可能性。")

        self.config: NGGSConfig = config
        self.llm: LLMClient = llm_client
        self.evaluator: Evaluator = evaluator
        self.vocab_manager: VocabularyManager = vocab_manager
        self.narrative_flow: NarrativeFlowFramework = narrative_flow
        self.eti_evaluator: ExtendedETIEvaluator = eti_evaluator
        self.ri_evaluator: RIEvaluator = ri_evaluator
        self.subjective_evaluator: SubjectiveEvaluator = subjective_evaluator
        self.ndgs_parser: Optional[Any] = ndgs_parser

        gen_template_res = validate_template(generation_template, expected_keys=["input_text", "target_length"])
        if gen_template_res.is_err: raise gen_template_res.error
        self.generation_template: str = generation_template

        imp_template_res = validate_template(improvement_template, expected_keys=["original_text", "evaluation_results_json"])
        if imp_template_res.is_err: raise imp_template_res.error
        self.improvement_template: str = improvement_template
        self.logger.info(f"TextProcessor初期化完了 (v{config.VERSION})")

    def _get_default_aggregated_scores(self) -> JsonDict: # Part 11で実装済み
        default_score = 0.0
        return {
            "overall_quality": default_score,"gothic_atmosphere": default_score,"stylistic_gravity": default_score,
            "indirect_emotion": default_score,"vocabulary_richness": default_score,"eti_overall": default_score,
            "subjective_depth": default_score,"phase_transition_quality": default_score,"colloquial_gothic_blend_quality": default_score,
            "layer_balance_quality": default_score,"emotion_arc_quality": default_score,
            "eti_total_calculated": default_score,"ri_total_calculated": default_score,"subjective_score": default_score,
            "phase_score": default_score,"layer_balance_score": default_score,"emotion_arc_score": default_score,"colloquial_score": default_score
        }

    def _perform_full_evaluation( # Part 11で実装済み
        self, text_to_evaluate: str, phase_focus_param: str,
        colloquial_level_param: str, emotion_arc_param: Optional[str]
    ) -> Result[JsonDict, EvaluationError]:
        full_results_dict: JsonDict = {
            "llm_eval_result": None, "eti_result": None, "ri_result": None, "subjective_result": None,
            "phase_distribution": {}, "layer_distribution": {}, "derived_scores": {},
            "aggregated_scores": self._get_default_aggregated_scores(), "errors": [], "llm_eval_success_flag": False
        }
        aggregated_scores_map: Dict[str, float] = self._get_default_aggregated_scores().copy()
        base_llm_eval_context = self._run_llm_eval(text_to_evaluate, full_results_dict, aggregated_scores_map)
        full_results_dict["llm_eval_success_flag"] = (base_llm_eval_context is not None and isinstance(base_llm_eval_context.scores, dict) and "overall_quality" in base_llm_eval_context.scores)
        self._run_eti_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)
        self._run_ri_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)
        self._run_subjective_eval(text_to_evaluate, base_llm_eval_context, full_results_dict, aggregated_scores_map)
        phase_dist_data, layer_dist_data = self._run_distribution_analysis(text_to_evaluate, full_results_dict)
        self._calculate_and_store_derived_scores(text_to_evaluate, phase_dist_data, layer_dist_data, phase_focus_param, colloquial_level_param, emotion_arc_param, full_results_dict, aggregated_scores_map)
        full_results_dict["aggregated_scores"] = {k: round(v, 2) for k, v in aggregated_scores_map.items() if isinstance(v, (int, float))}
        if full_results_dict["errors"]: self.logger.warning(f"完全評価完了、ただし {len(full_results_dict['errors'])} 件の回復可能エラー発生。")
        else: self.logger.info("完全評価成功。")
        return Result.ok(full_results_dict)

    def _run_llm_eval(self, text_to_eval: str, results_container: JsonDict, agg_scores_container: Dict[str, float]) -> Optional[EvaluationResult]: # Part 11で実装済み
        default_s, _ = self.evaluator._get_default_scores_and_reasons(); eval_res_obj: Optional[EvaluationResult] = None
        try:
            eval_res_obj = self.evaluator.evaluate(text_to_eval); results_container["llm_eval_result"] = eval_res_obj
            if eval_res_obj.scores and "overall_quality" in eval_res_obj.scores:
                agg_scores_container.update({k: float(v) for k,v in eval_res_obj.scores.items() if isinstance(v,(int,float))}); self.logger.info("基本LLM評価成功。")
            else: msg = eval_res_obj.analysis or "LLM評価有効スコア生成失敗。"; results_container["errors"].append(f"LLM Eval: {msg}"); self.logger.error(f"LLM評価失敗/無効データ: {msg}"); agg_scores_container.update(default_s)
        except Exception as e: crit_msg = f"LLM評価中致命的エラー: {e}"; eval_err = EvaluationError(crit_msg); self.logger.critical(str(eval_err),exc_info=True); results_container["errors"].append(str(eval_err)); agg_scores_container.update(default_s); eval_res_obj = EvaluationResult(analysis=f"LLM評価エラー: {e}", scores=default_s.copy()); results_container["llm_eval_result"] = eval_res_obj
        return eval_res_obj
    def _run_eti_eval(self, text_to_eval: str, base_eval_ctx: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None: # Part 11で実装済み
        default_s = 2.5; try: eti_res = self.eti_evaluator.evaluate(text_to_eval, base_eval_ctx); results_container["eti_result"]=eti_res; main_s=eti_res.scores.get("eti_total_calculated"); agg_scores_container["eti_total_calculated"]=float(main_s) if isinstance(main_s,(int,float)) else default_s; if not isinstance(main_s,(int,float)):warn="ETI評価数値スコアなし";self.logger.warning(warn);results_container["errors"].append(f"ETI:{warn}")
        except Exception as e: msg=f"ETI評価失敗:{e}";self.logger.error(msg,exc_info=True);results_container["errors"].append(msg);results_container["eti_result"]=EvaluationResult(analysis=f"ETI:{e}",scores={"eti_total_calculated":default_s});agg_scores_container["eti_total_calculated"]=default_s
    def _run_ri_eval(self, text_to_eval: str, base_eval_ctx: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None: # Part 11で実装済み
        default_s = 2.5; try: ri_res = self.ri_evaluator.evaluate(text_to_eval, base_eval_ctx); results_container["ri_result"]=ri_res; main_s=ri_res.scores.get("ri_total_calculated"); agg_scores_container["ri_total_calculated"]=float(main_s) if isinstance(main_s,(int,float)) else default_s; if not isinstance(main_s,(int,float)):warn="RI評価数値スコアなし";self.logger.warning(warn);results_container["errors"].append(f"RI:{warn}")
        except Exception as e: msg=f"RI評価失敗:{e}";self.logger.error(msg,exc_info=True);results_container["errors"].append(msg);results_container["ri_result"]=EvaluationResult(analysis=f"RI:{e}",scores={"ri_total_calculated":default_s});agg_scores_container["ri_total_calculated"]=default_s
    def _run_subjective_eval(self, text_to_eval: str, base_eval_ctx: Optional[EvaluationResult], results_container: JsonDict, agg_scores_container: Dict[str, float]) -> None: # Part 11で実装済み
        default_s = 2.5; try: subj_res = self.subjective_evaluator.evaluate(text_to_eval, base_eval_ctx); results_container["subjective_result"]=subj_res; main_s=subj_res.scores.get("subjective_score"); agg_scores_container["subjective_score"]=float(main_s) if isinstance(main_s,(int,float)) else default_s; if not isinstance(main_s,(int,float)):warn="主観性評価数値スコアなし";self.logger.warning(warn);results_container["errors"].append(f"Subj:{warn}")
        except Exception as e: msg=f"主観性評価失敗:{e}";self.logger.error(msg,exc_info=True);results_container["errors"].append(msg);results_container["subjective_result"]=EvaluationResult(analysis=f"Subj:{e}",scores={"subjective_score":default_s});agg_scores_container["subjective_score"]=default_s
    def _run_distribution_analysis(self, text_to_analyze: str, results_container: JsonDict) -> Tuple[JsonDict, JsonDict]: # Part 11で実装済み
        phase_d:JsonDict={}; layer_d:JsonDict={};
        if self.narrative_flow:
            try: phase_d=self.narrative_flow.analyze_phase_distribution(text_to_analyze); results_container["phase_distribution"]=phase_d; self.logger.debug(f"位相分布:{phase_d}")
            except Exception as e: msg=f"位相分布分析失敗:{e}";self.logger.error(msg,exc_info=True);results_container["errors"].append(msg);results_container["phase_distribution"]={}
            try: layer_d=self.narrative_flow.analyze_layer_distribution(text_to_analyze); results_container["layer_distribution"]=layer_d; self.logger.debug(f"層分布:{layer_d}")
            except Exception as e: msg=f"層分布分析失敗:{e}";self.logger.error(msg,exc_info=True);results_container["errors"].append(msg);results_container["layer_distribution"]={}
        else: msg="NFF利用不可、分布分析スキップ。";self.logger.warning(msg);results_container["errors"].append(msg);results_container["phase_distribution"]={};results_container["layer_distribution"]={}
        return phase_d, layer_d
    def _calculate_and_store_derived_scores( # Part 11で実装済み
        self, text_content: str, phase_dist_data: JsonDict, layer_dist_data: JsonDict, phase_focus_param: str,
        colloquial_level_param: str, emotion_arc_param: Optional[str], results_container: JsonDict, agg_scores_container: Dict[str, float]
    ) -> None:
        self.logger.debug("派生スコア計算中..."); def _safe_calc(fn, *a, name): d=2.5; try: s=fn(*a); return max(0,min(5,float(s))) if isinstance(s,(int,float)) else (self.logger.warning(f"{name}計算非数値:{type(s)}"),results_container["errors"].append(f"{name}スコアエラー:型{type(s)}"),d) [1]
        except Exception as e: msg=f"{name}スコア計算失敗:{e}";self.logger.warning(msg,exc_info=False);results_container["errors"].append(msg);return d
        phase_s=_safe_calc(self._calculate_phase_score,phase_dist_data,phase_focus_param,name="Phase");layer_s=_safe_calc(self._calculate_layer_balance_score,layer_dist_data,name="LayerBalance(D)")
        emotion_s=_safe_calc(self._calculate_emotion_arc_score,text_content,emotion_arc_param,name="EmotionArc(D)");colloquial_s=_safe_calc(self._calculate_colloquial_score,text_content,colloquial_level_param,name="Colloquial(D)")
        derived_s_map={"phase_score":round(phase_s,2),"layer_balance_score":round(layer_s,2),"emotion_arc_score":round(emotion_s,2),"colloquial_score":round(colloquial_s,2)}; results_container["derived_scores"]=derived_s_map
        agg_scores_container.update(derived_s_map); self.logger.debug(f"派生スコア:Phase={phase_s:.1f},Layer={layer_s:.1f},Emotion={emotion_s:.1f},Colloquial={colloquial_s:.1f}")
    def _calculate_phase_score(self, phase_dist_map: JsonDict, phase_focus_param: str) -> float: # Part 11で実装済み
        if not phase_dist_map:return 2.0;td=self.config.PHASE_BALANCE_TARGETS;tol=self.config.PHASE_DEVIATION_TOLERANCE;ftr=self.config.PHASE_SCORE_FOCUS_TARGET_RATIO;pf=self.config.PHASE_SCORE_BALANCE_PENALTY_FACTOR;db=self.config.PHASE_SCORE_DIVERSITY_BONUS/len(td)
        total_wd=0.0;vp=0;for ph,tr in td.items():
            if ph in phase_dist_map:vp+=1;ar=phase_dist_map.get(ph,0.0);d=abs(ar-tr);p=max(0,d-tol)*pf;total_wd+=p*(tr*2)
        max_p=pf*vp if vp>0 else 1.0;bs=max(0.0,5.0-(total_wd/max(0.1,max_p)*5.0));num_pp=len([k for k,r in phase_dist_map.items() if isinstance(r,(int,float)) and r>0.01 and k in td]);ds=min(5.0,1.0+num_pp*db*2.5)
        fs=3.0;if phase_focus_param!="balanced" and phase_focus_param in phase_dist_map:ar=phase_dist_map.get(phase_focus_param,0.0);fs=min(5.0,max(1.0,3.0+(ar-ftr)*10.0))
        return max(0.0,min(5.0,(fs*0.5)+(ds*0.3)+(bs*0.2) if phase_focus_param!="balanced" else (bs*0.45)+(ds*0.45)+(fs*0.1)))
    def _calculate_layer_balance_score(self, layer_dist_map: JsonDict) -> float: # Part 11で実装済み
        if not layer_dist_map:return 2.0;td=self.config.LAYER_BALANCE_TARGETS;tol=self.config.LAYER_DEVIATION_TOLERANCE;pf=self.config.LAYER_BALANCE_PENALTY_FACTOR
        total_wdp=0.0;vl=0;for lay,tr in td.items():
            if lay in layer_dist_map:vl+=1;ar=layer_dist_map.get(lay,0.0);d=abs(ar-tr);p=max(0,d-tol)*pf;total_wdp+=p*(tr*2)
        max_ptp=pf*vl if vl>0 else 1.0;np=(total_wdp/max(0.1,max_ptp))*5.0;s=5.0-np;return max(0.0,min(5.0,s))
    def _calculate_emotion_arc_score(self, text_content: str, emotion_arc_target_str: Optional[str]) -> float: # Part 11で実装済み
        if emotion_arc_target_str:self.logger.debug(f"感情変容スコア:目標'{emotion_arc_target_str}'。仮3.0。");return 3.0
        else:self.logger.debug("感情変容スコア:目標なし。仮2.5。");return 2.5
    def _calculate_colloquial_score(self, text_content: str, colloquial_target_level_str: str) -> float: # Part 11で実装済み
        self.logger.debug(f"口語スコア:目標'{colloquial_target_level_str}'。仮3.0。");s=3.0;tl=text_content.lower()
        im=getattr(self.config,"COLLOQUIAL_MARKERS_INFORMAL",["だよね","ってか","まじ"]);fm=getattr(self.config,"COLLOQUIAL_MARKERS_FORMAL",["である。","なり。","御座います"])
        ih=sum(m.lower() in tl for m in im);fh=sum(m in text_content for m in fm);inf=math.log1p(ih);ff=math.log1p(fh)
        if colloquial_target_level_str=="high":s+=(0.5 if inf>0.1 else -0.5) + (-1.5 if ff>inf*1.5 else 0)
        elif colloquial_target_level_str=="low":s+=(0.5 if ff>0.1 else -0.5) + (-1.5 if inf>ff*1.5 else 0)
        elif colloquial_target_level_str=="medium":s-=(1.0 if (inf>ff*2.5 and ff<0.1) or (ff>inf*2.5 and inf<0.1) else 0)
        return max(1.0,min(4.5,s))
    # --- End of Methods from Part 11 ---

    # --- Stub for _generate_text (Part 12 - Functional stub for Loop 0) ---
    # Full implementation will be in Part 14.
    def _generate_text(
        self,
        input_text_or_context: str, # 初期コンテキストまたは改善対象テキスト
        is_improvement: bool,       # Trueなら改善、Falseなら初期生成
        target_length: int,
        perspective_mode: str,
        phase_focus: str,
        colloquial_level: str,
        emotion_arc: Optional[str],
        narrative_flow_prompt: Optional[str], # 初期生成時の物語構成指示
        improvement_instructions: Optional[str], # 改善時の具体的指示
        evaluation_summary_json: Optional[str] = None, # 改善時の前ループ評価サマリ
        temperature_override: Optional[float] = None
    ) -> Result[str, LLMError]: # LLMErrorはPart 2で定義
        """
        コアなテキスト生成メソッド。プロンプトをフォーマットし、LLMを呼び出します。
        (Part 12ではループ0で機能するスタブ、Part 14で完全実装)
        """
        self.logger.info(
            f"コアテキスト生成メソッド呼び出し (スタブ in Part 12)。タイプ: {'改善' if is_improvement else '初期'}, "
            f"入力長: {len(input_text_or_context)}, 目標長: {target_length}"
        )
        
        # プロンプトテンプレートの選択
        current_template_str = self.improvement_template if is_improvement else self.generation_template
        
        # プロンプトに埋め込むデータの準備
        # SafeDictはPart 3で定義
        prompt_data = SafeDict()
        prompt_data['target_length'] = target_length
        prompt_data['perspective_mode'] = perspective_mode
        prompt_data['phase_focus'] = phase_focus
        prompt_data['colloquial_level'] = colloquial_level
        prompt_data['emotion_arc'] = emotion_arc if emotion_arc else "(指定なし)"
        
        # 語彙リストの取得 (改善提案: スタイルや文脈に応じて変化させる)
        # ここでは汎用的なものを取得するスタブ
        vocab_for_prompt_str = self.vocab_manager.get_vocabulary_for_prompt(count=30) # Part 7で定義
        prompt_data['vocabulary_list_str'] = vocab_for_prompt_str if vocab_for_prompt_str else "(利用可能な関連語彙なし)"

        if is_improvement:
            prompt_data['original_text'] = truncate_text(input_text_or_context, 1500) # 改善対象テキスト (長すぎると問題なので切り詰める)
            prompt_data['evaluation_results_json'] = evaluation_summary_json if evaluation_summary_json else "{}"
            # improvement_sectionは改善指示そのもの
            prompt_data['improvement_section'] = improvement_instructions if improvement_instructions else "(特定の改善指示なし。総合的に品質向上を目指してください。)"
            # 改善時はnarrative_flow_promptは通常使用しないか、あるいは補助的に使う
            # ここでは簡単のため、改善指示にnarrative_flow_promptの内容が含まれると仮定
            prompt_data['narrative_flow_section'] = "(改善指示に統合)"
            # low_score_items_str, high_score_items_str, layer_distribution_analysis, phase_distribution_analysis
            # これらはevaluation_summary_jsonから導出するか、別途渡す必要がある。
            # ここではプレースホルダー。
            prompt_data['low_score_items_str'] = "(低評価項目サマリープレースホルダー)"
            prompt_data['high_score_items_str'] = "(高評価項目サマリープレースホルダー)"
            prompt_data['layer_distribution_analysis'] = "(層分布分析プレースホルダー)"
            prompt_data['phase_distribution_analysis'] = "(位相分布分析プレースホルダー)"

        else: # 初期生成
            prompt_data['input_text'] = input_text_or_context # 初期コンテキスト
            prompt_data['narrative_flow_section'] = narrative_flow_prompt if narrative_flow_prompt else "(特定の物語構成指示なし)"
            # 初期生成時には改善関連のプレースホルダーは不要だが、テンプレートが共通なら空文字などで埋める
            prompt_data['original_text'] = "" # 使わない
            prompt_data['evaluation_results_json'] = "{}" # 使わない
            prompt_data['improvement_section'] = "" # 使わない

        try:
            final_prompt_str = current_template_str.format_map(prompt_data)
        except KeyError as e_key:
            # テンプレートに必要なキーがprompt_dataにない場合
            self.logger.error(f"プロンプトのフォーマット中にKeyError: {e_key}。テンプレートとデータを確認してください。")
            return Result.fail(LLMError(f"プロンプトフォーマットエラー(KeyError): {e_key}", details={"missing_key": str(e_key)}))
        except Exception as e_format_prompt:
            self.logger.error(f"プロンプトのフォーマット中に予期せぬエラー: {e_format_prompt}", exc_info=True)
            return Result.fail(LLMError(f"プロンプトフォーマットエラー: {e_format_prompt}"))

        # 温度設定: 引数 > configのデフォルト
        temp_for_llm_call = temperature_override if temperature_override is not None \
                            else self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)

        # LLM呼び出し (実際のAPIコール)
        llm_generation_result = self.llm.generate(final_prompt_str, temperature=temp_for_llm_call)

        if llm_generation_result.is_ok:
            generated_text_content = llm_generation_result.unwrap()
            # スタブとしての基本的な長さ調整 (簡易版)
            if len(generated_text_content) > target_length * 1.8: # 長すぎる場合は切り詰める
                generated_text_content = generated_text_content[:int(target_length * 1.5)]
                self.logger.debug(f"_generate_text: 生成テキストが長すぎたため切り詰めました (目標: {target_length})。")
            elif len(generated_text_content) < target_length * 0.3 and not is_improvement: # 初期生成で短すぎる場合
                padding_text = "\n(この部分はスタブ生成による自動補足テキストです。目標文字数に達していません...)"
                num_padding_needed = (target_length - len(generated_text_content)) // len(padding_text) +1
                generated_text_content += padding_text * max(0, num_padding_needed // 2) # ある程度補足
                self.logger.debug(f"_generate_text: 生成テキストが短すぎたため補足しました (目標: {target_length})。")
            
            self.logger.debug(f"_generate_textスタブが長さ {len(generated_text_content)} のテキストを生成しました。")
            return Result.ok(generated_text_content)
        else:
            self.logger.error(f"_generate_textスタブ内のLLM呼び出し失敗: {llm_generation_result.error}")
            return Result.fail(llm_generation_result.error) # LLMErrorをそのまま伝播

    # --- _perform_initial_generation (Part 11でスタブだったものを、スタブ化された_generate_textを呼び出すように更新) ---
    def _perform_initial_generation(
        self, initial_context_text: str, target_text_length: int, perspective_mode_str: str,
        phase_focus_str: str, colloquial_level_str: str, emotion_arc_str: Optional[str],
        narrative_flow_prompt_str: Optional[str]
    ) -> Result[str, LLMError]: # LLMErrorはPart 2で定義
        self.logger.info("初期テキスト生成を実行中 (_perform_initial_generation)...")
        return self._generate_text(
            input_text_or_context=initial_context_text,
            is_improvement=False, # 初期生成
            target_length=target_text_length,
            perspective_mode=perspective_mode_str,
            phase_focus=phase_focus_str,
            colloquial_level=colloquial_level_str,
            emotion_arc=emotion_arc_str,
            narrative_flow_prompt=narrative_flow_prompt_str,
            improvement_instructions=None, # 初期生成時は改善指示なし
            evaluation_summary_json=None, # 初期生成時は評価サマリなし
            temperature_override=None # configのデフォルト生成温度を使用
        )

    # --- Main process method (Skeleton & Loop 0 Implementation) ---
    def process(
        self,
        initial_text_input: str, # NDGSからはファイルパスが渡される場合もあるので、main()で読み込んで渡す
        target_length_override: Optional[int] = None,
        perspective_mode_override: Optional[str] = None,
        phase_focus_override: Optional[str] = None,
        colloquial_level_override: Optional[str] = None,
        emotion_arc_override: Optional[str] = None,
        max_loops_override: Optional[int] = None,
        improvement_threshold_override: Optional[float] = None,
        narrative_flow_prompt_override: Optional[str] = None, # 外部からの物語構成指示
        skip_initial_generation_flag: bool = False,
        ndgs_input_data_dict: Optional[JsonDict] = None # NDGSからの構造化データ
    ) -> Result[JsonDict, NGGSError]: # NGGSErrorはPart 2で定義
        """
        テキストを生成し、反復的な改善ループを通じて処理します (v1.8)。
        """
        # --- パラメータの検証と設定 ---
        # configのデフォルト値を使用し、オーバーライドがあれば適用
        final_max_loops = max_loops_override if max_loops_override is not None else self.config.DEFAULT_MAX_LOOPS
        final_improvement_threshold = improvement_threshold_override if improvement_threshold_override is not None else self.config.DEFAULT_IMPROVEMENT_THRESHOLD
        final_target_text_length = target_length_override if target_length_override is not None else self.config.DEFAULT_TARGET_LENGTH
        
        # 各パラメータのデフォルト値はconfigから取得するが、引数でNoneでない値が渡されたらそれを優先
        # (NGGSConfigにこれらのデフォルト値が定義されている前提)
        final_perspective_mode = perspective_mode_override if perspective_mode_override is not None else getattr(self.config, 'PERSPECTIVE_MODE_DEFAULT', "subjective_first_person")
        final_phase_focus = phase_focus_override if phase_focus_override is not None else getattr(self.config, 'PHASE_FOCUS_DEFAULT', "balanced")
        final_colloquial_level = colloquial_level_override if colloquial_level_override is not None else getattr(self.config, 'COLLOQUIAL_LEVEL_DEFAULT', "medium")
        final_emotion_arc = emotion_arc_override # Noneの可能性あり
        final_narrative_flow_prompt = narrative_flow_prompt_override # Noneの可能性あり

        # パラメータのバリデーション (範囲チェックなど)
        final_max_loops = max(0, min(final_max_loops, 10)) # ループ数は0から10まで (0は初期評価のみ)
        final_improvement_threshold = max(0.0, min(5.0, final_improvement_threshold))

        self.logger.info(
            f"TextProcessor.process開始。最大ループ:{final_max_loops},目標長:{final_target_text_length},改善閾値:{final_improvement_threshold:.1f},"
            f"視点:{final_perspective_mode},位相焦点:{final_phase_focus},口語レベル:{final_colloquial_level},"
            f"感情弧:{final_emotion_arc or '指定なし'},初期生成スキップ:{skip_initial_generation_flag},"
            f"NDGS入力:{'あり' if ndgs_input_data_dict else 'なし'}"
        )

        # --- 結果辞書の初期化 ---
        # NGGSErrorはPart 2で定義
        results_output_dict: JsonDict = {
            "job_id": self.config.generate_job_id(), # Part 1で定義
            "start_time_utc": datetime.now(timezone.utc).isoformat(),
            "original_text_input_preview": truncate_text(initial_text_input, 100) if isinstance(initial_text_input, str) else "(NDGSデータから抽出予定)",
            "parameters_used": {
                "target_length": final_target_text_length, "perspective_mode": final_perspective_mode,
                "phase_focus": final_phase_focus, "colloquial_level": final_colloquial_level,
                "emotion_arc": final_emotion_arc, "max_loops": final_max_loops,
                "improvement_threshold": final_improvement_threshold,
                "skip_initial_generation": skip_initial_generation_flag,
                "ndgs_input_provided_flag": bool(ndgs_input_data_dict),
                "llm_engine_used": self.llm.current_engine,
                "llm_model_used": self.llm._get_model_name() # Part 4で定義
            },
            "versions_data": [], # 各ループのテキストと評価結果を格納
            "final_generated_text": "",
            "best_text_loop_index": -1, # 最良結果が得られたループのインデックス
            "final_aggregated_scores": {}, # 最終的な集約スコア
            "final_distributions": {"phase": {}, "layer": {}}, # 最終テキストの分布
            "html_report_path": "", # HTMLレポートのパス (後のパートで生成)
            "overall_status": "Processing",
            "processing_errors_summary": [] # 処理中に発生したエラーの概要
        }

        # --- NDGS入力データの処理 (オプション) ---
        current_text_for_processing = initial_text_input if isinstance(initial_text_input, str) else ""
        if ndgs_input_data_dict and self.ndgs_parser:
            self.logger.info("NDGS入力データを解析中...")
            try:
                # NDGSIntegration.parseはResult[JsonDict, IntegrationError]を返すと想定 (Part 15で定義)
                # IntegrationErrorはPart 2で定義
                parsed_ndgs_result = self.ndgs_parser.parse(ndgs_input_data_dict) # type: ignore
                if parsed_ndgs_result.is_ok:
                    ndgs_parsed_content = parsed_ndgs_result.unwrap()
                    # NDGSから取得したテキストやパラメータでオーバーライド
                    current_text_for_processing = ndgs_parsed_content.get("initial_text", current_text_for_processing)
                    final_perspective_mode = ndgs_parsed_content.get("parameters_override", {}).get("perspective_mode", final_perspective_mode)
                    # 他のパラメータも同様にオーバーライド可能
                    results_output_dict["original_text_input_preview"] = truncate_text(current_text_for_processing, 100)
                    results_output_dict["parameters_used"]["perspective_mode"] = final_perspective_mode # 更新
                    self.logger.info("NDGSデータ解析完了。初期テキストとパラメータが更新された可能性があります。")
                else:
                    ndgs_parse_error_msg = f"NDGS入力データの解析に失敗: {parsed_ndgs_result.error}"
                    self.logger.error(ndgs_parse_error_msg)
                    results_output_dict["processing_errors_summary"].append(ndgs_parse_error_msg)
                    # NDGS解析失敗時は、提供されたinitial_text_inputで続行するか、エラーとするか検討
                    # ここでは、エラーを記録し、可能なら続行
            except Exception as e_ndgs_proc:
                ndgs_unexpected_error_msg = f"NDGS入力データの処理中に予期せぬエラー: {e_ndgs_proc}"
                self.logger.error(ndgs_unexpected_error_msg, exc_info=True)
                results_output_dict["processing_errors_summary"].append(ndgs_unexpected_error_msg)
        
        # 初期テキストが最終的に空で、かつ初期生成をスキップしない場合はエラー
        if not current_text_for_processing and not skip_initial_generation_flag:
            return Result.fail(ConfigurationError(
                "処理可能な初期テキストが見つかりませんでした（NDGSデータからも抽出できませんでした）。"
            ))

        # --- ループ変数の初期化 ---
        # current_text_for_next_loopは、次のループで処理されるテキスト
        current_text_for_next_loop = current_text_for_processing if skip_initial_generation_flag else ""
        best_text_overall = current_text_for_next_loop # 最良テキストの初期値
        # best_full_eval_data_overallは、最良テキストに対応する_perform_full_evaluationの全結果
        best_full_eval_data_overall: JsonDict = {}
        
        # --- 物語構成プロンプトの生成 (必要であれば) ---
        # narrative_flowはPart 10で定義
        final_narrative_flow_prompt_str_for_gen = final_narrative_flow_prompt # 引数で渡されたものを優先
        if final_narrative_flow_prompt_str_for_gen is None and self.narrative_flow:
            try:
                # テーマはパラメータ化するか、configから取得
                default_theme_for_flow = getattr(self.config, 'NARRATIVE_THEME_DEFAULT', "記憶回帰型")
                final_narrative_flow_prompt_str_for_gen = self.narrative_flow.generate_flow_template(
                    theme=default_theme_for_flow,
                    emotion_arc_str=final_emotion_arc, # final_emotion_arcはOptional[str]
                    perspective_mode_str=final_perspective_mode
                )
                self.logger.debug(f"生成された物語構成プロンプト: {truncate_text(final_narrative_flow_prompt_str_for_gen, 120)}")
            except Exception as e_narrative_flow_gen:
                nff_gen_error_msg = f"物語構成プロンプトの生成中にエラー: {e_narrative_flow_gen}"
                self.logger.error(nff_gen_error_msg, exc_info=True)
                results_output_dict["processing_errors_summary"].append(nff_gen_error_msg)
                final_narrative_flow_prompt_str_for_gen = None # エラー時は構成指示なしで続行

        # --- メイン処理ループ ---
        try:
            # === ループ 0: 初期生成または初期評価 ===
            current_loop_index = 0
            current_version_data: JsonDict = {"loop_index": current_loop_index, "status": "Pending"}
            results_output_dict["versions_data"].append(current_version_data)
            self.logger.info(f"--- 開始 ループ {current_loop_index} ({'初期評価' if skip_initial_generation_flag else '初期生成'}) ---")
            loop_start_monotonic_time = time.monotonic()
            error_occurred_in_current_loop = False

            if skip_initial_generation_flag:
                self.logger.info("初期生成はスキップされました。提供されたテキストを評価します。")
                current_text_for_next_loop = current_text_for_processing # 提供されたテキストを使用
                current_version_data["text_source_type"] = "provided_initial_text"
                current_version_data["generated_text_preview"] = truncate_text(current_text_for_next_loop, 150)
            else: # 初期生成を実行
                current_version_data["text_source_type"] = "initial_generation"
                initial_generation_result = self._perform_initial_generation(
                    initial_context_text=current_text_for_processing, # 生成コンテキスト
                    target_text_length=final_target_text_length,
                    perspective_mode_str=final_perspective_mode,
                    phase_focus_str=final_phase_focus,
                    colloquial_level_str=final_colloquial_level,
                    emotion_arc_str=final_emotion_arc,
                    narrative_flow_prompt_str=final_narrative_flow_prompt_str_for_gen
                )
                if initial_generation_result.is_err:
                    # _handle_errorはboolを返す (Trueなら継続可能、Falseなら致命的)
                    # NGGSError, ERROR_SEVERITY_MAP, ErrorSeverityはPart 2で定義
                    if not self._handle_error(initial_generation_result.error, current_loop_index, results_output_dict, current_version_data, is_fatal_loop_error=True):
                        self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, "致命的エラー（初期生成）")
                        return Result.fail(initial_generation_result.error) # 致命的エラーで終了
                    error_occurred_in_current_loop = True
                    current_text_for_next_loop = "" # 評価対象テキストなし
                    current_version_data["status"] = "失敗 (初期生成エラー)"
                else:
                    current_text_for_next_loop = initial_generation_result.unwrap()
                    current_version_data["generated_text_preview"] = truncate_text(current_text_for_next_loop, 150)
            
            # テキスト評価 (初期生成が成功した場合、または初期生成スキップの場合)
            if not error_occurred_in_current_loop and current_text_for_next_loop:
                full_evaluation_result_loop0 = self._perform_full_evaluation(
                    text_to_evaluate=current_text_for_next_loop,
                    phase_focus_param=final_phase_focus,
                    colloquial_level_param=final_colloquial_level,
                    emotion_arc_param=final_emotion_arc
                )
                if full_evaluation_result_loop0.is_err: # _perform_full_evaluation自体が致命的エラーを返した場合
                    if not self._handle_error(full_evaluation_result_loop0.error, current_loop_index, results_output_dict, current_version_data, is_fatal_loop_error=True):
                        self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, "致命的エラー（評価フレームワーク）")
                        return Result.fail(full_evaluation_result_loop0.error)
                    error_occurred_in_current_loop = True
                    current_version_data["status"] = "失敗 (評価フレームワークエラー)"
                else:
                    full_eval_data_content_loop0 = full_evaluation_result_loop0.unwrap()
                    current_version_data.update(full_eval_data_content_loop0) # このバージョンの全評価データを格納
                    if full_eval_data_content_loop0.get("llm_eval_success_flag", False):
                        best_text_overall = current_text_for_next_loop
                        best_full_eval_data_overall = full_eval_data_content_loop0
                        results_output_dict["best_text_loop_index"] = current_loop_index
                        current_version_data["status"] = "成功 (評価完了)"
                    else: # _perform_full_evaluation内のLLM評価部分が失敗した場合
                        error_occurred_in_current_loop = True
                        current_version_data["status"] = "失敗 (LLM基本評価エラー)"
            
            # ループ0のステータス最終確認
            if current_version_data["status"] == "Pending": # まだステータスが設定されていない場合
                current_version_data["status"] = "失敗 (ループ0で不明な問題)" if error_occurred_in_current_loop else "完了 (エラーなしでペンディング終了)"
            elif error_occurred_in_current_loop and "失敗" not in current_version_data["status"]:
                current_version_data["status"] += " (回復可能エラーあり)"

            loop_duration_seconds = time.monotonic() - loop_start_monotonic_time
            current_version_data["duration_seconds"] = round(loop_duration_seconds, 2)
            self.logger.info(f"--- 完了 ループ {current_loop_index} ({current_version_data.get('status', '?')}) - {loop_duration_seconds:.2f} 秒 ---")

            # --- ループ0終了後の処理継続判定 ---
            current_best_overall_score = best_full_eval_data_overall.get("aggregated_scores", {}).get("overall_quality", 0.0)
            min_loops_for_threshold_check = self.config.MIN_FEEDBACK_LOOPS if self.config.MIN_FEEDBACK_LOOPS > 0 else 1

            should_terminate_after_loop0 = (
                final_max_loops == 0 or # 評価のみモード
                ("失敗" in current_version_data.get("status", "") and \
                 not current_version_data.get("status", "").startswith("失敗 (LLM基本評価エラー)")) or # ループ0で致命的エラー
                (current_best_overall_score >= final_improvement_threshold and \
                 results_output_dict["best_text_loop_index"] == 0 and \
                 current_loop_index + 1 >= min_loops_for_threshold_check)
            )

            if should_terminate_after_loop0:
                final_process_status_msg = current_version_data.get("status", "完了")
                if results_output_dict["best_text_loop_index"] == -1 and "失敗" not in final_process_status_msg:
                    final_process_status_msg = "失敗 (有効な結果なし)"
                elif "失敗" in final_process_status_msg: # 既に失敗ステータス
                    final_process_status_msg = "エラーで終了"
                elif current_best_overall_score >= final_improvement_threshold and final_max_loops > 0:
                    final_process_status_msg = "完了 (改善閾値達成)"
                elif final_max_loops == 0:
                    final_process_status_msg = "完了 (初期評価のみ)"
                
                self.logger.info(f"ループ0完了後、処理を終了します。ステータス: {final_process_status_msg}")
                self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, final_process_status_msg)
                return Result.ok(results_output_dict)

            # --- 改善ループ (Part 12ではスケルトン、Part 13で本格実装) ---
            # この時点で、best_text_overall と best_full_eval_data_overall にはループ0の結果が格納されているはず
            # (エラーがなければ)
            for improvement_loop_num in range(1, final_max_loops + 1):
                current_loop_index = improvement_loop_num # 実際のループ番号
                current_version_data = {"loop_index": current_loop_index, "status": "Pending"}
                results_output_dict["versions_data"].append(current_version_data)
                self.logger.info(f"--- 開始 ループ {current_loop_index} (改善) ---")
                loop_start_monotonic_time = time.monotonic()
                error_occurred_in_current_loop = False

                # 改善のベースとなるテキストと評価データがあるか確認
                if not best_full_eval_data_overall or not best_text_overall:
                    current_version_data["status"] = "スキップ (有効な前ループ結果なし)"
                    self.logger.warning(
                        f"ループ {current_loop_index}: スキップ。前ループの有効なテキストまたは評価が存在しません。改善ループを停止します。"
                    )
                    results_output_dict["processing_errors_summary"].append(
                        f"ループ {current_loop_index}: 有効な前結果なしのためスキップ。"
                    )
                    break # 改善ループを抜ける

                # 現在のベストスコアが改善閾値に達しているか確認
                current_best_overall_score = best_full_eval_data_overall.get("aggregated_scores", {}).get("overall_quality", 0.0)
                if current_best_overall_score >= final_improvement_threshold and current_loop_index >= min_loops_for_threshold_check:
                    current_version_data["status"] = f"スキップ (改善閾値達成済み: {current_best_overall_score:.2f})"
                    self.logger.info(
                        f"ループ {current_loop_index}: 早期終了。スコア {current_best_overall_score:.2f} >= "
                        f"閾値 {final_improvement_threshold:.2f} (最小ループ {min_loops_for_threshold_check}回実行済み)"
                    )
                    break # 改善ループを抜ける
                
                # --- _perform_improvement_loop のプレースホルダー呼び出し (Part 13で実装) ---
                self.logger.info(f"ループ {current_loop_index}: _perform_improvement_loop 呼び出し (Part 13で実装予定)")
                # ここで _perform_improvement_loop(best_text_overall, best_full_eval_data_overall, ...) を呼び出す
                # Part 12では、ダミーのテキストと評価でシミュレート
                current_text_for_next_loop = f"改善ループ {current_loop_index} のプレースホルダーテキスト (元: {truncate_text(best_text_overall, 30)})"
                current_version_data["generated_text_preview"] = truncate_text(current_text_for_next_loop, 150)
                current_version_data["text_source_type"] = f"improvement_loop_{current_loop_index}_placeholder"

                placeholder_eval_result = self._perform_full_evaluation(
                    text_to_evaluate=current_text_for_next_loop,
                    phase_focus_param=final_phase_focus,
                    colloquial_level_param=final_colloquial_level,
                    emotion_arc_param=final_emotion_arc
                )
                if placeholder_eval_result.is_err:
                    if not self._handle_error(placeholder_eval_result.error, current_loop_index, results_output_dict, current_version_data, is_fatal_loop_error=True):
                        self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, "致命的エラー（改善ループ評価）")
                        return Result.fail(placeholder_eval_result.error)
                    error_occurred_in_current_loop = True
                else:
                    new_full_eval_data_content = placeholder_eval_result.unwrap()
                    current_version_data.update(new_full_eval_data_content)
                    # Part 13で、このnew_full_eval_data_contentとbest_full_eval_data_overallを比較し、
                    # best_text_overallとbest_full_eval_data_overallを更新するロジックが入る。
                    # ここでは、仮にスコアが少しだけ改善したと仮定して更新（デモ用）
                    new_overall_score = new_full_eval_data_content.get("aggregated_scores",{}).get("overall_quality", 0.0)
                    if new_overall_score > current_best_overall_score + 0.05 : # わずかな改善でも更新
                        best_text_overall = current_text_for_next_loop
                        best_full_eval_data_overall = new_full_eval_data_content
                        results_output_dict["best_text_loop_index"] = current_loop_index
                        self.logger.info(f"ループ {current_loop_index}: 新しいベストテキストが見つかりました (スコア: {new_overall_score:.2f})。")
                    
                    if new_full_eval_data_content.get("llm_eval_success_flag", False):
                        current_version_data["status"] = "成功 (改善ループ - プレースホルダー)"
                    else:
                        error_occurred_in_current_loop = True
                        current_version_data["status"] = "失敗 (LLM評価 - 改善ループプレースホルダー)"
                
                if current_version_data["status"] == "Pending":
                    current_version_data["status"] = "失敗 (改善ループで不明な問題 - プレースホルダー)"
                elif error_occurred_in_current_loop and "失敗" not in current_version_data["status"]:
                     current_version_data["status"] += " (回復可能エラーあり - プレースホルダー)"

                loop_duration_seconds = time.monotonic() - loop_start_monotonic_time
                current_version_data["duration_seconds"] = round(loop_duration_seconds, 2)
                self.logger.info(f"--- 完了 ループ {current_loop_index} ({current_version_data.get('status', '?')} - プレースホルダー) - {loop_duration_seconds:.2f} 秒 ---")
                
                # ループ内で致命的でないエラーが発生した場合でも、次の改善ループに進むか、停止するか
                # ここでは、エラーがあれば改善ループを停止する方針
                if error_occurred_in_current_loop and "失敗" in current_version_data.get("status", ""):
                    self.logger.warning(f"ループ {current_loop_index} はエラーで終了しました。改善ループを停止します。")
                    break # 改善ループを抜ける
            
            # === ループ終了 / 最終処理 ===
            final_process_status_msg = "完了"
            if results_output_dict["processing_errors_summary"]: final_process_status_msg = "エラーありで完了"
            if results_output_dict["best_text_loop_index"] == -1: final_process_status_msg = "失敗 (有効な最終結果なし)"
            elif current_best_overall_score >= final_improvement_threshold and results_output_dict["best_text_loop_index"] >=0 :
                final_process_status_msg = "完了 (改善閾値達成)"
            elif current_loop_index >= final_max_loops : # current_loop_indexは最後に試行されたループ番号
                final_process_status_msg = "完了 (最大改善ループ到達)"

            self.logger.info(f"全ての処理ループが完了しました。最終ステータス: {final_process_status_msg}")
            self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, final_process_status_msg)
            return Result.ok(results_output_dict)

        except Exception as e_process_unexpected:
            # TextProcessor.processフロー内の予期せぬクリティカルエラーをキャッチ
            error_type_name_str = type(e_process_unexpected).__name__
            critical_error_message_process = f"TextProcessor.processで予期せぬクリティカルエラー発生: {error_type_name_str} - {e_process_unexpected}"
            self.logger.critical(critical_error_message_process, exc_info=True)
            results_output_dict["overall_status"] = "致命的エラー発生"
            results_output_dict["processing_errors_summary"].append(critical_error_message_process)
            self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, "致命的エラー発生")
            # NGGSErrorはPart 2で定義
            return Result.fail(NGGSError(critical_error_message_process, details={"exception_type": error_type_name_str}))

    def _handle_error(
        self,
        error_obj: Exception,
        current_loop_idx: int,
        results_container: JsonDict, # results_output_dict を想定
        current_version_data_container: JsonDict, # results_output_dict["versions_data"]の該当要素
        is_fatal_loop_error: bool = False # Trueならこのループは継続不可
    ) -> bool: # Trueなら処理継続可能、Falseなら致命的で処理全体を停止すべき
        """
        エラーを処理し、ログ記録し、version_dataとresults_dictを更新します。
        処理を継続できる場合はTrueを、致命的な場合はFalseを返します。
        """
        error_obj_type = type(error_obj)
        # エラーの重大度を決定: is_fatal_loop_errorがTrueなら、基本LOOP、ただし元がFATALならFATALのまま。
        # それ以外はERROR_SEVERITY_MAPに従う。
        # ERROR_SEVERITY_MAP, ErrorSeverityはPart 2で定義
        error_severity_val: ErrorSeverity
        if is_fatal_loop_error:
            base_severity_from_map = ERROR_SEVERITY_MAP.get(error_obj_type, ErrorSeverity.LOOP) # デフォルトはLOOP
            error_severity_val = ErrorSeverity.FATAL if base_severity_from_map == ErrorSeverity.FATAL else ErrorSeverity.LOOP
        else:
            error_severity_val = ERROR_SEVERITY_MAP.get(error_obj_type, ErrorSeverity.FATAL) # デフォルトはFATAL

        error_message_str = f"ループ {current_loop_idx}: [{error_severity_val.name}] {error_obj_type.__name__} - {str(error_obj)}"
        
        log_function_to_call: Callable = self.logger.critical if error_severity_val == ErrorSeverity.FATAL else \
                                   self.logger.error if error_severity_val == ErrorSeverity.LOOP else \
                                   self.logger.warning # RECOVERABLEの場合

        # トレースバックをログに出力するかどうか (FATAL時、または安全ブロック以外の未知のエラー時)
        should_log_exc_info = (error_severity_val == ErrorSeverity.FATAL)
        # 安全ブロックによるValueErrorはトレースバック不要
        is_safety_block_error = isinstance(error_obj, ValueError) and \
                                ("コンテンツ生成ブロック" in str(error_obj) or "SAFETY" in str(error_obj).upper())
        if is_safety_block_error: should_log_exc_info = False
        # 特定のAPIエラーもトレースバック抑制検討 (例: InvalidArgument)
        if GEMINI_AVAILABLE and google_exceptions and isinstance(error_obj, (google_exceptions.InvalidArgument, google_exceptions.PermissionDenied)):
            should_log_exc_info = False
            
        log_function_to_call(error_message_str, exc_info=should_log_exc_info)

        # version_dataとresults_dictを更新
        current_version_data_container["status"] = f"失敗 ({error_severity_val.name})"
        current_version_data_container["error_details_str"] = f"[{error_severity_val.name}] {error_obj_type.__name__}: {str(error_obj)}"
        if isinstance(error_obj, NGGSError) and error_obj.get_context(): # NGGSErrorはPart 2で定義
            current_version_data_container["error_details_str"] += f" Context: {error_obj.get_context()}"
        
        # グローバルエラーリストに簡潔なエラー概要を追加 (重複を避ける)
        global_error_summary_str = f"ループ {current_loop_idx}: [{error_severity_val.name}] {error_obj_type.__name__}"
        if global_error_summary_str not in results_container["processing_errors_summary"]:
            results_container["processing_errors_summary"].append(global_error_summary_str)
        
        return error_severity_val != ErrorSeverity.FATAL # FATALでなければTrue (継続可能)

    def _finalize_results(
        self,
        results_container: JsonDict, # results_output_dict
        final_best_text: str,
        final_best_full_eval_data: JsonDict, # 最良バージョンの_perform_full_evaluation結果
        overall_final_status_msg: str
    ) -> None:
        """結果辞書を最終的なデータで更新し、レポート生成を準備します（スタブ）。"""
        self.logger.info(f"処理結果を最終化中... 最終ステータス: {overall_final_status_msg}")
        
        results_container["final_generated_text"] = final_best_text if final_best_text else "(有効な生成テキストなし)"
        # best_loop_indexは既に設定されているはずだが、念のためbest_full_eval_dataからも取得試行
        results_container["best_text_loop_index"] = final_best_full_eval_data.get("loop_index", results_container.get("best_text_loop_index", -1))
        
        aggregated_scores_from_best_eval = final_best_full_eval_data.get("aggregated_scores", {})
        results_container["final_aggregated_scores"] = {
            key_str: round(val_float, 2) for key_str, val_float in aggregated_scores_from_best_eval.items()
            if isinstance(val_float, (int, float))
        }
        
        phase_dist_from_best_eval = final_best_full_eval_data.get("phase_distribution")
        layer_dist_from_best_eval = final_best_full_eval_data.get("layer_distribution")
        results_container["final_distributions"] = {
            "phase": phase_dist_from_best_eval if isinstance(phase_dist_from_best_eval, dict) else {},
            "layer": layer_dist_from_best_eval if isinstance(layer_dist_from_best_eval, dict) else {}
        }
        
        results_container["completion_time_utc"] = datetime.now(timezone.utc).isoformat()
        
        # 最終ステータスを設定 (ただし、より深刻な「致命的エラー」が既に設定されていれば上書きしない)
        if "致命的エラー" not in results_container.get("overall_status", ""):
            results_container["overall_status"] = overall_final_status_msg
        
        self.logger.info(f"_finalize_results完了。HTMLレポート生成は後のパートで実装されます。")
        # HTMLレポート生成はここで呼び出される予定 (例: self._generate_html_report(results_container))
        # results_container["html_report_path"] = self._generate_html_report(results_container)

# =============================================================================
# Part 12 End: TextProcessor Main Logic (Process Method Skeleton & Loop 0)
# =============================================================================
# =============================================================================
# Part 13: TextProcessor Improvement Loop and Instruction Generation
# (Continues TextProcessor class from Part 12)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, LLMClient, EvaluationResult, Evaluator, VocabularyManager,
# NarrativeFlowFramework, ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator,
# Result, NGGSError, ConfigurationError, TemplateError, LLMError, GenerationError,
# JsonDict, CompactJSONEncoder, get_metric_display_name, truncate_text,
# extract_improvement_instructions, validate_template など。

import logging
import json # _generate_llm_improvement_instructions で評価サマリをJSON化するため
from typing import (
    Dict, List, Any, Optional, Tuple, Callable # Part 1からの型も含む
)

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# from nggs_lite_part2 import Result, NGGSError, LLMError, EvaluationError, GenerationError
# from nggs_lite_part3 import CompactJSONEncoder, get_metric_display_name, truncate_text
# from nggs_lite_part4 import LLMClient, TemplateError, validate_template, extract_improvement_instructions # LLMClientはPart 5で完成
# from nggs_lite_part6 import VocabularyManager
# from nggs_lite_part8 import EvaluationResult, Evaluator # BaseEvaluatorは直接使用しない
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part11 import logger_text_processor # TextProcessorのロガー
# from nggs_lite_part12 import TextProcessor # TextProcessorクラス定義を開始するため

# logger_text_processor はPart 11で定義済みと仮定
# logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")

class TextProcessor:
    # --- __init__ and other methods from Part 11 & 12 ---
    # これらのメソッドはPart 11, 12で定義済みであると仮定し、ここでは簡略化して再掲または省略します。
    # 完全なスクリプトでは、これらがこのクラス定義内に存在します。
    def __init__(
        self, config: NGGSConfig, llm_client: LLMClient, evaluator: Evaluator,
        vocab_manager: VocabularyManager, narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator, ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator, generation_template: str,
        improvement_template: str, ndgs_parser: Optional[Any] = None
    ):
        self.logger = logger_text_processor
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TP: NGGSConfig無効")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TP: LLMClient無効")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TP: Evaluator無効")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TP: VocabularyManager無効")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TP: NarrativeFlowFramework無効")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TP: ExtendedETIEvaluator無効")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TP: RIEvaluator無効")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TP: SubjectiveEvaluator無効")
        self.config: NGGSConfig = config
        self.llm: LLMClient = llm_client
        self.evaluator: Evaluator = evaluator
        self.vocab_manager: VocabularyManager = vocab_manager
        self.narrative_flow: NarrativeFlowFramework = narrative_flow
        self.eti_evaluator: ExtendedETIEvaluator = eti_evaluator
        self.ri_evaluator: RIEvaluator = ri_evaluator
        self.subjective_evaluator: SubjectiveEvaluator = subjective_evaluator
        self.ndgs_parser: Optional[Any] = ndgs_parser
        gen_res = validate_template(generation_template, ["input_text","target_length"]); imp_res = validate_template(improvement_template, ["original_text","evaluation_results_json"])
        if gen_res.is_err: raise gen_res.error
        if imp_res.is_err: raise imp_res.error
        self.generation_template: str = generation_template
        self.improvement_template: str = improvement_template
        self.logger.info(f"TextProcessor初期化 (v{config.VERSION})")

    def _get_default_aggregated_scores(self) -> JsonDict: # Part 11で実装済み
        s=0.0; return {"overall_quality":s,"gothic_atmosphere":s,"stylistic_gravity":s,"indirect_emotion":s,"vocabulary_richness":s,"eti_overall":s,"subjective_depth":s,"phase_transition_quality":s,"colloquial_gothic_blend_quality":s,"layer_balance_quality":s,"emotion_arc_quality":s,"eti_total_calculated":s,"ri_total_calculated":s,"subjective_score":s,"phase_score":s,"layer_balance_score":s,"emotion_arc_score":s,"colloquial_score":s}
    def _perform_full_evaluation(self, t:str, pf:str, cl:str, ea:Optional[str]) -> Result[JsonDict, EvaluationError]: return Result.ok(self._get_default_aggregated_scores()) # Part 11で実装済み, ここでは簡略化
    def _generate_text(self, input_text_or_context:str, is_improvement:bool, target_length:int, perspective_mode:str, phase_focus:str, colloquial_level:str, emotion_arc:Optional[str], narrative_flow_prompt:Optional[str], improvement_instructions:Optional[str], evaluation_summary_json:Optional[str]=None, temperature_override:Optional[float]=None) -> Result[str, LLMError]: return Result.ok(f"Stub gen text (improve={is_improvement})") # Part 12でスタブ実装済み, ここでは簡略化
    def _perform_initial_generation(self,ict:str,tl:int,pm:str,pf:str,cl:str,ea:Optional[str],nfp:Optional[str])->Result[str,LLMError]: return self._generate_text(ict,False,tl,pm,pf,cl,ea,nfp,None,None,None) # Part 12で実装済み, ここでは簡略化
    def process(self, it:str, tl_o:Optional[int]=None, pm_o:Optional[str]=None, pf_o:Optional[str]=None, cl_o:Optional[str]=None, ea_o:Optional[str]=None, ml_o:Optional[int]=None, th_o:Optional[float]=None, nfp_o:Optional[str]=None, sigf:bool=False, ndgs_id:Optional[JsonDict]=None) -> Result[JsonDict, NGGSError]: pass # Part 12で骨子実装済み, ここでは省略
    def _handle_error(self,e:Exception,lidx:int,rd:JsonDict,vd:JsonDict,ifl:bool=False)->bool: return True # Part 12で実装済み, ここでは簡略化
    def _finalize_results(self,rd:JsonDict,bt:str,bfed:JsonDict,fs:str)->None: pass # Part 12で実装済み, ここでは簡略化
    # --- End of Methods from Part 11 & 12 ---

    # --- Improvement Loop Core (Part 13) ---
    def _perform_improvement_loop(
        self,
        loop_idx: int, # 現在の改善ループ番号 (1から開始)
        current_best_text_content: str,
        current_best_evaluation_data: JsonDict, # 前ループの_perform_full_evaluation結果
        results_container: JsonDict, # 全体結果辞書 (エラーログ用)
        current_version_data_container: JsonDict, # このループのバージョンデータ格納用
        # 生成パラメータ (processメソッドから引き継ぎ)
        final_target_text_length: int,
        perspective_mode_str: str,
        phase_focus_str: str,
        colloquial_level_str: str,
        emotion_arc_str: Optional[str],
        narrative_flow_prompt_str: Optional[str] # 初期生成時の物語構成指示 (改善時も参照する可能性)
    ) -> Result[JsonDict, NGGSError]: # このループでの full_eval_data を返す
        """
        改善ループの1イテレーションを実行します。
        戦略決定、指示生成、新テキスト生成、評価を行います。
        """
        self.logger.info(f"改善ループ {loop_idx} を開始します。")
        current_version_data_container["status"] = "改善中..."
        error_occurred_in_this_improvement_loop = False

        # 1. 改善戦略の決定
        # _determine_improvement_strategyはJsonDict (current_best_eval_data) を取る
        improvement_strategy_key = self._determine_improvement_strategy(current_best_evaluation_data)
        current_version_data_container["improvement_strategy_selected"] = improvement_strategy_key
        self.logger.info(f"ループ {loop_idx}: 改善戦略「{improvement_strategy_key}」が選択されました。")

        # 2. 改善指示の生成
        #    まず特定戦略ベースの指示を試み、失敗/不適ならLLM標準指示、それでもダメならフォールバック
        generated_improvement_instructions: Optional[str] = None
        specific_instructions_result = self._generate_specific_improvement_instructions(
            strategy_key=improvement_strategy_key,
            last_evaluation_data=current_best_evaluation_data,
            perspective_mode=perspective_mode_str,
            phase_focus=phase_focus_str,
            colloquial_level=colloquial_level_str,
            emotion_arc=emotion_arc_str
        )

        if specific_instructions_result.is_ok:
            generated_improvement_instructions = specific_instructions_result.unwrap()
            if not generated_improvement_instructions: # 戦略テンプレートが空を返した場合 (該当なしなど)
                self.logger.warning(f"ループ {loop_idx}: 特定戦略「{improvement_strategy_key}」の指示が空でした。LLM標準指示を試みます。")
                llm_standard_instructions_result = self._generate_llm_improvement_instructions(
                    current_text_content=current_best_text_content,
                    last_evaluation_data=current_best_evaluation_data,
                    perspective_mode=perspective_mode_str, phase_focus=phase_focus_str,
                    colloquial_level=colloquial_level_str, emotion_arc=emotion_arc_str
                )
                if llm_standard_instructions_result.is_ok:
                    generated_improvement_instructions = llm_standard_instructions_result.unwrap()
                else: # LLM標準指示も失敗
                    self.logger.error(f"ループ {loop_idx}: LLM標準改善指示の生成も失敗: {llm_standard_instructions_result.error}")
                    generated_improvement_instructions = self._get_contextual_fallback_improvement(current_best_evaluation_data)
                    current_version_data_container.setdefault("errors_in_loop", []).append(f"LLM Instruction Gen Error: {llm_standard_instructions_result.error}")
        else: # 特定戦略指示の生成自体がエラー (TemplateErrorなど)
            self.logger.error(f"ループ {loop_idx}: 特定戦略「{improvement_strategy_key}」の指示生成に失敗: {specific_instructions_result.error}")
            generated_improvement_instructions = self._get_contextual_fallback_improvement(current_best_evaluation_data)
            current_version_data_container.setdefault("errors_in_loop", []).append(f"Specific Instruction Gen Error: {specific_instructions_result.error}")

        if not generated_improvement_instructions or not generated_improvement_instructions.strip():
            self.logger.warning(f"ループ {loop_idx}: 有効な改善指示が得られませんでした。NGSConfigのデフォルトフォールバックを使用します。")
            generated_improvement_instructions = self.config.DEFAULT_FALLBACK_IMPROVEMENT
        
        current_version_data_container["improvement_instructions_generated"] = generated_improvement_instructions
        self.logger.debug(f"ループ {loop_idx} の改善指示:\n{truncate_text(generated_improvement_instructions, 250)}")

        # 3. 指示に基づいて新しいテキストを生成
        #    改善ループ用に温度を調整 (徐々に下げる)
        current_loop_temperature = max(
            self.config.IMPROVEMENT_MIN_TEMPERATURE,
            self.config.IMPROVEMENT_BASE_TEMPERATURE - (self.config.IMPROVEMENT_TEMP_DECREASE_PER_LOOP * (loop_idx - 1)) # loop_idxは1ベース
        )
        self.logger.info(f"ループ {loop_idx}: 改善テキスト生成に温度 {current_loop_temperature:.2f} を使用します。")

        # 改善プロンプト用の評価サマリJSONを作成
        evaluation_summary_json_for_prompt: str
        try:
            compact_eval_summary_for_prompt = {
                "overall_quality": current_best_evaluation_data.get("aggregated_scores", {}).get("overall_quality"),
                "llm_scores": current_best_evaluation_data.get("llm_eval_result", {}).get("scores", {}),
                "eti_score": current_best_evaluation_data.get("eti_result", {}).get("scores", {}).get("eti_total_calculated"),
                "ri_score": current_best_evaluation_data.get("ri_result", {}).get("scores", {}).get("ri_total_calculated"),
                "subjective_score": current_best_evaluation_data.get("subjective_result", {}).get("scores", {}).get("subjective_score"),
                "phase_distribution": current_best_evaluation_data.get("phase_distribution"),
                "layer_distribution": current_best_evaluation_data.get("layer_distribution"),
                "derived_scores": current_best_evaluation_data.get("derived_scores")
            }
            # 不要なNone値を除去
            compact_eval_summary_for_prompt = {k:v for k,v in compact_eval_summary_for_prompt.items() if v is not None}
            # CompactJSONEncoderはPart 3で定義
            evaluation_summary_json_for_prompt = json.dumps(compact_eval_summary_for_prompt, ensure_ascii=False, cls=CompactJSONEncoder)
        except Exception as e_json_dump:
            self.logger.warning(f"ループ {loop_idx}: 評価結果のJSON化に失敗（改善プロンプト用）: {e_json_dump}")
            evaluation_summary_json_for_prompt = "{ \"error\": \"評価サマリJSON化失敗\" }"
        
        # _generate_text (Part 12でスタブ実装、Part 14で本格実装) を改善モードで呼び出し
        new_text_generation_result = self._generate_text(
            input_text_or_context=current_best_text_content, # 改善対象のテキスト
            is_improvement=True,
            target_length=final_target_text_length, # 目標長は維持
            perspective_mode=perspective_mode_str,
            phase_focus=phase_focus_str,
            colloquial_level=colloquial_level_str,
            emotion_arc=emotion_arc_str,
            narrative_flow_prompt=narrative_flow_prompt_str, # 改善時も参照する可能性
            improvement_instructions=generated_improvement_instructions,
            evaluation_summary_json=evaluation_summary_json_for_prompt,
            temperature_override=current_loop_temperature
        )

        newly_generated_text: str
        if new_text_generation_result.is_err:
            # _handle_errorはPart 12で定義
            if not self._handle_error(new_text_generation_result.error, loop_idx, results_container, current_version_data_container, is_fatal_loop_error=True):
                # このエラーがプロセス全体にとって致命的な場合、NGGSErrorを伝播
                return Result.fail(new_text_generation_result.error) # NGGSErrorを期待
            error_occurred_in_this_improvement_loop = True
            newly_generated_text = "" # 新しいテキストは生成されなかった
            current_version_data_container["status"] = "失敗 (改善テキスト生成エラー)"
        else:
            newly_generated_text = new_text_generation_result.unwrap()
            current_version_data_container["generated_text_preview"] = truncate_text(newly_generated_text, 150)
            self.logger.info(f"ループ {loop_idx}: 改善テキストの生成に成功しました (長さ: {len(newly_generated_text)})。")

        # 4. 新しいテキストの評価
        # _perform_full_evaluationはPart 11で定義
        if not error_occurred_in_this_improvement_loop and newly_generated_text:
            full_evaluation_of_new_text_result = self._perform_full_evaluation(
                text_to_evaluate=newly_generated_text,
                phase_focus_param=phase_focus_str,
                colloquial_level_param=colloquial_level_str,
                emotion_arc_param=emotion_arc_str
            )
            if full_evaluation_of_new_text_result.is_err: # _perform_full_evaluation自体の致命的エラー
                if not self._handle_error(full_evaluation_of_new_text_result.error, loop_idx, results_container, current_version_data_container, is_fatal_loop_error=True):
                    return Result.fail(full_evaluation_of_new_text_result.error)
                error_occurred_in_this_improvement_loop = True
                # current_version_data_container["status"] は _handle_error で設定される
            else: # 評価は実行された（回復可能エラー含む可能性あり）
                full_eval_data_for_this_loop = full_evaluation_of_new_text_result.unwrap()
                current_version_data_container.update(full_eval_data_for_this_loop) # このループの全評価データを格納
                if full_eval_data_for_this_loop.get("llm_eval_success_flag", False):
                    current_version_data_container["status"] = "成功 (改善後評価完了)"
                else: # LLM基本評価が失敗した場合
                    error_occurred_in_this_improvement_loop = True
                    current_version_data_container["status"] = "失敗 (改善後LLM基本評価エラー)"
        
        # ループステータスの最終確認
        if current_version_data_container["status"] == "改善中...": # ステータスが更新されなかった場合
            current_version_data_container["status"] = "失敗 (改善ループで不明な問題)" if error_occurred_in_this_improvement_loop else "完了 (エラーなしでペンディング終了)"
        elif error_occurred_in_this_improvement_loop and "失敗" not in current_version_data_container["status"]:
            current_version_data_container["status"] += " (回復可能エラーあり)"
        
        # このループで生成・評価された full_eval_data を返す
        # (呼び出し元の process メソッドが best_full_eval_data と比較・更新する)
        # current_version_data_container は既に full_eval_data を含んでいる
        return Result.ok(current_version_data_container)


    def _determine_improvement_strategy(self, last_evaluation_data: JsonDict) -> str:
        """
        前回の評価スコアに基づいて改善戦略を決定します。
        戦略の文字列キーを返します (例: "gothic", "readability")。
        """
        aggregated_scores_map = last_evaluation_data.get("aggregated_scores", {})
        if not aggregated_scores_map:
            self.logger.warning("評価スコアが利用できないため、デフォルト戦略「balance」を使用します。")
            return "balance" # デフォルト戦略

        # 各メトリックの「低い」と見なす閾値 (調整可能)
        # スコアが低いほど、その領域の改善優先度が高い
        # overall_qualityが最優先
        overall_quality_score = aggregated_scores_map.get("overall_quality", 0.0)

        # 全体品質が非常に低い場合は、基本的なゴシック要素または可読性に焦点を当てる
        if overall_quality_score < 2.8:
            # ETIが低い場合はゴシック性、そうでなければ可読性を優先
            if aggregated_scores_map.get("eti_total_calculated", 5.0) < 3.0: return "gothic"
            if aggregated_scores_map.get("ri_total_calculated", 5.0) < 3.0: return "readability"
            return "gothic" # 全体品質が低い場合のデフォルト

        # 特定の弱点領域 (スコアが低いほど優先度が高い)
        # (スコア, 戦略キー) のリストを作成
        # キーは_create_*_templateメソッド名や評価指標名と関連付ける
        potential_focus_areas_list: List[Tuple[float, str]] = [
            (aggregated_scores_map.get("eti_total_calculated", 5.0), "gothic"),
            (aggregated_scores_map.get("subjective_score", 5.0), "subjective"),
            (aggregated_scores_map.get("ri_total_calculated", 5.0), "readability"),
            (aggregated_scores_map.get("layer_balance_score", 5.0), "layer_balance"), # 派生スコア
            (aggregated_scores_map.get("phase_score", 5.0), "phase_balance"),       # 派生スコア
            (aggregated_scores_map.get("emotion_arc_score", 5.0), "emotion_arc"),     # 派生スコア
            (aggregated_scores_map.get("colloquial_score", 5.0), "colloquial_blend") # 派生スコア
        ]
        
        # 中程度の閾値 (例: 3.5) を下回る領域をフィルタリングし、スコアでソート (昇順)
        weak_areas_sorted = sorted(
            [(score, name) for score, name in potential_focus_areas_list
             if isinstance(score, (int, float)) and score < 3.5],
            key=lambda x_tuple: x_tuple[0] # スコアでソート
        )

        if weak_areas_sorted:
            # 最もスコアが低い領域に焦点を当てる
            chosen_strategy = weak_areas_sorted[0][1]
            self.logger.info(
                f"改善戦略決定: 最も低いスコアのエリア「{get_metric_display_name(chosen_strategy)}」"
                f"({weak_areas_sorted[0][0]:.2f}) に焦点を当てます。"
            )
            return chosen_strategy
        else:
            # 全ての主要スコアが良好な場合は、全体バランスまたはデフォルト戦略
            self.logger.info("全ての主要スコアが良好です。全体バランス戦略「balance」を選択します。")
            return "balance"

    def _generate_specific_improvement_instructions(
        self, strategy_key: str, last_evaluation_data: JsonDict,
        # 生成パラメータも指示内容に影響する可能性があるため渡す
        perspective_mode: Optional[str], phase_focus: Optional[str],
        colloquial_level: Optional[str], emotion_arc: Optional[str]
    ) -> Result[str, TemplateError]: # TemplateErrorはPart 2で定義
        """
        選択された戦略に基づいて特定の改善指示を生成します。
        対応する _create_*_template メソッドを呼び出します。
        """
        self.logger.debug(f"戦略「{strategy_key}」に基づいた特定改善指示を生成中...")
        
        # 必要な評価結果が利用可能か確認
        # EvaluationResultオブジェクトはJsonDictとして渡されるため、内部のscoresなどを参照
        aggregated_scores_map = last_evaluation_data.get("aggregated_scores", {})
        eti_result_obj = last_evaluation_data.get("eti_result") # EvaluationResultオブジェクト
        ri_result_obj = last_evaluation_data.get("ri_result")
        subjective_result_obj = last_evaluation_data.get("subjective_result")
        phase_distribution_map = last_evaluation_data.get("phase_distribution", {})
        layer_distribution_map = last_evaluation_data.get("layer_distribution", {})

        # EvaluationResultオブジェクトの型チェック
        # (実際には、これらのオブジェクトは_perform_full_evaluationで生成されるため型は保証されているはず)
        if strategy_key == "gothic" and not isinstance(eti_result_obj, EvaluationResult):
            return Result.fail(TemplateError(f"ETI結果が無効なため「{strategy_key}」戦略の指示を生成できません。"))
        if strategy_key == "readability" and not isinstance(ri_result_obj, EvaluationResult):
            return Result.fail(TemplateError(f"RI結果が無効なため「{strategy_key}」戦略の指示を生成できません。"))
        if strategy_key == "subjective" and not isinstance(subjective_result_obj, EvaluationResult):
            return Result.fail(TemplateError(f"主観性評価結果が無効なため「{strategy_key}」戦略の指示を生成できません。"))

        # 対応する _create_*_template メソッドにディスパッチ (Part 13ではスタブ、Part 14で本格実装)
        if strategy_key == "gothic":
            return self._create_gothic_template(eti_result_obj, aggregated_scores_map, phase_distribution_map, colloquial_level)
        elif strategy_key == "readability":
            return self._create_readability_template(ri_result_obj, aggregated_scores_map, phase_distribution_map, colloquial_level)
        elif strategy_key == "subjective":
            return self._create_subjective_template(subjective_result_obj, phase_distribution_map, perspective_mode)
        elif strategy_key == "phase_balance":
            return self._create_phase_template(phase_distribution_map, phase_focus)
        elif strategy_key == "layer_balance":
            layer_derived_score = aggregated_scores_map.get("layer_balance_score", 0.0) # 派生スコア
            return self._create_layer_template(layer_distribution_map, layer_derived_score)
        elif strategy_key == "emotion_arc":
            emotion_derived_score = aggregated_scores_map.get("emotion_arc_score", 0.0) # 派生スコア
            return self._create_emotion_template(emotion_derived_score, emotion_arc)
        elif strategy_key == "colloquial_blend":
            colloquial_derived_score = aggregated_scores_map.get("colloquial_score", 0.0) # 派生スコア
            return self._create_colloquial_template(colloquial_level, colloquial_derived_score)
        elif strategy_key == "balance": # 全体バランス
            return self._create_balance_template(last_evaluation_data, perspective_mode, colloquial_level)
        else:
            self.logger.warning(f"不明な改善戦略「{strategy_key}」。LLM標準指示へのフォールバックを試みます。")
            return Result.ok("") # 空文字列を返し、LLM標準指示をトリガー

    # --- _create_*_template メソッド群のスタブ (Part 14で本格実装) ---
    def _create_gothic_template(self, eti_result: Optional[EvaluationResult], agg_scores: JsonDict, phase_dist: JsonDict, cl_level: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_gothic_template (スタブ) 呼び出し。")
        return Result.ok("# (ゴシック性強化指示スタブ from _create_gothic_template)\n- 象徴的な語彙の使用を増やし、その意味合いを文脈に織り込んでください。\n- 不安、神秘、あるいは両義的な雰囲気を高めるような描写を加えてください。")
    def _create_readability_template(self, ri_result: Optional[EvaluationResult], agg_scores: JsonDict, phase_dist: JsonDict, cl_level: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_readability_template (スタブ) 呼び出し。")
        return Result.ok("# (可読性向上指示スタブ from _create_readability_template)\n- 一文の長さをより意識し、複雑な修飾関係を避けて簡潔にしてください。\n- 段落の構成を見直し、各段落の主題が明確になるようにしてください。")
    def _create_subjective_template(self, subjective_result: Optional[EvaluationResult], phase_dist: JsonDict, perspective: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_subjective_template (スタブ) 呼び出し。")
        return Result.ok(f"# (主観性深化指示スタブ from _create_subjective_template)\n- 視点「{perspective or '指定なし'}」からの主人公の内面描写（思考、感情、知覚）をより具体的に、かつ深く掘り下げてください。\n- モノローグや内省的な記述を効果的に使用し、読者の没入感を高めてください。")
    def _create_phase_template(self, phase_distribution: JsonDict, phase_focus: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_phase_template (スタブ) 呼び出し。")
        return Result.ok(f"# (位相バランス調整指示スタブ from _create_phase_template)\n- 現在の位相分布 ({self._generate_distribution_analysis_text(phase_distribution, '現在の位相')}) を参考に、特に「{phase_focus or 'バランス'}」を意識してセリフ、ナレーション、モノローグのバランスを調整してください。\n- 位相間の移行がより自然で効果的になるように、接続表現や場面転換を工夫してください。")
    def _create_layer_template(self, layer_distribution: JsonDict, current_layer_score: float) -> Result[str, TemplateError]:
        self.logger.debug("_create_layer_template (スタブ) 呼び出し。")
        return Result.ok(f"# (層バランス調整指示スタブ from _create_layer_template)\n- 現在の層バランススコア ({current_layer_score:.1f}) と分布 ({self._generate_distribution_analysis_text(layer_distribution, '現在の層')}) を踏まえ、特に心理層と感覚層の描写を増やすことを検討してください。\n- 各層の描写が互いに連携し、物語全体の深みを増すように意識してください。")
    def _create_emotion_template(self, current_emotion_score: float, emotion_arc_target: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_emotion_template (スタブ) 呼び出し。")
        return Result.ok(f"# (感情変容指示スタブ from _create_emotion_template)\n- 目標とする感情の弧「{emotion_arc_target or '指定なし'}」に沿った感情の変化が、より明確かつ説得力を持って読者に伝わるように表現を調整してください。\n- 感情が変化するきっかけとなる出来事や、登場人物の内的な葛藤を丁寧に描写してください。")
    def _create_colloquial_template(self, colloquial_target_level: Optional[str], current_colloquial_score: float) -> Result[str, TemplateError]:
        self.logger.debug("_create_colloquial_template (スタブ) 呼び出し。")
        return Result.ok(f"# (口語性調整指示スタブ from _create_colloquial_template)\n- 目標とする口語レベル「{colloquial_target_level or 'medium'}」(現在のスコア: {current_colloquial_score:.1f}) に合わせて、文体や語彙のフォーマルさを調整してください。\n- 例えば、よりゴシック的な表現を求める場合は、現代的な口語表現を減らし、格調高い語彙や文語的な言い回しを増やしてください。")
    def _create_balance_template(self, last_eval_data: JsonDict, perspective: Optional[str], cl_level: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_balance_template (スタブ) 呼び出し。")
        # バランス戦略では、複数の低評価項目を指摘できると良い
        agg_scores = last_eval_data.get("aggregated_scores", {})
        low_score_details = []
        for k,v in sorted(agg_scores.items(), key=lambda item: item[1] if isinstance(item[1], (int,float)) else 5.0):
            if isinstance(v, (int,float)) and v < 3.5 and len(low_score_details) < 3: # 上位3つの低評価項目
                low_score_details.append(f"{get_metric_display_name(k)} ({v:.1f})")
        low_scores_str = ", ".join(low_score_details) if low_score_details else "特になし"
        return Result.ok(f"# (全体バランス調整指示スタブ from _create_balance_template)\n- 全体的な品質向上を目指し、特に評価の低い項目（例: {low_scores_str}）を意識しつつ、各評価項目をバランス良く改善してください。\n- 視点「{perspective or '指定なし'}」と口語レベル「{cl_level or 'medium'}」の一貫性も保ってください。")
    # --- End Stub _create_*_template methods ---

    def _generate_llm_improvement_instructions(
        self, current_text_content: str, last_evaluation_data: JsonDict,
        # 生成パラメータもコンテキストとして渡す
        perspective_mode: Optional[str], phase_focus: Optional[str],
        colloquial_level: Optional[str], emotion_arc: Optional[str]
    ) -> Result[str, LLMError]: # LLMErrorはPart 2で定義
        """
        LLMに改善指示自体を生成させます。
        既存の改善テンプレートを流用し、LLMに「改善指示を生成する」ように促します。
        """
        self.logger.debug("LLMによる標準改善指示を生成中...")
        try:
            # 評価結果のサマリを作成 (CompactJSONEncoderはPart 3で定義)
            # 改善プロンプト用の評価サマリJSONを作成 (より詳細に)
            eval_summary_for_llm_instruction_prompt: Dict[str, Any] = {
                "aggregated_scores": last_evaluation_data.get("aggregated_scores"),
                "llm_evaluation_details": {
                    "scores": getattr(last_evaluation_data.get("llm_eval_result"), "scores", {}),
                    "reasoning_preview": {
                        k: truncate_text(v, 70) for k,v in getattr(last_evaluation_data.get("llm_eval_result"), "reasoning", {}).items()
                    }
                },
                "eti_details": getattr(last_evaluation_data.get("eti_result"), "scores", {}),
                "ri_details": getattr(last_evaluation_data.get("ri_result"), "scores", {}),
                "subjective_details": getattr(last_evaluation_data.get("subjective_result"), "scores", {}),
                "phase_distribution": last_evaluation_data.get("phase_distribution"),
                "layer_distribution": last_evaluation_data.get("layer_distribution"),
                "derived_scores": last_evaluation_data.get("derived_scores")
            }
            # Noneや空の辞書を除去して簡潔に
            eval_summary_for_llm_instruction_prompt = {
                k:v for k,v in eval_summary_for_llm_instruction_prompt.items() if v and v != {}
            }
            evaluation_summary_json_str = json.dumps(
                eval_summary_for_llm_instruction_prompt, ensure_ascii=False, indent=2, cls=CompactJSONEncoder
            )

            # 低評価・高評価項目の文字列化 (get_metric_display_nameはPart 3で定義)
            agg_s = last_evaluation_data.get("aggregated_scores", {})
            low_s_str = ", ".join([f"{get_metric_display_name(k)}({v:.1f})" for k,v in agg_s.items() if isinstance(v,(int,float)) and v<3.5]) or "特になし"
            high_s_str = ", ".join([f"{get_metric_display_name(k)}({v:.1f})" for k,v in agg_s.items() if isinstance(v,(int,float)) and v>=4.0]) or "特になし"
            
            # 改善指示生成専用のプロンプトセクションを既存の改善テンプレートに追加するか、
            # 専用の指示生成テンプレートを用意する。
            # ここでは、既存のimprovement_templateを使い、improvement_sectionに指示生成を促す文言を入れる。
            instruction_generation_prompt_text = (
                "上記評価結果と現在のテキストに基づき、テキスト全体の品質を向上させるための、"
                "具体的で実行可能な改善指示を3～5点、箇条書きで提案してください。"
                "各指示は、どの評価項目を改善することを目的としているかを明確に示してください。"
            )
            
            # SafeDictはPart 3で定義
            prompt_format_data = SafeDict({
                "original_text": truncate_text(current_text_content, 800), # 改善対象テキスト
                "evaluation_results_json": evaluation_summary_json_str,
                "low_score_items_str": low_s_str,
                "high_score_items_str": high_s_str,
                "vocabulary_list_str": self.vocab_manager.get_vocabulary_for_prompt(count=10), # 汎用的な語彙
                "perspective_mode": perspective_mode or "指定なし",
                "phase_focus": phase_focus or "指定なし",
                "colloquial_level": colloquial_level or "指定なし",
                "emotion_arc": emotion_arc or "指定なし",
                "layer_distribution_analysis": self._generate_distribution_analysis_text(last_evaluation_data.get("layer_distribution",{}), "層"),
                "phase_distribution_analysis": self._generate_distribution_analysis_text(last_evaluation_data.get("phase_distribution",{}), "位相"),
                "improvement_section": instruction_generation_prompt_text # ここでLLMに指示生成を依頼
            })
            
            final_llm_instruction_prompt = self.improvement_template.format_map(prompt_format_data)
            
            # 指示生成なので温度は低め (例: 0.3)
            llm_instruction_generation_result = self.llm.generate(final_llm_instruction_prompt, temperature=0.3)

            if llm_instruction_generation_result.is_ok:
                generated_instruction_text = llm_instruction_generation_result.unwrap()
                # extract_improvement_instructionsはPart 4で定義
                extracted_instructions_str = extract_improvement_instructions(generated_instruction_text)
                if extracted_instructions_str:
                    self.logger.info("LLMベースの改善指示を正常に生成・抽出しました。")
                    return Result.ok(extracted_instructions_str)
                else: # 抽出失敗時は、LLMの応答全体を指示として使用
                    self.logger.warning("LLM応答から構造化された改善指示を抽出できませんでした。LLMの応答全体を指示として使用します。")
                    return Result.ok(generated_instruction_text.strip())
            else: # LLM呼び出し自体が失敗
                self.logger.error(f"LLMによる標準改善指示の生成に失敗しました: {llm_instruction_generation_result.error}")
                return Result.fail(llm_instruction_generation_result.error) # LLMErrorを伝播
        except TemplateError as e_template_instr: # improvement_templateのフォーマットエラー
            self.logger.error(f"改善指示生成用テンプレートのフォーマットエラー: {e_template_instr}", exc_info=True)
            return Result.fail(LLMError(f"Improvement instruction template error: {e_template_instr}"))
        except Exception as e_unexpected_instr_gen: # その他の予期せぬエラー
            self.logger.error(f"LLM改善指示生成中に予期せぬエラー: {e_unexpected_instr_gen}", exc_info=True)
            return Result.fail(LLMError(f"Unexpected error generating LLM instructions: {e_unexpected_instr_gen}"))

    def _get_contextual_fallback_improvement(self, last_evaluation_data: JsonDict) -> str:
        """LLMや戦略ベースの指示生成が失敗した場合の、文脈に応じたフォールバック指示を生成します。"""
        self.logger.warning("コンテキスト認識型フォールバック改善指示を生成中...")
        aggregated_scores_map = last_evaluation_data.get("aggregated_scores", {})
        
        # 改善対象となりうる主要な評価項目と、その閾値
        # (閾値を下回る項目の中から最も低いものを選ぶ)
        improvement_candidate_thresholds: Dict[str, float] = {
            "eti_total_calculated": 3.2, "ri_total_calculated": 3.2, "subjective_score": 3.5,
            "phase_score": 3.2, "layer_balance_score": 3.2, "emotion_arc_score": 3.0,
            "overall_quality": 3.0 # overall_qualityも対象に含める
        }
        
        weakest_area_key = "balance" # デフォルトは全体バランス
        min_score_found = 5.1 # 閾値より高い初期値

        for eval_key, threshold_val in improvement_candidate_thresholds.items():
            current_score = aggregated_scores_map.get(eval_key, 5.0) # 見つからなければペナルティなし
            if isinstance(current_score, (int, float)) and current_score < threshold_val:
                if current_score < min_score_found:
                    min_score_found = current_score
                    weakest_area_key = eval_key # 最も低いスコアのキーを更新
        
        self.logger.debug(f"フォールバック改善: 最もスコアの低いエリアは「{get_metric_display_name(weakest_area_key)}」(スコア: {min_score_found:.2f})")
        
        # 戦略キーへのマッピング (より汎用的な指示のため)
        strategy_map_for_fallback: Dict[str, str] = {
            "eti_total_calculated": "gothic", "ri_total_calculated": "readability",
            "subjective_score": "subjective", "phase_score": "phase_balance",
            "layer_balance_score": "layer_balance", "emotion_arc_score": "emotion_arc",
            "overall_quality": "balance" # overall_qualityが低い場合はバランス戦略
        }
        fallback_strategy_key = strategy_map_for_fallback.get(weakest_area_key, "balance")

        # 各戦略に対応する汎用的な指示 (NGGSConfigのDEFAULT_FALLBACK_IMPROVEMENTをベースに調整)
        base_fallback_instruction = self.config.DEFAULT_FALLBACK_IMPROVEMENT
        specific_focus_instruction: str = ""
        # _create_*_templateスタブを呼び出す代わりに、ここで簡易的な指示を生成
        if fallback_strategy_key == "gothic": specific_focus_instruction = "特に、ゴシック的な雰囲気と存在論的震え（ETI）の要素を強化してください。"
        elif fallback_strategy_key == "readability": specific_focus_instruction = "特に、文章の明瞭性とリズムを改善し、読者の認知負荷を軽減してください。"
        elif fallback_strategy_key == "subjective": specific_focus_instruction = "特に、主人公の内的描写を深め、読者の没入感を高めるようにしてください。"
        # 他の戦略も同様に追加可能

        final_fallback_instructions = f"{base_fallback_instruction}\n\n{specific_focus_instruction}"
        
        # 関連語彙の追加
        # VocabularyManager.VALID_LAYERS と fallback_strategy_key (または関連するレイヤー名) を照合
        related_vocab_layer: Optional[str] = None
        if fallback_strategy_key == "gothic": related_vocab_layer = "symbolic" # 例
        elif fallback_strategy_key == "subjective": related_vocab_layer = "psychological"
        
        relevant_vocab_str = self.vocab_manager.get_vocabulary_for_prompt(
            count=10,
            layer=related_vocab_layer if related_vocab_layer in self.vocab_manager.VALID_LAYERS else None,
            category=fallback_strategy_key if fallback_strategy_key not in self.vocab_manager.VALID_LAYERS else None # カテゴリ名としても試行
        )
        if relevant_vocab_str:
            final_fallback_instructions += f"\n\n関連語彙の提案: {relevant_vocab_str}"
        
        return final_fallback_instructions.strip()

    def _generate_distribution_analysis_text(self, distribution_data_map: Optional[Dict[str, Any]], distribution_type_name: str) -> str:
        """分布辞書（位相または層）の簡単なテキスト要約を生成します。"""
        if not distribution_data_map or not isinstance(distribution_data_map, dict):
            return f"{distribution_type_name}分布データが利用できません。"
        
        # 有効な数値の比率を持つ要素のみをフィルタリング
        valid_distribution_elements = [
            (key_str, val_float) for key_str, val_float in distribution_data_map.items()
            if isinstance(val_float, (int, float)) and val_float > 0.001 # 0.1%以上の要素のみ
        ]
        if not valid_distribution_elements:
            return f"{distribution_type_name}分布: 有効な要素が見つかりません。"
            
        # 値（比率）の降順でソート
        sorted_distribution_elements = sorted(valid_distribution_elements, key=lambda item_tuple: item_tuple[1], reverse=True)
        
        # 上位3要素まで表示 (2%以上のもの)
        # get_metric_display_nameはPart 3で定義
        display_parts_list = [
            f"{get_metric_display_name(key_str)} ({val_float*100:.1f}%)"
            for key_str, val_float in sorted_distribution_elements if val_float > 0.02
        ]
        
        if not display_parts_list:
            return f"{distribution_type_name}分布: 主要な構成要素（2%超）が見つかりません。"
        
        return f"{distribution_type_name}分布 - 主な要素: {', '.join(display_parts_list[:3])}"


# =============================================================================
# Part 13 End: TextProcessor Improvement Loop and Instruction Generation
# =============================================================================
# =============================================================================
# Part 14: TextProcessor Instruction Templates and Text Generation Method
# (Continues TextProcessor class from Part 13)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, LLMClient, EvaluationResult, Evaluator, VocabularyManager,
# NarrativeFlowFramework, ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator,
# Result, NGGSError, ConfigurationError, TemplateError, LLMError, GenerationError,
# JsonDict, CompactJSONEncoder, get_metric_display_name, truncate_text,
# extract_improvement_instructions, validate_template, SafeDict など。

import logging
import json # _generate_text で評価サマリをJSON化するため
from typing import (
    Dict, List, Any, Optional, Tuple, Callable, Union # Part 1からの型も含む
)

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError
# from nggs_lite_part2 import Result, NGGSError, LLMError, EvaluationError, GenerationError, TemplateError
# from nggs_lite_part3 import CompactJSONEncoder, get_metric_display_name, truncate_text, SafeDict
# from nggs_lite_part4 import LLMClient, validate_template, extract_improvement_instructions # LLMClientはPart 5で完成
# from nggs_lite_part6 import VocabularyManager
# from nggs_lite_part8 import EvaluationResult, Evaluator # BaseEvaluatorは直接使用しない
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part11 import logger_text_processor # TextProcessorのロガー
# from nggs_lite_part12 import TextProcessor # TextProcessorクラス定義を開始するため
# from nggs_lite_part13 import TextProcessor # TextProcessorクラス定義を継続するため

# logger_text_processor はPart 11で定義済みと仮定
# logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")

class TextProcessor:
    # --- __init__ and other methods from Part 11, 12, 13 ---
    # これらのメソッドは以前のパートで定義済みであると仮定し、ここでは簡略化して再掲または省略します。
    # 完全なスクリプトでは、これらがこのクラス定義内に存在します。
    def __init__(
        self, config: NGGSConfig, llm_client: LLMClient, evaluator: Evaluator,
        vocab_manager: VocabularyManager, narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator, ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator, generation_template: str,
        improvement_template: str, ndgs_parser: Optional[Any] = None
    ):
        self.logger = logger_text_processor
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TP: NGGSConfig無効")
        if not isinstance(llm_client, LLMClient): raise ConfigurationError("TP: LLMClient無効")
        if not isinstance(evaluator, Evaluator): raise ConfigurationError("TP: Evaluator無効")
        if not isinstance(vocab_manager, VocabularyManager): raise ConfigurationError("TP: VocabularyManager無効")
        if not isinstance(narrative_flow, NarrativeFlowFramework): raise ConfigurationError("TP: NarrativeFlowFramework無効")
        if not isinstance(eti_evaluator, ExtendedETIEvaluator): raise ConfigurationError("TP: ExtendedETIEvaluator無効")
        if not isinstance(ri_evaluator, RIEvaluator): raise ConfigurationError("TP: RIEvaluator無効")
        if not isinstance(subjective_evaluator, SubjectiveEvaluator): raise ConfigurationError("TP: SubjectiveEvaluator無効")
        self.config: NGGSConfig = config
        self.llm: LLMClient = llm_client
        self.evaluator: Evaluator = evaluator
        self.vocab_manager: VocabularyManager = vocab_manager
        self.narrative_flow: NarrativeFlowFramework = narrative_flow
        self.eti_evaluator: ExtendedETIEvaluator = eti_evaluator
        self.ri_evaluator: RIEvaluator = ri_evaluator
        self.subjective_evaluator: SubjectiveEvaluator = subjective_evaluator
        self.ndgs_parser: Optional[Any] = ndgs_parser
        gen_res = validate_template(generation_template, ["input_text","target_length"]); imp_res = validate_template(improvement_template, ["original_text","evaluation_results_json"])
        if gen_res.is_err: raise gen_res.error # type: ignore
        if imp_res.is_err: raise imp_res.error # type: ignore
        self.generation_template: str = generation_template
        self.improvement_template: str = improvement_template
        self.logger.info(f"TextProcessor初期化 (v{config.VERSION})")

    def _get_default_aggregated_scores(self) -> JsonDict: return {} # Part 11で実装済み, 簡略化
    def _perform_full_evaluation(self, t:str, pf:str, cl:str, ea:Optional[str]) -> Result[JsonDict, EvaluationError]: return Result.ok({}) # Part 11で実装済み, 簡略化
    def _perform_initial_generation(self,ict:str,tl:int,pm:str,pf:str,cl:str,ea:Optional[str],nfp:Optional[str])->Result[str,LLMError]: return Result.ok("stub") # Part 12で実装済み, 簡略化
    def process(self, it:str, tl_o:Optional[int]=None, pm_o:Optional[str]=None, pf_o:Optional[str]=None, cl_o:Optional[str]=None, ea_o:Optional[str]=None, ml_o:Optional[int]=None, th_o:Optional[float]=None, nfp_o:Optional[str]=None, sigf:bool=False, ndgs_id:Optional[JsonDict]=None) -> Result[JsonDict, NGGSError]: pass # Part 12で骨子実装済み, ここでは省略
    def _handle_error(self,e:Exception,lidx:int,rd:JsonDict,vd:JsonDict,ifl:bool=False)->bool: return True # Part 12で実装済み, 簡略化
    def _finalize_results(self,rd:JsonDict,bt:str,bfed:JsonDict,fs:str)->None: pass # Part 12で実装済み, ここでは簡略化
    def _perform_improvement_loop(self, lidx:int, cbt:str, cbed:JsonDict, rc:JsonDict, vd:JsonDict, ftl:int, pm:str, pf:str, cl:str, ea:Optional[str], nfp:Optional[str]) -> Result[JsonDict, NGGSError]: return Result.ok({}) # Part 13で実装済み, 簡略化
    def _determine_improvement_strategy(self, led:JsonDict) -> str: return "balance" # Part 13で実装済み, 簡略化
    def _generate_specific_improvement_instructions(self, sk:str, led:JsonDict, pm:Optional[str], pf:Optional[str], cl:Optional[str], ea:Optional[str]) -> Result[str,TemplateError]: return Result.ok("") # Part 13で実装済み, 簡略化
    def _generate_llm_improvement_instructions(self, ct:str, led:JsonDict, pm:Optional[str], pf:Optional[str], cl:Optional[str], ea:Optional[str]) -> Result[str,LLMError]: return Result.ok("") # Part 13で実装済み, 簡略化
    def _get_contextual_fallback_improvement(self, led:JsonDict) -> str: return "" # Part 13で実装済み, 簡略化
    def _generate_distribution_analysis_text(self, ddm:Optional[Dict[str,Any]], dtn:str) -> str: return "" # Part 13で実装済み, 簡略化
    # --- End of Methods from Part 11, 12, 13 ---

    # --- _create_*_template methods (Full Implementations for Part 14) ---
    # これらのメソッドは、各改善戦略に応じた詳細な指示文字列を生成します。
    # 入力として前回の評価結果オブジェクトや関連パラメータを受け取ります。
    # 返り値は Result[str, TemplateError] です。

    def _create_gothic_template(
        self,
        eti_result_obj: Optional[EvaluationResult], # ETI評価結果オブジェクト
        aggregated_scores: JsonDict,        # 全体の集約スコア
        phase_distribution: JsonDict,       # 位相分布
        colloquial_level: Optional[str]     # 設定された口語レベル
    ) -> Result[str, TemplateError]:
        """ゴシック性（ETI）強化に焦点を当てた改善指示を生成します。"""
        self.logger.debug("ゴシック性強化テンプレートを生成中...")
        if not isinstance(eti_result_obj, EvaluationResult) or not isinstance(eti_result_obj.scores, dict):
            return Result.fail(TemplateError("ETI評価結果のスコアデータが不正です。"))

        instructions_list: List[str] = ["# ゴシック性（ETI）強化のための具体的な改善指示:"]
        eti_component_scores = eti_result_obj.scores
        
        # ETI構成要素のスコアを取得し、低いもの（例: 3.5未満）を特定
        # self.config.EXTENDED_ETI_WEIGHTS のキーを元にループ
        weak_eti_components: List[Tuple[str, float]] = []
        for eti_key_str in self.config.EXTENDED_ETI_WEIGHTS.keys():
            score_val = eti_component_scores.get(eti_key_str)
            if isinstance(score_val, (int, float)) and score_val < 3.5:
                weak_eti_components.append((eti_key_str, score_val))
        
        weak_eti_components.sort(key=lambda item_tuple: item_tuple[1]) # スコアが低い順にソート

        if weak_eti_components:
            instructions_list.append("\n## 特に強化すべきETI要素:")
            for component_key_str, component_score_val in weak_eti_components[:2]: # 上位2つの弱点を指摘
                display_name_str = get_metric_display_name(component_key_str) # Part 3で定義
                guidance_text_map: Dict[str, str] = {
                    "境界性": "光と闇、生と死、現実と夢、内と外といった二項対立の「境界」を曖昧にし、その狭間や移行領域を具体的に描写してください。読者の現実感を揺さぶるような表現を試みましょう。",
                    "両義性": "相反する感情（例：魅惑と恐怖、崇高とグロテスク、聖性と冒涜）が同時に存在するような「両義的」な状況やキャラクターを描写してください。単純な善悪二元論を避け、複雑な心理描写を。",
                    "超越侵犯": "日常の規範、倫理、あるいは物理法則の限界を「超越」または「侵犯」するような要素（狂気、異形なるもの、禁忌の探求、超自然現象など）を導入または強化し、読者に衝撃を与えてください。",
                    "不確定性": "物語の全貌や真相を容易には明らかにせず、「不確定」な要素や謎を残してください。暗示的な表現、信頼できない語り手、未解決の伏線などを活用し、読者の想像力を刺激しましょう。",
                    "内的変容": "登場人物の認識、記憶、アイデンティティ、あるいは精神状態が、外的要因や内的葛藤によって「変容」していく過程を、心理描写を重視して深く描写してください。",
                    "位相移行": "ゴシック的な雰囲気を高める形で、語りの位相（特にモノローグや象徴的なナレーション、あるいは過去への回想）への「移行」を効果的に使用し、物語に奥行きを与えてください。",
                    "主観性": "「主観的」な恐怖、不安、混乱、あるいは特異な感覚体験（幻覚、既視感など）を、より鮮烈に、読者が登場人物の視点から追体験できるような一人称的描写を強化してください。"
                }
                instructions_list.append(
                    f"- **{display_name_str} (現在スコア: {component_score_val:.1f})**: {guidance_text_map.get(component_key_str, f'{display_name_str}に関連する表現を強化してください。')}"
                )
                # 関連語彙の提案 (VocabularyManagerはPart 7でゲッター実装済み)
                suggested_vocab_str = self.vocab_manager.get_vocabulary_by_eti_category(component_key_str, count=3)
                if suggested_vocab_str:
                    instructions_list.append(f"  - 推奨語彙例 ({display_name_str}関連): {suggested_vocab_str}")
        else: # 全体的にETIスコアが良い場合
            instructions_list.append("\n- ETIの各要素は比較的良好です。現在の質を維持しつつ、特に「両義性」や「不確定性」の表現をさらに洗練させ、物語に深みと多層的な解釈の可能性を加えてください。")

        # 層バランスに関する指示 (ゴシック性の観点から)
        layer_balance_derived_score = aggregated_scores.get("layer_balance_score", 3.0) # 派生スコア
        if layer_balance_derived_score < 3.2:
            instructions_list.append("\n## 層バランスの調整（ゴシック性強化のため）:")
            instructions_list.append(
                "- ゴシック的雰囲気を深化させるためには、特に「象徴層」（隠された意味、運命、呪いなど）と「心理層」（内面の葛藤、恐怖、狂気など）の描写を増やし、"
                "それらを「感覚層」（不気味な音、冷たい感触、暗い光景など）と効果的に連携させてください。"
            )
            # layer_distributionのサマリを提示
            layer_dist_text = self._generate_distribution_analysis_text(last_evaluation_data.get("layer_distribution",{}), "現在の層")
            instructions_list.append(f"  - {layer_dist_text}")


        # 口語レベルに応じた文体調整の提案 (ゴシック性の観点から)
        safe_colloquial_level = colloquial_level or "medium"
        if safe_colloquial_level != "low": # 口語レベルが「低」(つまり文語的)でない場合
            instructions_list.append("\n## 文体調整（ゴシック性の強調）:")
            instructions_list.append(
                f"- 現在の口語レベル設定（{safe_colloquial_level}）を考慮しつつも、ゴシック文体の荘重さや雰囲気を高めるために、"
                "特にナレーションや内省的な場面では、より格調高い文語的表現や古風な言い回しを適度に試みてください。"
                "ただし、過度に難解にならないよう注意してください。"
            )

        instructions_list.append("\n## 推奨ゴシック語彙（総合）:")
        instructions_list.append("- 以下の語彙も全体を通して文脈に合わせて効果的に使用し、ゴシック的な世界観を構築してください。")
        general_gothic_vocab = self.vocab_manager.get_vocabulary_for_prompt(count=12, category="神秘", tags=["gothic", "dark"], layer="symbolic")
        instructions_list.append(f"  - {general_gothic_vocab or '（廃墟、深淵、月影、古城、血痕、契約、魂、永遠など）'}")
        
        return Result.ok("\n".join(instructions_list))

    # 他の _create_*_template メソッドも同様に、具体的な指示内容を充実させる
    # (以下、主要なポイントのみ記述し、完全な実装は省略するが、上記_create_gothic_templateと同様の粒度で実装する想定)

    def _create_readability_template(self, ri_result_obj: Optional[EvaluationResult], agg_scores: JsonDict, phase_dist: JsonDict, cl_level: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_readability_templateを生成中...")
        if not isinstance(ri_result_obj, EvaluationResult) or not isinstance(ri_result_obj.scores, dict):
            return Result.fail(TemplateError("RI評価結果のスコアデータが不正です。"))
        instructions: List[str] = ["# 可読性（RI）向上指示:"]
        # RI構成要素で低いものを指摘し、具体的な改善策を提示
        # 例: clarityが低ければ「一文を短く」「接続詞を効果的に」など
        # visualRhythmが低ければ「段落長に変化を」「適度な改行」など
        # cognitiveLoadが高ければ「難解語を避ける」「漢字比率調整」など
        # ... (具体的な指示内容は省略) ...
        instructions.append("- (可読性向上のための詳細指示はここに生成されます)")
        return Result.ok("\n".join(instructions))

    def _create_subjective_template(self, subjective_result_obj: Optional[EvaluationResult], phase_dist: JsonDict, perspective: Optional[str]) -> Result[str, TemplateError]:
        self.logger.debug("_create_subjective_templateを生成中...")
        if not isinstance(subjective_result_obj, EvaluationResult) or not isinstance(subjective_result_obj.components, dict): # componentsを参照
            return Result.fail(TemplateError("主観性評価結果のコンポーネントデータが不正です。"))
        instructions: List[str] = ["# 主観的語り（没入感）深化指示:"]
        # SubjectiveEvaluatorの構成要素スコアに基づき指示生成
        # 例: first_person_scoreが低ければ「一人称視点を強化」
        # inner_expression_scoreが低ければ「内的表現を具体的に」
        # monologue_quality_scoreが低ければ「モノローグを効果的に」
        # consistency_scoreが低ければ「視点の一貫性確保」など
        # ... (具体的な指示内容は省略) ...
        instructions.append("- (主観性深化のための詳細指示はここに生成されます)")
        return Result.ok("\n".join(instructions))

    # _create_phase_template, _create_layer_template, _create_emotion_template,
    # _create_colloquial_template, _create_balance_template も同様に本格実装する。
    # 各メソッドは、関連する評価スコアや分布、設定パラメータを引数に取り、
    # それに基づいて具体的な改善指示（箇条書きなど）と、関連する語彙の提案を含む文字列を生成する。
    # ここでは、これらのメソッドの完全な実装は省略し、スタブのままとしておく。
    # （実際のプロジェクトでは、これらのメソッドを上記_create_gothic_templateと同様の粒度で実装する必要がある）

    def _create_phase_template(self, phase_distribution: JsonDict, phase_focus: Optional[str]) -> Result[str, TemplateError]:
        return Result.ok(f"# (位相バランス調整指示スタブ from _create_phase_template)\n- 現在の位相分布 ({self._generate_distribution_analysis_text(phase_distribution, '現在の位相')}) を参考に、特に「{phase_focus or 'バランス'}」を意識してセリフ、ナレーション、モノローグのバランスを調整してください。\n- 位相間の移行がより自然で効果的になるように、接続表現や場面転換を工夫してください。")
    def _create_layer_template(self, layer_distribution: JsonDict, current_layer_score: float) -> Result[str, TemplateError]:
        return Result.ok(f"# (層バランス調整指示スタブ from _create_layer_template)\n- 現在の層バランススコア ({current_layer_score:.1f}) と分布 ({self._generate_distribution_analysis_text(layer_distribution, '現在の層')}) を踏まえ、特に心理層と感覚層の描写を増やすことを検討してください。\n- 各層の描写が互いに連携し、物語全体の深みを増すように意識してください。")
    def _create_emotion_template(self, current_emotion_score: float, emotion_arc_target: Optional[str]) -> Result[str, TemplateError]:
        return Result.ok(f"# (感情変容指示スタブ from _create_emotion_template)\n- 目標とする感情の弧「{emotion_arc_target or '指定なし'}」に沿った感情の変化が、より明確かつ説得力を持って読者に伝わるように表現を調整してください。\n- 感情が変化するきっかけとなる出来事や、登場人物の内的な葛藤を丁寧に描写してください。")
    def _create_colloquial_template(self, colloquial_target_level: Optional[str], current_colloquial_score: float) -> Result[str, TemplateError]:
        return Result.ok(f"# (口語性調整指示スタブ from _create_colloquial_template)\n- 目標とする口語レベル「{colloquial_target_level or 'medium'}」(現在のスコア: {current_colloquial_score:.1f}) に合わせて、文体や語彙のフォーマルさを調整してください。\n- 例えば、よりゴシック的な表現を求める場合は、現代的な口語表現を減らし、格調高い語彙や文語的な言い回しを増やしてください。")
    def _create_balance_template(self, last_eval_data: JsonDict, perspective: Optional[str], cl_level: Optional[str]) -> Result[str, TemplateError]:
        agg_scores = last_eval_data.get("aggregated_scores", {}); low_s_details = [];
        for k,v in sorted(agg_scores.items(), key=lambda item: item[1] if isinstance(item[1],(int,float)) else 5.0):
            if isinstance(v,(int,float)) and v < 3.5 and len(low_s_details) < 3: low_s_details.append(f"{get_metric_display_name(k)} ({v:.1f})")
        low_s_str = ", ".join(low_s_details) if low_s_details else "特になし"
        return Result.ok(f"# (全体バランス調整指示スタブ from _create_balance_template)\n- 全体的な品質向上を目指し、特に評価の低い項目（例: {low_s_str}）を意識しつつ、各評価項目をバランス良く改善してください。\n- 視点「{perspective or '指定なし'}」と口語レベル「{cl_level or 'medium'}」の一貫性も保ってください。")
    # --- End _create_*_template methods ---


    # --- Text Generation Method (Full Implementation for Part 14) ---
    def _generate_text(
        self,
        input_text_or_context: str, # 初期生成: コンテキスト, 改善生成: 前回の最良テキスト
        is_improvement: bool,       # Trueなら改善、Falseなら初期生成
        target_length: int,
        # スタイルパラメータ群
        perspective_mode: str,
        phase_focus: str,
        colloquial_level: str,
        emotion_arc: Optional[str],
        # 初期生成用
        narrative_flow_prompt: Optional[str], # NarrativeFlowFrameworkからの物語構成指示
        # 改善生成用
        evaluation_summary_json_str: Optional[str] = None, # 前ループの評価結果サマリ(JSON文字列)
        low_score_items_summary_str: Optional[str] = None,
        high_score_items_summary_str: Optional[str] = None,
        layer_distribution_analysis_str: Optional[str] = None,
        phase_distribution_analysis_str: Optional[str] = None,
        improvement_section_content_str: Optional[str] = None, # _create_*_template等からの改善指示本文
        # 温度
        temperature_override: Optional[float] = None
    ) -> Result[str, LLMError]: # LLMErrorはPart 2で定義
        """
        コアなテキスト生成メソッド。適切なプロンプトをフォーマットし、LLMを呼び出します。
        初期生成と改善生成の両方に対応します。
        """
        self.logger.info(
            f"コアテキスト生成メソッド呼び出し。タイプ: {'改善' if is_improvement else '初期'}, "
            f"目標長: {target_length}, 温度オーバーライド: {temperature_override if temperature_override is not None else 'なし'}"
        )

        final_prompt_str: str
        template_to_use_str: str
        prompt_parameters_map: Dict[str, Any] # SafeDictで使用するパラメータマップ

        # 語彙リストの取得 (改善提案: is_improvementに応じて内容を変える)
        # ここでは汎用的なものを取得するが、改善時は戦略に応じた語彙が良い
        # (これは_perform_improvement_loopで準備し、引数で渡すのが望ましい)
        # 今回は、improvement_section_content_strに語彙提案が含まれると仮定するか、
        # あるいは、ここで戦略キーを受け取って語彙を出し分ける。
        # プランでは、_create_*_templateが語彙提案を含むため、ここでは汎用でよい。
        vocabulary_for_prompt = self.vocab_manager.get_vocabulary_for_prompt(
            count=20, # プロンプトに含める語彙数
            # layer=strategy_key if is_improvement and strategy_key in self.vocab_manager.VALID_LAYERS else None
        )

        if is_improvement:
            template_to_use_str = self.improvement_template
            prompt_parameters_map = {
                "original_text": truncate_text(input_text_or_context, 1000), # 改善対象テキスト (長すぎるとプロンプトサイズを超えるため切り詰め)
                "evaluation_results_json": evaluation_summary_json_str or "{}",
                "low_score_items_str": low_score_items_summary_str or "特になし",
                "high_score_items_str": high_score_items_summary_str or "特になし",
                "vocabulary_list_str": vocabulary_for_prompt or "(関連語彙の提案なし)",
                "perspective_mode": perspective_mode,
                "phase_focus": phase_focus,
                "colloquial_level": colloquial_level,
                "emotion_arc": emotion_arc or "指定なし",
                "layer_distribution_analysis": layer_distribution_analysis_str or "(層分布分析なし)",
                "phase_distribution_analysis": phase_distribution_analysis_str or "(位相分布分析なし)",
                "improvement_section": improvement_section_content_str or self.config.DEFAULT_FALLBACK_IMPROVEMENT
            }
            self.logger.debug(f"改善プロンプトパラメータ準備完了。指示セクションのプレビュー: {truncate_text(improvement_section_content_str, 100)}")
        else: # 初期生成
            template_to_use_str = self.generation_template
            prompt_parameters_map = {
                "input_text": input_text_or_context, # 初期コンテキスト
                "target_length": target_length,
                "perspective_mode": perspective_mode,
                "phase_focus": phase_focus,
                "colloquial_level": colloquial_level,
                "emotion_arc": emotion_arc or "指定なし",
                "vocabulary_list_str": vocabulary_for_prompt or "(関連語彙の提案なし)",
                "narrative_flow_section": narrative_flow_prompt or "（特定の物語構成指示なし）"
                # 改善テンプレート用のキーはSafeDictが処理するので、ここでは不要
            }
            self.logger.debug("初期生成プロンプトパラメータ準備完了。")

        try:
            # SafeDictはPart 3で定義
            final_prompt_str = template_to_use_str.format_map(SafeDict(prompt_parameters_map))
        except KeyError as e_key_format:
            error_message = f"{'改善' if is_improvement else '初期'}生成テンプレートのフォーマットエラー、キーが見つかりません: {e_key_format}"
            self.logger.error(error_message)
            # TemplateErrorはPart 2で定義
            return Result.fail(TemplateError(error_message, details={"missing_key": str(e_key_format)}))
        except Exception as e_format_unexpected:
            error_message = f"{'改善' if is_improvement else '初期'}生成テンプレートのフォーマット中に予期せぬエラー: {e_format_unexpected}"
            self.logger.error(error_message, exc_info=True)
            return Result.fail(TemplateError(error_message))

        # 温度設定: 引数 > (改善時ならループ調整温度) > configのデフォルト
        final_temperature_for_llm: float
        if temperature_override is not None:
            final_temperature_for_llm = temperature_override
        elif is_improvement:
            # 改善ループの場合、温度は _perform_improvement_loop で計算され、
            # temperature_overrideとして渡される想定。
            # もしNoneなら、configの改善ベース温度を使用。
            final_temperature_for_llm = self.config.IMPROVEMENT_BASE_TEMPERATURE
            self.logger.warning("_generate_textが改善モードで呼び出されましたが、温度オーバーライドがNoneです。configの改善ベース温度を使用します。")
        else: # 初期生成
            final_temperature_for_llm = self.config.GENERATION_CONFIG_DEFAULT.get('temperature', 0.8)

        self.logger.debug(f"LLM呼び出し実行: 温度={final_temperature_for_llm:.2f}, プロンプト長={len(final_prompt_str)}")
        if len(final_prompt_str) > 2500: # 長大なプロンプトのプレビュー
            self.logger.debug(f"長文プロンプトプレビュー（先頭と末尾）: {truncate_text(final_prompt_str, 150)} ... {final_prompt_str[-150:]}")

        # LLM呼び出し (LLMClient.generateはPart 5で定義)
        llm_generation_result = self.llm.generate(final_prompt_str, temperature=final_temperature_for_llm)

        if llm_generation_result.is_ok:
            generated_text_output = llm_generation_result.unwrap()
            # オプション: 基本的な長さ制御 (LLMがmax_output_tokensを尊重するなら不要な場合も)
            # self.logger.debug(f"LLMからの生出力長: {len(generated_text_output)}")
            # if len(generated_text_output) > target_length * 1.8: # 大幅に長い場合
            #     self.logger.warning(f"生成テキストが目標長({target_length})より大幅に長いため({len(generated_text_output)})、切り詰めます。")
            #     generated_text_output = generated_text_output[:int(target_length * 1.5)]
            # elif not is_improvement and len(generated_text_output) < target_length * 0.5: # 初期生成で大幅に短い場合
            #     self.logger.warning(f"生成テキストが目標長({target_length})より大幅に短いです({len(generated_text_output)})。")

            self.logger.info(f"{'改善' if is_improvement else '初期'}テキスト生成成功。最終的な長さ: {len(generated_text_output)}")
            return Result.ok(generated_text_output)
        else: # LLM呼び出し失敗
            self.logger.error(f"{'改善' if is_improvement else '初期'}テキスト生成に失敗しました: {llm_generation_result.error}")
            return Result.fail(llm_generation_result.error) # LLMErrorをそのまま伝播

# =============================================================================
# Part 14 End: TextProcessor Instruction Templates and Text Generation Method
# =============================================================================
# =============================================================================
# Part 15: Integration Components (NDGS Parser & GLCAI Feedback) [v1.8]
# (Continues TextProcessor class definition, adds NDGSIntegration and GLCAIVocabularyFeedback)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, Result, NGGSError, IntegrationError, FileProcessingError, JsonDict,
# VocabularyManager, VocabularyItem, safe_read_file, safe_write_file, CompactJSONEncoder,
# TextProcessor (既存のメソッドは簡略化して示す) など。

import logging
import json
import pathlib
import re # GLCAIVocabularyFeedbackのjob_idサニタイズ用
from typing import (
    Dict, List, Any, Optional, Union, Set, Tuple, Callable # Part 1からの型も含む
)
from datetime import datetime, timezone # GLCAIVocabularyFeedbackのタイムスタンプ用

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError, BASE_DIR
# from nggs_lite_part2 import Result, NGGSError, IntegrationError, FileProcessingError
# from nggs_lite_part3 import safe_read_file, safe_write_file, CompactJSONEncoder, truncate_text, get_metric_display_name
# from nggs_lite_part4 import LLMClient, TemplateError, validate_template, extract_improvement_instructions
# from nggs_lite_part6 import VocabularyManager, VocabularyItem
# from nggs_lite_part8 import EvaluationResult, Evaluator
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part11 import logger_text_processor
# from nggs_lite_part14 import TextProcessor # TextProcessorクラス定義を開始/継続するため

# logger_text_processor はPart 11で定義済みと仮定
# logger_text_processor = logging.getLogger("NGGS-Lite.TextProcessor")


# --- NDGS Integration (Basic Parser) ---
class NDGSIntegration:
    """
    NDGS出力データ（JSON形式）の基本的な解析を処理し、NGGS-Lite v1.8用の
    最小限のコンテキストを抽出します。
    v1.8スコープ: ファイルベースのJSON入力を想定。必須のキャラクターおよび
    シーン情報の抽出に焦点を当てます。基本的な検証とエラー処理を含みます。
    """
    def __init__(self, config: NGGSConfig):
        """
        NDGSIntegrationコンポーネントを初期化します。

        Args:
            config: NGGSConfigオブジェクト。
        """
        if not isinstance(config, NGGSConfig): # NGGSConfigはPart 1で定義
            raise ConfigurationError("NDGSIntegrationには有効なNGGSConfigインスタンスが必要です。")
        self.config: NGGSConfig = config
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.NDGSIntegration")
        # 将来的にJSON Schema検証オブジェクトを使用する場合のプレースホルダー
        # self.schema_validator = load_schema(...)
        self.logger.info("NDGSIntegration (v1.8 Basic Parser) が初期化されました。")

    def parse_from_file(self, file_path: Union[str, pathlib.Path]) -> Result[JsonDict, IntegrationError]: # IntegrationErrorはPart 2で定義
        """
        指定されたJSONファイルからNDGSデータを解析します。

        Args:
            file_path: NDGS出力JSONファイルへのパス。

        Returns:
            解析されたコンテキストを含む辞書 (Ok)、またはIntegrationError (Err)。
        """
        self.logger.info(f"ファイルからNDGSデータの解析を試行: {file_path}")
        # safe_read_fileはPart 3で定義
        read_result = safe_read_file(file_path)
        if read_result.is_err:
            # safe_read_fileからのエラーをIntegrationErrorでラップ
            original_file_error = read_result.error # FileProcessingErrorのはず
            integration_err = IntegrationError(
                f"NDGS入力ファイル '{file_path}' の読み込みに失敗しました: {original_file_error}",
                details={"path": str(file_path), "original_error_type": type(original_file_error).__name__}
            )
            self.logger.error(str(integration_err))
            return Result.fail(integration_err)

        try:
            ndgs_data_content_str = read_result.unwrap()
            if not ndgs_data_content_str or not ndgs_data_content_str.strip(): # 空ファイルの内容を処理
                empty_file_err = IntegrationError("NDGS入力ファイルが空です。", details={"path": str(file_path)})
                self.logger.error(str(empty_file_err))
                return Result.fail(empty_file_err)

            ndgs_json_data = json.loads(ndgs_data_content_str)
            if not isinstance(ndgs_json_data, dict):
                invalid_json_err = IntegrationError(
                    "NDGS入力ファイルに有効なJSONオブジェクトが含まれていません。",
                    details={"path": str(file_path), "parsed_data_type": str(type(ndgs_json_data))}
                )
                self.logger.error(str(invalid_json_err))
                return Result.fail(invalid_json_err)

            return self.parse(ndgs_json_data)  # メインのparseメソッドに委譲

        except json.JSONDecodeError as e_json_decode:
            json_decode_err = IntegrationError(
                f"ファイル '{file_path}' からのNDGS JSONデータの解析に失敗しました: {e_json_decode.msg} at pos {e_json_decode.pos}",
                details={"path": str(file_path), "json_error_message": e_json_decode.msg, "position": e_json_decode.pos}
            )
            self.logger.error(str(json_decode_err))
            return Result.fail(json_decode_err)
        except Exception as e_unexpected_parse_file: # その他の予期せぬエラー
            unexpected_err = IntegrationError(
                f"NDGSファイル '{file_path}' の解析中に予期せぬエラー: {e_unexpected_parse_file}",
                details={"path": str(file_path), "exception_type": type(e_unexpected_parse_file).__name__}
            )
            self.logger.error(str(unexpected_err), exc_info=True)
            return Result.fail(unexpected_err)

    def parse(self, ndgs_data_dict: JsonDict) -> Result[JsonDict, IntegrationError]:
        """
        期待される最小スキーマ（v1.8）に基づいてNDGSデータ辞書を解析します。
        ユーザーガイド7.1節の「期待されるキーと抽出情報」を参照。

        Args:
            ndgs_data_dict: NDGS JSON出力からロードされた辞書。

        Returns:
            NGGSパラメータに適した抽出コンテキストを含む辞書 (Ok)、
            またはIntegrationError (Err)。
        """
        self.logger.debug("NDGSデータ辞書の解析を開始します...")
        # TextProcessor.processが期待するキーを持つ辞書を準備
        # parameters_overrideは、NDGSデータからNGGSの実行パラメータを上書きする場合に使用
        parsed_context_for_nggs: JsonDict = {
            "initial_text": "",         # NGGSの初期テキストとして使用
            "parameters_override": {},  # NGGSのパラメータを上書きするための辞書
            "characters_info": [],      # 抽出されたキャラクター情報
            "scene_info": {},           # 抽出されたシーン情報
            "parsing_warnings_list": [] # 解析中の警告リスト
        }
        validation_warning_messages: List[str] = []

        # 1. キャラクター定義 (character_definitions)
        #    期待: {"char_id1": {"name": "名前", "personality_summary": "性格概要"}, ...}
        character_definitions_input = ndgs_data_dict.get("character_definitions", {})
        if isinstance(character_definitions_input, dict):
            for char_id_str, char_data_dict in character_definitions_input.items():
                if isinstance(char_data_dict, dict):
                    char_info_entry: Dict[str, Any] = {
                        "id": char_id_str,
                        "name": char_data_dict.get("name", f"不明なキャラ_{char_id_str}")
                    }
                    # personality_summary または memo を性格概要として使用
                    personality_summary_text = char_data_dict.get("personality_summary", char_data_dict.get("memo"))
                    if personality_summary_text and isinstance(personality_summary_text, str):
                        char_info_entry["personality_summary"] = personality_summary_text.strip()
                    parsed_context_for_nggs["characters_info"].append(char_info_entry)
                else:
                    validation_warning_messages.append(f"キャラID '{char_id_str}' のデータ形式が無効です。辞書を期待しましたが {type(char_data_dict)} を受け取りました。")
        elif ndgs_data_dict.get("character_definitions") is not None: # キーは存在するが辞書ではない
            validation_warning_messages.append("'character_definitions' フィールドが辞書形式ではありません。")

        # 2. シーンコンテキスト (scene_context)
        #    期待: {"scene_overview": "概要", "atmosphere": "雰囲気"}
        scene_context_input = ndgs_data_dict.get("scene_context", {})
        if isinstance(scene_context_input, dict):
            scene_info_map: Dict[str, Any] = {}
            if isinstance(scene_context_input.get("scene_overview"), str):
                scene_info_map["overview"] = scene_context_input["scene_overview"].strip()
            if isinstance(scene_context_input.get("atmosphere"), str):
                scene_info_map["atmosphere"] = scene_context_input["atmosphere"].strip()
            parsed_context_for_nggs["scene_info"] = scene_info_map
        elif ndgs_data_dict.get("scene_context") is not None:
            validation_warning_messages.append("'scene_context' フィールドが辞書形式ではありません。")

        # 3. 入力テキスト (nggs_input_text または dialogue_blocks)
        #    ユーザーガイド7.1節: nggs_input_text > dialogue_blocks[0].text
        input_text_from_source: Optional[str] = None
        input_text_origin_description: str = "不明"

        if isinstance(ndgs_data_dict.get("nggs_input_text"), str) and ndgs_data_dict["nggs_input_text"].strip():
            input_text_from_source = ndgs_data_dict["nggs_input_text"].strip()
            input_text_origin_description = "nggs_input_text フィールド"
        elif isinstance(ndgs_data_dict.get("dialogue_blocks"), list):
            dialogue_blocks_list = ndgs_data_dict["dialogue_blocks"]
            if dialogue_blocks_list: # リストが空でない場合
                first_dialogue_block = dialogue_blocks_list[0]
                if isinstance(first_dialogue_block, dict) and \
                   isinstance(first_dialogue_block.get("text"), str) and \
                   first_dialogue_block["text"].strip():
                    input_text_from_source = first_dialogue_block["text"].strip()
                    input_text_origin_description = "最初のdialogue_blocksのtextフィールド"
                else:
                    validation_warning_messages.append("最初のdialogue_blocksが存在しないか、有効な'text'フィールドを含んでいません。")
        
        if input_text_from_source:
            parsed_context_for_nggs["initial_text"] = input_text_from_source
        else: # 有効な入力テキストが見つからなかった場合
            # NGGS-Liteがキャラクターやシーン情報のみからテキストを生成できるモードを持たない限り、
            # initial_textは必須に近い。v1.8では何らかのテキスト入力を前提とする。
            validation_warning_messages.append(
                "主要な入力テキストが 'nggs_input_text' または 'dialogue_blocks[0].text' から見つかりませんでした。"
                "NGGS-Liteは、広範なコンテキストからの生成を試みるかもしれませんが、品質が低下する可能性があります。"
            )
            # parsed_context_for_nggs["initial_text"] は空文字列のまま
        
        # 4. NGGSパラメータのオーバーライド (オプション)
        #    例: {"parameters_override": {"perspective_mode": "subjective_third_person"}}
        parameters_override_input = ndgs_data_dict.get("parameters_override")
        if isinstance(parameters_override_input, dict):
            parsed_context_for_nggs["parameters_override"] = parameters_override_input
            self.logger.debug(f"NDGSデータからパラメータオーバーライドを検出: {parameters_override_input}")
        elif parameters_override_input is not None:
            validation_warning_messages.append("'parameters_override' フィールドが辞書形式ではありません。")


        if validation_warning_messages:
            parsed_context_for_nggs["parsing_warnings_list"] = validation_warning_messages
            self.logger.warning("NDGSデータの解析が警告付きで完了しました:")
            for warning_msg in validation_warning_messages:
                self.logger.warning(f"  - {warning_msg}")
        
        # 必須データが抽出されたか確認
        # v1.8では、NGGS-Liteは何らかのテキスト入力（初期コンテキストでも可）を期待する
        if not parsed_context_for_nggs["initial_text"] and \
           not parsed_context_for_nggs["scene_info"] and \
           not parsed_context_for_nggs["characters_info"]:
            # 全く情報が取れなかった場合
            critical_extraction_err = IntegrationError(
                "NDGSデータから意味のあるコンテキスト（テキスト、シーン、キャラクターのいずれか）を抽出できませんでした。"
            )
            self.logger.error(str(critical_extraction_err))
            return Result.fail(critical_extraction_err)

        self.logger.info(f"NDGSデータのコンテキスト解析に成功しました。抽出元: {input_text_origin_description if input_text_from_source else 'なし'}")
        return Result.ok(parsed_context_for_nggs)


# --- GLCAI Vocabulary Feedback System ---
class GLCAIVocabularyFeedback:
    """
    NGGS-Liteからの語彙使用状況データを追跡し、エクスポートします。
    GLCAIによる将来的な分析を目的としています (v1.8スコープ: データ収集)。
    """
    def __init__(self, config: NGGSConfig, job_specific_output_dir: pathlib.Path):
        """
        GLCAIVocabularyFeedbackコンポーネントを初期化します。

        Args:
            config: NGGSConfigオブジェクト。
            job_specific_output_dir: このジョブ専用の出力ディレクトリ。
                                     フィードバックファイルはこの下に作成されます。
        """
        if not isinstance(config, NGGSConfig): raise ConfigurationError("GLCAIFeedback: NGGSConfig無効")
        if not isinstance(job_specific_output_dir, pathlib.Path): raise ConfigurationError("GLCAIFeedback: job_specific_output_dir無効")

        self.config: NGGSConfig = config
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.GLCAIFeedback")
        
        # 専用フィードバックディレクトリパスを定義
        self.feedback_output_dir: Optional[pathlib.Path] = job_specific_output_dir / config.GLCAI_FEEDBACK_DIR_NAME
        
        self._ensure_feedback_directory_exists() # feedback_output_dir設定後に呼び出し

        # 現在のジョブの追跡データを格納する内部状態
        self.current_job_id_tracking: Optional[str] = None
        # 構造: {word_str: {"count": int, "layers": List[str], "categories": List[str], "source": Optional[str]}}
        self.vocabulary_usage_tracked_data: Dict[str, Dict[str, Any]] = {}
        
        if self.feedback_output_dir: # ディレクトリが利用可能な場合のみ
            self.logger.info(f"GLCAIVocabularyFeedbackが初期化されました。出力ディレクトリ: {self.feedback_output_dir.resolve()}")
        else:
            self.logger.warning("GLCAIVocabularyFeedbackは初期化されましたが、フィードバックディレクトリが利用できません。保存は無効になります。")

    def _ensure_feedback_directory_exists(self) -> None:
        """フィードバックディレクトリが存在することを確認します。なければ作成します。"""
        if self.feedback_output_dir is None: # ディレクトリが既に利用不可とマークされている場合
            return
        try:
            self.feedback_output_dir.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"GLCAIフィードバックディレクトリが検証/作成されました: {self.feedback_output_dir}")
        except OSError as e_mkdir_feedback:
            self.logger.critical(
                f"GLCAIフィードバックディレクトリ {self.feedback_output_dir} の作成に失敗しました: {e_mkdir_feedback}。"
                "このセッションの語彙フィードバック保存は無効になります。"
            )
            self.feedback_output_dir = None # 利用不可としてマーク

    def start_tracking_for_job(self, job_id_str: str) -> None:
        """新しいジョブのために追跡状態をリセットします。"""
        self.current_job_id_tracking = job_id_str
        self.vocabulary_usage_tracked_data = {} # 新しいジョブのためにリセット
        self.logger.info(f"ジョブID: {job_id_str} のための語彙使用状況追跡を開始しました。")

    def track_vocabulary_usage(
        self,
        text_to_analyze_content: str,
        # VocabularyManagerから提供されるVocabularyItemのリスト
        vocabulary_items_to_track: Optional[List[VocabularyItem]] = None, # VocabularyItemはPart 6で定義
        # または、単純な単語リスト (VocabularyItemオブジェクトが利用できない場合)
        word_list_only_to_track: Optional[List[str]] = None
    ) -> None:
        """
        与えられたテキスト内で語彙単語の使用状況を追跡します。
        VocabularyItemオブジェクトのリストまたは単純な単語リストのいずれかを受け付けます。
        """
        if not self.current_job_id_tracking:
            self.logger.warning("語彙使用状況を追跡できません: 追跡が開始されていません（現在のジョブIDなし）。")
            return
        if not text_to_analyze_content or not isinstance(text_to_analyze_content, str):
            self.logger.debug("語彙使用状況追跡をスキップ: 入力テキストが空または無効です。")
            return
        if not vocabulary_items_to_track and not word_list_only_to_track:
            self.logger.debug("語彙使用状況追跡をスキップ: 語彙アイテムまたは単語リストが提供されていません。")
            return

        self.logger.debug(f"テキスト内の語彙使用状況を追跡中 (テキスト長 {len(text_to_analyze_content)})...")
        text_content_lower = text_to_analyze_content.lower() # 大文字・小文字を区別しないカウントのため小文字化

        # (単語文字列, レイヤーセット, カテゴリリスト, ソース文字列) のタプルリスト
        items_to_check_for_tracking: List[Tuple[str, Optional[Set[str]], Optional[List[str]], Optional[str]]] = []

        if vocabulary_items_to_track:
            for vocab_item_obj in vocabulary_items_to_track:
                if isinstance(vocab_item_obj, VocabularyItem) and vocab_item_obj.word:
                    items_to_check_for_tracking.append((
                        vocab_item_obj.word,
                        vocab_item_obj.layers,
                        vocab_item_obj.categories,
                        vocab_item_obj.source
                    ))
        elif word_list_only_to_track: # VocabularyItemがない場合のフォールバック
            for word_str_entry in word_list_only_to_track:
                if isinstance(word_str_entry, str) and word_str_entry.strip():
                    items_to_check_for_tracking.append((word_str_entry.strip(), None, None, "unknown_list"))

        if not items_to_check_for_tracking:
            self.logger.debug("入力リスト処理後、追跡対象の有効な単語がありません。")
            return

        for word_str, layers_set, categories_list, source_str in items_to_check_for_tracking:
            # 出現回数をカウント (大文字・小文字区別なし)
            # 単純カウント。より正確には単語境界を考慮した正規表現 r'\b' + re.escape(word_str.lower()) + r'\b'
            # を使用するが、フィードバック用途では単純カウントで十分な場合が多い。
            num_occurrences = text_content_lower.count(word_str.lower())

            if num_occurrences > 0:
                # 単語が見つかった場合に使用状況を記録
                if word_str not in self.vocabulary_usage_tracked_data:
                    self.vocabulary_usage_tracked_data[word_str] = {
                        "count": 0,
                        # layersとcategoriesはソート済みリストとして保存 (JSON出力の一貫性のため)
                        "layers": sorted(list(layers_set)) if layers_set else [],
                        "categories": sorted(categories_list) if categories_list else [],
                        "source": source_str or "unknown"
                    }
                self.vocabulary_usage_tracked_data[word_str]["count"] += num_occurrences
        
        self.logger.debug(f"追跡完了。このテキストで新たに使用が確認/更新されたユニーク語彙数: {len(self.vocabulary_usage_tracked_data)}")

    def save_feedback_to_file(self) -> Result[pathlib.Path, FileProcessingError]: # FileProcessingErrorはPart 2で定義
        """
        収集された語彙使用状況データをJSONファイルに保存します。
        ファイル名にはジョブIDが含まれます。ユーザーガイド7.2節の形式に従います。

        Returns:
            保存されたファイルへのパス (Ok) またはエラー (Err)。
        """
        if not self.current_job_id_tracking:
            return Result.fail(FileProcessingError("フィードバックを保存できません: 追跡が開始されていません（現在のジョブIDなし）。"))
        if self.feedback_output_dir is None: # ディレクトリが利用不可の場合
            return Result.fail(FileProcessingError("フィードバックを保存できません: GLCAIフィードバック用の出力ディレクトリが利用できません。"))

        # JSON出力用のデータ構造を準備
        feedback_data_to_save: JsonDict = {
            "job_id": self.current_job_id_tracking,
            "timestamp_utc": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'), # ISO 8601 UTC
            "nggs_version": self.config.VERSION,
            "vocab_usage": self.vocabulary_usage_tracked_data # {word: {details}} 形式
        }

        # ファイル名生成 (ジョブIDをファイル名セーフにする)
        safe_job_id_for_filename = re.sub(r'[^\w\-.]', '_', self.current_job_id_tracking)
        output_file_path = self.feedback_output_dir / f"nggs_vocab_feedback_{safe_job_id_for_filename}.json"

        self.logger.info(f"ジョブID {self.current_job_id_tracking} のGLCAI語彙フィードバックデータを保存中: {output_file_path}")

        try:
            # CompactJSONEncoderはPart 3で定義
            json_content_to_write = json.dumps(feedback_data_to_save, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
        except Exception as e_json_serialize:
            json_serialize_err = FileProcessingError(f"フィードバックデータのJSONシリアライズに失敗: {e_json_serialize}")
            self.logger.error(str(json_serialize_err), exc_info=True)
            return Result.fail(json_serialize_err)

        # safe_write_fileユーティリティを使用 (Part 3で定義)
        save_operation_result = safe_write_file(output_file_path, json_content_to_write)

        if save_operation_result.is_ok:
            self.logger.info("語彙フィードバックデータの保存に成功しました。")
            return Result.ok(output_file_path) # 実際のパスを返す
        else: # safe_write_fileがエラーを返した場合
            original_write_error = save_operation_result.error
            # エラーはsafe_write_file内でログ記録済みだが、コンテキスト追加も可能
            final_save_err = FileProcessingError(
                f"語彙フィードバックファイル '{output_file_path}' の書き込みに失敗しました: {original_write_error}",
                details=getattr(original_write_error, 'details', {}) # 元のエラー詳細を保持
            )
            self.logger.error(str(final_save_err)) # 必要であれば追加ログ
            return Result.fail(final_save_err)


# --- TextProcessorクラスへの統合 (Part 11, 12からの継続と修正) ---
# TextProcessorクラスの定義は非常に長いため、ここでは__init__とprocessメソッドの
# NDGSIntegrationおよびGLCAIVocabularyFeedbackに関連する部分のみを示します。
# 実際の完全なスクリプトでは、TextProcessorの全メソッドが1つのクラス定義内に含まれます。

class TextProcessor:
    # ... (Part 11, 12, 13, 14で定義された他のメソッドは省略) ...

    def __init__(
        self,
        config: NGGSConfig,
        llm_client: LLMClient,
        evaluator: Evaluator,
        vocab_manager: VocabularyManager,
        narrative_flow: NarrativeFlowFramework,
        eti_evaluator: ExtendedETIEvaluator,
        ri_evaluator: RIEvaluator,
        subjective_evaluator: SubjectiveEvaluator,
        generation_template: str,
        improvement_template: str,
        # NDGSIntegrationとGLCAIVocabularyFeedbackを注入可能にする
        ndgs_parser: Optional[NDGSIntegration] = None, # 型をNDGSIntegrationに
        glcai_feedback: Optional[GLCAIVocabularyFeedback] = None # 型をGLCAIVocabularyFeedbackに
    ):
        self.logger = logger_text_processor # Part 11で初期化済み
        # --- コアコンポーネントの検証と格納 (既存の検証に追加) ---
        if not isinstance(config, NGGSConfig): raise ConfigurationError("TP: NGGSConfig無効")
        # ... (他のコンポーネントの検証は省略) ...
        if ndgs_parser is not None and not isinstance(ndgs_parser, NDGSIntegration):
            raise ConfigurationError("TextProcessorには有効なNDGSIntegrationインスタンスが必要です（提供された場合）。")
        if glcai_feedback is not None and not isinstance(glcai_feedback, GLCAIVocabularyFeedback):
            raise ConfigurationError("TextProcessorには有効なGLCAIVocabularyFeedbackインスタンスが必要です（提供された場合）。")

        self.config: NGGSConfig = config
        self.llm: LLMClient = llm_client
        self.evaluator: Evaluator = evaluator
        self.vocab_manager: VocabularyManager = vocab_manager
        self.narrative_flow: NarrativeFlowFramework = narrative_flow
        self.eti_evaluator: ExtendedETIEvaluator = eti_evaluator
        self.ri_evaluator: RIEvaluator = ri_evaluator
        self.subjective_evaluator: SubjectiveEvaluator = subjective_evaluator
        
        self.ndgs_parser: Optional[NDGSIntegration] = ndgs_parser
        self.glcai_feedback: Optional[GLCAIVocabularyFeedback] = glcai_feedback

        # --- テンプレートの検証と格納 (既存のロジック) ---
        gen_res = validate_template(generation_template, ["input_text","target_length"]); imp_res = validate_template(improvement_template, ["original_text","evaluation_results_json"])
        if gen_res.is_err: raise gen_res.error # type: ignore
        if imp_res.is_err: raise imp_res.error # type: ignore
        self.generation_template: str = generation_template
        self.improvement_template: str = improvement_template
        self.logger.info(f"TextProcessor初期化完了 (v{config.VERSION})。NDGS Parser: {'有効' if self.ndgs_parser else '無効'}, GLCAI Feedback: {'有効' if self.glcai_feedback else '無効'}")


    def process(
        self,
        initial_text_input: str,
        target_length_override: Optional[int] = None,
        perspective_mode_override: Optional[str] = None,
        phase_focus_override: Optional[str] = None,
        colloquial_level_override: Optional[str] = None,
        emotion_arc_override: Optional[str] = None,
        max_loops_override: Optional[int] = None,
        improvement_threshold_override: Optional[float] = None,
        narrative_flow_prompt_override: Optional[str] = None,
        skip_initial_generation_flag: bool = False,
        ndgs_input_data_dict: Optional[JsonDict] = None, # NDGSからの構造化データ
        job_id_override: Optional[str] = None # 外部からジョブIDを指定する場合
    ) -> Result[JsonDict, NGGSError]:
        # --- パラメータの検証と設定 (既存のロジック) ---
        final_max_loops = max_loops_override if max_loops_override is not None else self.config.DEFAULT_MAX_LOOPS
        final_improvement_threshold = improvement_threshold_override if threshold_override is not None else self.config.DEFAULT_IMPROVEMENT_THRESHOLD
        final_target_text_length = target_length_override if target_length_override is not None else self.config.DEFAULT_TARGET_LENGTH
        final_perspective_mode = perspective_mode_override if perspective_mode_override is not None else getattr(self.config, 'PERSPECTIVE_MODE_DEFAULT', "subjective_first_person")
        final_phase_focus = phase_focus_override if phase_focus_override is not None else getattr(self.config, 'PHASE_FOCUS_DEFAULT', "balanced")
        final_colloquial_level = colloquial_level_override if colloquial_level_override is not None else getattr(self.config, 'COLLOQUIAL_LEVEL_DEFAULT', "medium")
        final_emotion_arc = emotion_arc_override
        final_narrative_flow_prompt = narrative_flow_prompt_override

        # --- 結果辞書の初期化 (ジョブIDの処理を追加) ---
        current_job_id = job_id_override if job_id_override else self.config.generate_job_id()
        results_output_dict: JsonDict = {
            "job_id": current_job_id,
            # ... (Part 12で定義された他のフィールド) ...
            "original_text_input_preview": truncate_text(initial_text_input, 100) if isinstance(initial_text_input, str) else "(NDGSデータから抽出予定)",
            "parameters_used": {
                "target_length": final_target_text_length, "perspective_mode": final_perspective_mode,
                "phase_focus": final_phase_focus, "colloquial_level": final_colloquial_level,
                "emotion_arc": final_emotion_arc, "max_loops": final_max_loops,
                "improvement_threshold": final_improvement_threshold,
                "skip_initial_generation": skip_initial_generation_flag,
                "ndgs_input_provided_flag": bool(ndgs_input_data_dict),
                "llm_engine_used": self.llm.current_engine,
                "llm_model_used": self.llm._get_model_name()
            },
            "versions_data": [], "final_generated_text": "", "best_text_loop_index": -1,
            "final_aggregated_scores": {}, "final_distributions": {"phase": {}, "layer": {}},
            "html_report_path": "", "overall_status": "Processing", "processing_errors_summary": []
        }

        # GLCAIフィードバック追跡開始
        if self.glcai_feedback:
            self.glcai_feedback.start_tracking_for_job(current_job_id)

        # --- NDGS入力データの処理 (既存のロジック、エラーハンドリング改善) ---
        current_text_for_processing = initial_text_input if isinstance(initial_text_input, str) else ""
        if ndgs_input_data_dict and self.ndgs_parser:
            self.logger.info(f"ジョブID {current_job_id}: NDGS入力データを解析中...")
            parsed_ndgs_result = self.ndgs_parser.parse(ndgs_input_data_dict)
            if parsed_ndgs_result.is_ok:
                ndgs_parsed_content = parsed_ndgs_result.unwrap()
                current_text_for_processing = ndgs_parsed_content.get("initial_text", current_text_for_processing)
                # パラメータオーバーライドの適用 (より堅牢に)
                override_params = ndgs_parsed_content.get("parameters_override", {})
                if isinstance(override_params, dict):
                    final_perspective_mode = override_params.get("perspective_mode", final_perspective_mode)
                    final_phase_focus = override_params.get("phase_focus", final_phase_focus)
                    # 他のパラメータも同様に
                    results_output_dict["parameters_used"].update(override_params) # 使用パラメータを更新
                results_output_dict["original_text_input_preview"] = truncate_text(current_text_for_processing, 100)
                self.logger.info(f"ジョブID {current_job_id}: NDGSデータ解析完了。")
            else: # NDGS解析エラー
                ndgs_parse_error_msg = f"ジョブID {current_job_id}: NDGS入力データの解析に失敗: {parsed_ndgs_result.error}"
                self.logger.error(ndgs_parse_error_msg)
                results_output_dict["processing_errors_summary"].append(ndgs_parse_error_msg)
                # NDGS解析失敗は致命的とするか、フォールバックするかは要件次第
                # ここではエラーを記録し、可能なら提供されたinitial_text_inputで続行
        
        if not current_text_for_processing and not skip_initial_generation_flag:
            # 致命的エラーとして処理
            cfg_err = ConfigurationError("処理可能な初期テキストが見つかりません（NDGSデータからも抽出不可）。")
            self._handle_error(cfg_err, -1, results_output_dict, {}, is_fatal_loop_error=True) # ループ外エラー
            self._finalize_results(results_output_dict, "", {}, "致命的エラー（入力不足）")
            return Result.fail(cfg_err)

        # ... (ループ0と改善ループのロジックはPart 12, 13で定義済み) ...
        # ループ内でテキストが生成されるたびにGLCAIフィードバックを呼び出す必要がある
        # 例: _perform_initial_generation や _perform_improvement_loop の成功後
        # generated_text = ...
        # if self.glcai_feedback and generated_text:
        #     self.glcai_feedback.track_vocabulary_usage(generated_text, self.vocab_manager.items)

        # 仮のループ処理 (実際のループはPart 12, 13のものを想定)
        best_text_overall = current_text_for_processing # 仮
        best_full_eval_data_overall = {} # 仮
        try:
            # === ループ 0: 初期生成または初期評価 (Part 12のロジックをここに展開) ===
            # ... (この部分はPart 12のprocessメソッド内のループ0ロジックを移植・統合) ...
            # ループ0で生成/評価されたテキスト (loop0_text) に対して:
            # if self.glcai_feedback and loop0_text:
            #     self.glcai_feedback.track_vocabulary_usage(loop0_text, self.vocab_manager.items)

            # === 改善ループ (Part 13のロジックをここに展開) ===
            # for improvement_loop_idx in range(1, final_max_loops + 1):
            #     ... (改善ループの処理) ...
            #     # 改善ループで生成されたテキスト (improved_text) に対して:
            #     if self.glcai_feedback and improved_text:
            #         self.glcai_feedback.track_vocabulary_usage(improved_text, self.vocab_manager.items)
            pass # Part 12, 13のロジックがここに入る

        except Exception as e_process_unexpected:
            # ... (Part 12の例外処理) ...
            self.logger.critical(f"ジョブID {current_job_id}: TextProcessor.processで予期せぬエラー: {e_process_unexpected}", exc_info=True)
            # エラー処理と最終化
            self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, "致命的エラー発生")
            if self.glcai_feedback: # エラー発生時でも、それまでの追跡データを保存試行
                save_res = self.glcai_feedback.save_feedback_to_file()
                if save_res.is_err: self.logger.error(f"ジョブID {current_job_id}: エラー発生後のGLCAIフィードバック保存失敗: {save_res.error}")
            return Result.fail(NGGSError(f"プロセス致命的エラー: {e_process_unexpected}"))


        # --- 最終処理 ---
        final_overall_status = results_output_dict.get("overall_status", "完了") # finalize_resultsで設定されるが、ここでも更新
        if not results_output_dict["processing_errors_summary"] and final_overall_status == "Processing":
            final_overall_status = "完了 (エラーなし)"
        elif results_output_dict["processing_errors_summary"] and final_overall_status == "Processing":
            final_overall_status = "完了 (回復可能エラーあり)"

        self._finalize_results(results_output_dict, best_text_overall, best_full_eval_data_overall, final_overall_status)
        
        # GLCAIフィードバック保存
        if self.glcai_feedback:
            save_result = self.glcai_feedback.save_feedback_to_file()
            if save_result.is_err:
                results_output_dict["processing_errors_summary"].append(f"GLCAIフィードバック保存失敗: {save_result.error}")
                self.logger.error(f"ジョブID {current_job_id}: GLCAIフィードバックの保存に失敗: {save_result.error}")
            else:
                results_output_dict["glcai_feedback_file"] = str(save_result.unwrap().resolve())

        return Result.ok(results_output_dict)

    # ... (Part 11, 12, 13, 14で定義された他のメソッドは省略) ...
    # _handle_error, _finalize_results, _create_*_templates, _generate_text など

# =============================================================================
# Part 15 End: Integration Components (NDGS Parser & GLCAI Feedback)
# =============================================================================
# =============================================================================
# Part 16: Batch Processing Class (v1.8)
# (Continues TextProcessor class definition, adds BatchProcessor)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, TextProcessor, Result, NGGSError, ConfigurationError, JsonDict,
# safe_read_file, safe_write_file, CompactJSONEncoder, truncate_text,
# CONCURRENT_FUTURES_AVAILABLE など。

import logging
import json
import pathlib
import time
import sys # _print_progress で使用
import re # _save_job_results で使用
from datetime import datetime, timezone
from typing import (
    Dict, List, Any, Optional, Union, Tuple # Part 1からの型も含む
)
# concurrent.futures は条件付きでインポート
if CONCURRENT_FUTURES_AVAILABLE: # CONCURRENT_FUTURES_AVAILABLEはPart 1で定義
    try:
        from concurrent.futures import ThreadPoolExecutor, as_completed
    except ImportError:
        # この場合、CONCURRENT_FUTURES_AVAILABLEがFalseのはずだが念のため
        ThreadPoolExecutor = None # type: ignore
        as_completed = None # type: ignore
else:
    ThreadPoolExecutor = None # type: ignore
    as_completed = None # type: ignore


# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError, CONCURRENT_FUTURES_AVAILABLE, BASE_DIR
# from nggs_lite_part2 import Result, NGGSError
# from nggs_lite_part3 import safe_read_file, safe_write_file, CompactJSONEncoder, truncate_text
# from nggs_lite_part14 import TextProcessor # TextProcessorはPart 14で完成している想定

# HTMLエスケープ用の簡易ヘルパー (標準ライブラリhtml.escapeが望ましいが、依存を避けるため簡易版)
def escape_html(text: Optional[Any]) -> str:
    """簡易的なHTMLエスケープを行います。"""
    if text is None:
        return ""
    text_str = str(text)
    return text_str.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;").replace("'", "&#39;")


class BatchProcessor:
    """
    TextProcessorを使用して複数の入力テキストファイルをバッチ処理します (v1.8)。
    進捗追跡機能を提供し、サマリーレポートを生成します。基本的な並列処理機能を含みます。
    """
    def __init__(self, config: NGGSConfig, processor: TextProcessor, batch_output_base_dir: pathlib.Path):
        """
        BatchProcessorを初期化します。

        Args:
            config: NGGSConfigオブジェクト。
            processor: 初期化済みのTextProcessorインスタンス。
            batch_output_base_dir: バッチ結果とサマリーを保存するディレクトリ。
                                   この下にバッチごとのサブディレクトリが作成されます。
        Raises:
            ConfigurationError: processorが無効な場合、または出力ディレクトリを扱えない場合。
        """
        if not isinstance(config, NGGSConfig): # NGGSConfigはPart 1で定義
            raise ConfigurationError("BatchProcessorには有効なNGGSConfigインスタンスが必要です。")
        if not isinstance(processor, TextProcessor): # TextProcessorはPart 14で完成
            raise ConfigurationError("BatchProcessorには有効なTextProcessorインスタンスが必要です。")

        self.config: NGGSConfig = config
        self.processor: TextProcessor = processor
        # バッチ処理全体の出力ディレクトリ (例: ./nggs_lite_output_v1.8/batch_runs/)
        # この下に、個々のバッチ実行ごとのタイムスタンプ付きディレクトリが作成される
        self.batch_runs_base_dir: pathlib.Path = batch_output_base_dir.resolve() / "batch_runs"
        self.current_batch_output_dir: Optional[pathlib.Path] = None # 現在のバッチ実行用ディレクトリ
        self.logger: logging.Logger = logging.getLogger("NGGS-Lite.BatchProcessor")

        # バッチ状態変数
        self.processed_files_count: int = 0
        self.total_files_to_process: int = 0
        self.successful_job_count: int = 0
        self.failed_job_count: int = 0
        self.skipped_file_count: int = 0
        self.batch_results_summary_list: List[JsonDict] = [] # 個々のファイル処理結果サマリ
        self.batch_start_time_monotonic: float = 0.0 # time.monotonic()による開始時刻
        self.batch_start_time_utc_iso: str = "" # ISO形式のUTC開始時刻

        try:
            self.batch_runs_base_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"バッチ実行のベース出力ディレクトリ: {self.batch_runs_base_dir}")
        except PermissionError as e_perm_batch_base:
            self.logger.critical(f"ベースバッチ出力ディレクトリの作成権限がありません: {self.batch_runs_base_dir}")
            raise ConfigurationError(f"ベースバッチ出力ディレクトリの作成権限がありません: {self.batch_runs_base_dir}") from e_perm_batch_base
        except OSError as e_os_batch_base:
            self.logger.critical(f"ベースバッチ出力ディレクトリ {self.batch_runs_base_dir} の作成に失敗: {e_os_batch_base}")
            raise ConfigurationError(f"ベースバッチ出力ディレクトリ {self.batch_runs_base_dir} の作成に失敗") from e_os_batch_base

    def _initialize_current_batch_run(self) -> None:
        """現在のバッチ実行のための状態と出力ディレクトリを初期化します。"""
        self.processed_files_count = 0
        self.successful_job_count = 0
        self.failed_job_count = 0
        self.skipped_file_count = 0
        self.batch_results_summary_list = []
        self.batch_start_time_monotonic = time.monotonic()
        self.batch_start_time_utc_iso = datetime.now(timezone.utc).isoformat()

        # 現在のバッチ実行用のタイムスタンプ付きサブディレクトリを作成
        current_batch_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        self.current_batch_output_dir = self.batch_runs_base_dir / f"batch_{current_batch_timestamp}"
        try:
            self.current_batch_output_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"現在のバッチ実行出力ディレクトリ: {self.current_batch_output_dir}")
        except Exception as e_mkdir_current_batch:
            self.logger.error(f"現在のバッチ出力ディレクトリ {self.current_batch_output_dir} の作成に失敗: {e_mkdir_current_batch}。ルートバッチディレクトリに保存します。")
            self.current_batch_output_dir = self.batch_runs_base_dir # フォールバック

    def process_batch(
        self,
        input_directory_path_str: Union[str, pathlib.Path],
        max_workers_override: Optional[int] = None, # Noneならconfigのデフォルト、または1
        file_glob_pattern: str = "*.txt", # デフォルトはテキストファイル
        save_individual_job_results: bool = True,
        # TextProcessor.processに渡す共通の引数
        # (initial_textはファイルから読み込むため、ここでは指定しない)
        **common_processor_args: Any
    ) -> JsonDict:
        """
        入力ディレクトリ内のパターンに一致する全てのファイルを処理します。

        Args:
            input_directory_path_str: 入力テキストファイルを含むディレクトリ。
            max_workers_override: 並列ワーカー数 (>=1)。1より大きい場合並列処理。
            file_glob_pattern: 入力ファイルのglobパターン (例: "*.txt", "**/*.json")。
            save_individual_job_results: 各ジョブのJSON、HTML、TXTを保存するかどうか。
            **common_processor_args: 各ファイルのTextProcessor.processに渡す共通引数。

        Returns:
            バッチ処理サマリーを含む辞書。
        """
        self._initialize_current_batch_run() # バッチ実行ごとの初期化

        input_dir_resolved = pathlib.Path(input_directory_path_str).resolve()
        if not input_dir_resolved.is_dir():
            error_msg_input_dir = f"バッチ入力ディレクトリが見つからないか、ディレクトリではありません: {input_dir_resolved}"
            self.logger.error(error_msg_input_dir)
            return self._create_empty_batch_summary(error_message=error_msg_input_dir)

        self.logger.info(f"'{file_glob_pattern}' に一致するファイルを '{input_dir_resolved}' で検索中...")
        try:
            # input_filesはpathlib.Pathオブジェクトのリスト
            input_files_list: List[pathlib.Path] = sorted(list(input_dir_resolved.glob(file_glob_pattern)))
        except Exception as e_glob:
            error_msg_glob = f"パターン '{file_glob_pattern}' でのファイル検索中にエラー ({input_dir_resolved}): {e_glob}"
            self.logger.error(error_msg_glob, exc_info=True)
            return self._create_empty_batch_summary(error_message=error_msg_glob)

        self.total_files_to_process = len(input_files_list)

        if not input_files_list:
            self.logger.warning(f"'{file_glob_pattern}' に一致するファイルが {input_dir_resolved} に見つかりませんでした。")
            return self._create_empty_batch_summary(no_files_found=True)

        self.logger.info(f"{self.total_files_to_process} 個のファイルのバッチ処理を開始します...")
        
        # ワーカー数の決定 (configにBATCH_MAX_WORKERS_DEFAULTのような設定があればそれを使用)
        num_workers = 1
        if max_workers_override is not None:
            num_workers = max(1, max_workers_override)
        elif hasattr(self.config, 'BATCH_MAX_WORKERS_DEFAULT'):
            num_workers = max(1, getattr(self.config, 'BATCH_MAX_WORKERS_DEFAULT', 1))
        
        if num_workers > 1 and not CONCURRENT_FUTURES_AVAILABLE: # Part 1で定義されたフラグ
            self.logger.warning(
                f"並列処理 ({num_workers}ワーカー) が要求されましたが、'concurrent.futures'が利用できません。"
                "逐次処理にフォールバックします。"
            )
            num_workers = 1
        
        if num_workers > 1: self.logger.info(f"最大 {num_workers} ワーカーで並列処理を使用します。")
        else: self.logger.info("逐次処理を使用します (1ワーカー)。")

        self._print_progress() # 初期進捗表示

        # _process_single_fileに渡す共通引数
        shared_args_for_processing = {
            "save_results_flag": save_individual_job_results,
            "processor_args_dict": common_processor_args
        }

        if num_workers > 1 and ThreadPoolExecutor is not None and as_completed is not None:
            self._run_parallel_processing(input_files_list, num_workers, shared_args_for_processing)
        else:
            self._run_sequential_processing(input_files_list, shared_args_for_processing)

        final_batch_summary = self._generate_final_batch_summary()
        self._save_batch_summary_files(final_batch_summary)
        self._print_progress(is_final_progress=True) # 最終進捗表示
        self.logger.info(
            f"バッチ処理完了。成功: {self.successful_job_count}, "
            f"失敗: {self.failed_job_count}, スキップ: {self.skipped_file_count}"
        )
        return final_batch_summary

    def _run_sequential_processing(self, files_to_process: List[pathlib.Path], shared_processing_args: Dict[str, Any]) -> None:
        """ファイルを逐次処理します。"""
        self.logger.info("ジョブを逐次実行中...")
        for file_path_obj in files_to_process:
            try:
                # _process_single_fileは個々のファイル処理結果サマリ(JsonDict)を返す
                single_file_summary_item = self._process_single_file(file_path_obj, **shared_processing_args)
                self.batch_results_summary_list.append(single_file_summary_item)
                # ステータスに基づいてカウンターを更新
                status_str = single_file_summary_item.get("status", "Unknown").lower()
                if "fail" in status_str or "error" in status_str: self.failed_job_count += 1
                elif "skip" in status_str: self.skipped_file_count += 1
                else: self.successful_job_count += 1 # "success", "完了" など
            except Exception as e_seq_proc: # _process_single_file内でキャッチされなかった例外
                self.logger.error(f"ファイル {file_path_obj.name} の逐次処理中に致命的エラー: {e_seq_proc}", exc_info=True)
                self.batch_results_summary_list.append({
                    "file": file_path_obj.name, "status": "失敗 (逐次処理中の致命的エラー)",
                    "error_snippet": [f"{type(e_seq_proc).__name__}: {str(e_seq_proc)}"], "job_id": "N/A"
                })
                self.failed_job_count += 1
            finally:
                self.processed_files_count += 1
                self._print_progress()

    def _run_parallel_processing(self, files_to_process: List[pathlib.Path], num_workers: int, shared_processing_args: Dict[str, Any]) -> None:
        """ThreadPoolExecutorを使用してファイルを並列処理します。"""
        if ThreadPoolExecutor is None or as_completed is None: # 再度確認
             self.logger.error("並列処理が要求されましたが、concurrent.futuresコンポーネントがロードされていません。逐次処理にフォールバックします。")
             self._run_sequential_processing(files_to_process, shared_processing_args)
             return

        self.logger.info(f"ジョブを並列実行中 (最大ワーカー数={num_workers})...")
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_file_map = {
                executor.submit(self._process_single_file, file_path_obj, **shared_processing_args): file_path_obj
                for file_path_obj in files_to_process
            }
            for future_obj in as_completed(future_to_file_map):
                file_path_obj_completed = future_to_file_map[future_obj]
                try:
                    single_file_summary_item = future_obj.result() # 結果を取得
                    self.batch_results_summary_list.append(single_file_summary_item)
                    status_str = single_file_summary_item.get("status", "Unknown").lower()
                    if "fail" in status_str or "error" in status_str: self.failed_job_count += 1
                    elif "skip" in status_str: self.skipped_file_count += 1
                    else: self.successful_job_count += 1
                except Exception as e_thread_proc: # スレッド内での例外
                    self.logger.error(f"ファイル {file_path_obj_completed.name} のスレッド処理中にエラー: {e_thread_proc}", exc_info=True)
                    self.batch_results_summary_list.append({
                        "file": file_path_obj_completed.name, "status": "失敗 (スレッド例外)",
                        "error_snippet": [f"{type(e_thread_proc).__name__}: {str(e_thread_proc)}"], "job_id": "N/A"
                    })
                    self.failed_job_count += 1
                finally:
                    self.processed_files_count += 1
                    self._print_progress()

    def _process_single_file(
        self,
        file_path_obj: pathlib.Path,
        save_results_flag: bool,
        processor_args_dict: Dict[str, Any] # TextProcessor.processに渡す引数
    ) -> JsonDict: # このファイル処理のサマリ情報を返す
        """
        単一ファイルを読み込み、処理し、結果を保存し、サマリを返します。
        NDGS JSONファイルか通常のテキストファイルかを判断します。
        """
        self.logger.info(f"ファイル処理開始: {file_path_obj.name}...")
        file_processing_start_time = time.monotonic()
        # 各ファイル処理結果のサマリ
        current_file_summary: JsonDict = {"file": file_path_obj.name, "job_id": "N/A", "status": "Pending"}

        # NDGS JSONファイルかテキストファイルかを拡張子で簡易判定
        is_ndgs_json_input = file_path_obj.suffix.lower() == ".json"
        
        initial_text_for_processor: str = ""
        ndgs_data_for_processor: Optional[JsonDict] = None

        if is_ndgs_json_input:
            if self.processor.ndgs_parser: # NDGSパーサーが設定されていれば
                # NDGSIntegration.parse_from_fileはResult[JsonDict, IntegrationError]を返す
                ndgs_parse_result = self.processor.ndgs_parser.parse_from_file(file_path_obj)
                if ndgs_parse_result.is_err:
                    error_detail_str = str(ndgs_parse_result.error)
                    self.logger.error(f"NDGS JSONファイル {file_path_obj.name} の解析失敗: {error_detail_str}")
                    current_file_summary["status"] = "失敗 (NDGS解析エラー)"
                    current_file_summary["error_snippet"] = [error_detail_str]
                    return current_file_summary
                ndgs_data_for_processor = ndgs_parse_result.unwrap()
                # initial_textはndgs_data_for_processorから取得されるか、空のまま
                initial_text_for_processor = ndgs_data_for_processor.get("initial_text", "")
            else: # NDGSパーサーなしでJSONファイルが来た場合
                self.logger.warning(f"JSONファイル {file_path_obj.name} が指定されましたが、NDGSパーサーが設定されていません。テキストファイルとして読み込みます。")
                is_ndgs_json_input = False # テキストファイルとして扱う

        if not is_ndgs_json_input: # 通常のテキストファイルとして読み込み
            # safe_read_fileはResult[str, FileProcessingError]を返す
            read_operation_result = safe_read_file(file_path_obj) # Part 3で定義
            if read_operation_result.is_err:
                error_detail_str = str(read_operation_result.error)
                self.logger.error(f"ファイル {file_path_obj.name} の読み込み失敗: {error_detail_str}")
                current_file_summary["status"] = "失敗 (読み込みエラー)"
                current_file_summary["error_snippet"] = [error_detail_str]
                return current_file_summary
            initial_text_for_processor = read_operation_result.unwrap()

        if not initial_text_for_processor and not (is_ndgs_json_input and ndgs_data_for_processor): # テキストが空で、NDGSコンテキストもない場合
            self.logger.warning(f"ファイル {file_path_obj.name} が空か、有効なコンテキストがありません。スキップします。")
            current_file_summary["status"] = "スキップ (空またはコンテキストなし)"
            return current_file_summary

        # TextProcessor.processを呼び出し
        job_processing_results: JsonDict = {} # TextProcessor.processが返す結果
        try:
            # TextProcessor.processはResult[JsonDict, NGGSError]を返す
            # job_idはTextProcessor.process内部で生成される
            process_operation_result = self.processor.process(
                initial_text_input=initial_text_for_processor, # ファイルから読み込んだテキスト
                ndgs_input_data_dict=ndgs_data_for_processor, # NDGS JSONの場合の解析済みデータ
                **processor_args_dict # バッチ実行時の共通パラメータ
            )
            if process_operation_result.is_ok:
                job_processing_results = process_operation_result.unwrap()
            else: # TextProcessor.processがErrを返した場合 (致命的エラー)
                processor_error_obj = process_operation_result.error
                job_processing_results = {
                    "job_id": self.config.generate_job_id("error_proc_"),
                    "status": "失敗 (TextProcessor致命的エラー)",
                    "errors": [str(processor_error_obj)], "final_text": "", "final_scores": {},
                    "parameters": processor_args_dict, "start_time": datetime.now(timezone.utc).isoformat()
                }
                self.logger.error(f"ファイル {file_path_obj.name} のTextProcessor処理中に致命的エラー: {processor_error_obj}")
        except Exception as e_proc_unexpected: # TextProcessor.process呼び出し自体の予期せぬエラー
            self.logger.critical(f"ファイル {file_path_obj.name} の processor.process呼び出しで予期せぬエラー: {e_proc_unexpected}", exc_info=True)
            job_processing_results = {
                "job_id": self.config.generate_job_id("unexpected_batch_err_"),
                "status": "失敗 (バッチ内TextProcessor呼び出しエラー)",
                "errors": [f"予期せぬエラー: {type(e_proc_unexpected).__name__} - {str(e_proc_unexpected)}"],
                "final_text": "", "final_scores": {}, "parameters": processor_args_dict,
                "start_time": datetime.now(timezone.utc).isoformat()
            }
        
        current_job_id_str = job_processing_results.get("job_id", f"batch_{file_path_obj.stem}_{int(time.time())}")
        current_file_summary["job_id"] = current_job_id_str

        if save_results_flag and self.current_batch_output_dir: # 保存フラグと出力先ディレクトリ確認
            self._save_individual_job_results(current_job_id_str, job_processing_results)

        file_processing_duration_seconds = time.monotonic() - file_processing_start_time
        self.logger.info(
            f"ファイル処理完了: {file_path_obj.name} ({file_processing_duration_seconds:.2f}秒)。"
            f"ステータス: {job_processing_results.get('status', '不明')}"
        )

        current_file_summary["status"] = job_processing_results.get("status", "不明")
        current_file_summary["final_score_overall"] = job_processing_results.get("final_aggregated_scores", {}).get("overall_quality")
        current_file_summary["duration_seconds"] = round(file_processing_duration_seconds, 2)
        errors_list_from_job = job_processing_results.get("processing_errors_summary", []) # TextProcessorからのエラーリスト
        if errors_list_from_job and isinstance(errors_list_from_job, list) and errors_list_from_job:
            current_file_summary["error_snippet"] = [str(errors_list_from_job[0])] # 最初のエラー概要のみ
        
        return current_file_summary

    def _save_individual_job_results(self, job_id_str: str, job_processing_results: JsonDict) -> None:
        """個々のジョブ結果（JSON、HTML、TXT）を保存します。"""
        if not self.current_batch_output_dir:
            self.logger.error(f"ジョブ {job_id_str} の個別結果を保存できません: バッチ出力ディレクトリが未設定です。")
            return

        safe_job_id_for_file = re.sub(r'[^\w\-.]', '_', job_id_str) # ファイル名として安全なIDに
        # 各ジョブの結果は、現在のバッチ実行ディレクトリの下にジョブID名のサブディレクトリを作成して保存
        job_specific_output_subdir = self.current_batch_output_dir / safe_job_id_for_file
        try:
            job_specific_output_subdir.mkdir(parents=True, exist_ok=True)
        except OSError as e_mkdir_job_subdir:
            self.logger.error(
                f"ジョブ {job_id_str} の出力サブディレクトリ {job_specific_output_subdir} 作成失敗: {e_mkdir_job_subdir}。"
                "現在のバッチディレクトリ直下に保存します。"
            )
            job_specific_output_subdir = self.current_batch_output_dir # フォールバック

        # JSON結果の保存
        json_output_path = job_specific_output_subdir / f"{safe_job_id_for_file}_results.json"
        # CompactJSONEncoderはPart 3で定義
        save_json_result = safe_write_file(json_output_path, json.dumps(job_processing_results, ensure_ascii=False, indent=2, cls=CompactJSONEncoder))
        if save_json_result.is_err: self.logger.error(f"ジョブ {job_id_str} のJSON結果保存失敗: {save_json_result.error}")

        # HTMLレポートの保存 (TextProcessorが生成した場合)
        # html_report_contentはTextProcessor._finalize_results内で生成・設定される想定 (Part 14で実装)
        html_report_content_str = job_processing_results.get("html_report_path") # または html_report_content
        if html_report_content_str and isinstance(html_report_content_str, str): # パスが格納されている場合
            # ここでは、html_report_path にHTMLファイルへのパスが格納されていると仮定する。
            # もしHTML内容そのものが格納されているなら、それを直接書き込む。
            # この部分はTextProcessor側の実装と整合させる必要がある。
            # 今は、パスが格納されていると仮定し、そのファイルをコピーするか、
            # あるいは、BatchProcessorがHTMLを別途生成する。
            # プランでは、TextProcessorがhtml_reportを生成し、BatchProcessorがそれを参照。
            # ここでは、job_resultsにHTML内容そのものが含まれると仮定して書き出す。
            if "<html>" in html_report_content_str.lower(): # 簡単なHTML内容チェック
                 html_output_path = job_specific_output_subdir / f"{safe_job_id_for_file}_report.html"
                 save_html_result = safe_write_file(html_output_path, html_report_content_str)
                 if save_html_result.is_err: self.logger.error(f"ジョブ {job_id_str} のHTMLレポート保存失敗: {save_html_result.error}")
        elif "Fail" not in job_processing_results.get("status", ""): # 失敗ジョブでなければデバッグログ
            self.logger.debug(f"ジョブ {job_id_str} のHTMLレポート内容が見つかりません。")

        # 最終テキストの保存
        final_generated_text_content = job_processing_results.get("final_generated_text")
        if final_generated_text_content and isinstance(final_generated_text_content, str):
            text_output_path = job_specific_output_subdir / f"{safe_job_id_for_file}_final.txt"
            save_text_result = safe_write_file(text_output_path, final_generated_text_content)
            if save_text_result.is_err: self.logger.error(f"ジョブ {job_id_str} の最終テキスト保存失敗: {save_text_result.error}")
        elif "Fail" not in job_processing_results.get("status", ""):
            self.logger.debug(f"ジョブ {job_id_str} の最終テキスト内容が見つかりません。")
        
        # GLCAIフィードバックファイルの保存はTextProcessor.processの最後で行われるため、ここでは不要
        # ただし、そのファイルパスをバッチサマリに含めることは検討可能

    def _generate_final_batch_summary(self) -> JsonDict:
        """最終的なバッチサマリー辞書を生成します。"""
        batch_end_monotonic_time = time.monotonic()
        total_batch_duration_seconds = batch_end_monotonic_time - self.batch_start_time_monotonic \
                                    if self.batch_start_time_monotonic > 0 else 0.0
        
        return {
            "batch_id": self.current_batch_output_dir.name if self.current_batch_output_dir else "unknown_batch",
            "batch_start_time_utc": self.batch_start_time_utc_iso,
            "batch_end_time_utc": datetime.now(timezone.utc).isoformat(),
            "total_execution_time_seconds": round(total_batch_duration_seconds, 2),
            "total_files_found_in_input": self.total_files_to_process,
            "files_actually_processed": self.processed_files_count,
            "successful_processing_jobs": self.successful_job_count,
            "failed_processing_jobs": self.failed_job_count,
            "skipped_empty_or_invalid_files": self.skipped_file_count,
            "individual_job_results_summary": self.batch_results_summary_list, # 個々の処理結果サマリリスト
        }

    def _save_batch_summary_files(self, final_batch_summary_data: JsonDict) -> None:
        """バッチサマリーレポート（JSONおよびHTML）を保存します。"""
        if not self.current_batch_output_dir or \
           not self.current_batch_output_dir.is_dir() or \
           not os.access(self.current_batch_output_dir, os.W_OK):
            self.logger.error(f"バッチサマリーを保存できません: 出力ディレクトリ {self.current_batch_output_dir} にアクセスできません。")
            return

        # JSONサマリレポート
        json_summary_output_path = self.current_batch_output_dir / "_batch_summary_report.json"
        # CompactJSONEncoderはPart 3で定義
        save_json_summary_result = safe_write_file(
            json_summary_output_path,
            json.dumps(final_batch_summary_data, ensure_ascii=False, indent=2, cls=CompactJSONEncoder)
        )
        if save_json_summary_result.is_ok: self.logger.info(f"バッチサマリーJSONレポート保存完了: {json_summary_output_path}")
        else: self.logger.error(f"バッチサマリーJSONレポートの保存失敗: {save_json_summary_result.error}")

        # HTMLサマリレポート
        try:
            html_summary_content_str = self._generate_html_batch_summary_report(final_batch_summary_data)
            html_summary_output_path = self.current_batch_output_dir / "_batch_summary_report.html"
            save_html_summary_result = safe_write_file(html_summary_output_path, html_summary_content_str)
            if save_html_summary_result.is_ok: self.logger.info(f"バッチHTMLサマリーレポート保存完了: {html_summary_output_path}")
            else: self.logger.error(f"バッチHTMLサマリーレポートの保存失敗: {save_html_summary_result.error}")
        except Exception as e_html_gen_save:
            self.logger.error(f"バッチHTMLサマリーの生成または保存に失敗: {e_html_gen_save}", exc_info=True)

    def _generate_html_batch_summary_report(self, batch_summary_data: JsonDict) -> str:
        """バッチ処理のHTMLサマリーレポートを生成します。"""
        execution_time_seconds = batch_summary_data.get("total_execution_time_seconds", 0)
        formatted_execution_time_str = self._format_duration_for_html(execution_time_seconds)
        nggs_system_version = self.config.VERSION

        html_parts_list: List[str] = [
            f"<!DOCTYPE html><html lang='ja'><head><meta charset='UTF-8'>",
            f"<title>NGGS-Lite v{nggs_system_version} バッチ処理サマリー ({batch_summary_data.get('batch_id', '')})</title>",
            "<style>",
            "body { font-family: 'Meiryo UI', 'メイリオ', Meiryo, 'ヒラギノ角ゴ Pro W3', 'Hiragino Kaku Gothic Pro', Osaka, 'ＭＳ Ｐゴシック', 'MS PGothic', sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa; color: #343a40; font-size: 14px; line-height: 1.6; }",
            ".container { max-width: 1400px; margin: 20px auto; background-color: #ffffff; padding: 25px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.05); }",
            "h1, h2 { color: #2c3e50; border-bottom: 2px solid #007bff; padding-bottom: 10px; margin-top: 30px; }",
            "h1 { text-align: center; font-size: 2em; margin-bottom: 10px; }",
            ".report-meta { text-align: center; font-size: 0.9em; color: #6c757d; margin-bottom: 25px; }",
            ".summary-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 15px; margin-bottom: 25px; padding: 20px; background-color: #e9ecef; border-radius: 6px; }",
            ".summary-item { text-align: center; padding: 15px; background-color: #fff; border-radius: 6px; box-shadow: 0 2px 4px rgba(0,0,0,0.04); transition: transform 0.2s ease-in-out; }",
            ".summary-item:hover { transform: translateY(-3px); }",
            ".summary-item .value { display: block; font-size: 2.2em; font-weight: 700; color: #007bff; margin-bottom: 5px; }",
            ".summary-item .label { font-size: 0.9em; color: #495057; }",
            ".summary-item.status-ok .value { color: #28a745; }",
            ".summary-item.status-error .value { color: #dc3545; }",
            ".summary-item.status-warn .value { color: #ffc107; }",
            "table { width: 100%; border-collapse: separate; border-spacing: 0; margin-top: 25px; font-size: 0.9em; box-shadow: 0 2px 4px rgba(0,0,0,0.03); border-radius: 6px; overflow: hidden; }",
            "th, td { border-bottom: 1px solid #dee2e6; padding: 12px 15px; text-align: left; }",
            "th { background-color: #007bff; color: white; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; }",
            "tr:nth-child(even) { background-color: #f8f9fa; } tr:hover { background-color: #e9ecef; }",
            "td a { color: #0056b3; text-decoration: none; font-weight: 500; } td a:hover { text-decoration: underline; color: #003875; }",
            ".status-ok { color: #198754; font-weight: bold; }",
            ".status-error, .status-failed, .status-failed-critically, .status-failed-read-error, .status-failed-thread-exception, .status-failed-critical-sequence-error, .status-failed-unexpected-processor-error { color: #dc3545; font-weight: bold; }",
            ".status-skipped, .status-warn { color: #fd7e14; font-weight: bold; }",
            ".error-details-column { max-width: 350px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }",
            "footer { text-align: center; margin-top: 30px; font-size: 0.85em; color: #6c757d; }",
            "</style></head><body><div class='container'>",
            f"<h1>NGGS-Lite バッチ処理サマリー (v{nggs_system_version})</h1>",
            f"<p class='report-meta'>バッチID: {escape_html(batch_summary_data.get('batch_id', 'N/A'))}<br>"
            f"レポート生成日時 (UTC): {escape_html(batch_summary_data.get('batch_end_time_utc', 'N/A'))}</p>",
            "<div class='summary-grid'>",
            f"<div class='summary-item'><span class='value'>{batch_summary_data.get('total_files_found_in_input', 0)}</span><span class='label'>検出ファイル数</span></div>",
            f"<div class='summary-item'><span class='value'>{batch_summary_data.get('files_actually_processed', 0)}</span><span class='label'>処理済みファイル数</span></div>",
            f"<div class='summary-item status-ok'><span class='value'>{batch_summary_data.get('successful_processing_jobs', 0)}</span><span class='label'>成功ジョブ数</span></div>",
            f"<div class='summary-item status-error'><span class='value'>{batch_summary_data.get('failed_processing_jobs', 0)}</span><span class='label'>失敗ジョブ数</span></div>",
            f"<div class='summary-item status-warn'><span class='value'>{batch_summary_data.get('skipped_empty_or_invalid_files', 0)}</span><span class='label'>スキップファイル数</span></div>",
            f"<div class='summary-item'><span class='value'>{formatted_execution_time_str}</span><span class='label'>総処理時間</span></div>",
            "</div>",
            "<h2>個別ファイル処理結果詳細</h2>",
            "<table><thead><tr><th>ファイル名</th><th>ジョブID</th><th>ステータス</th><th>総合スコア</th><th>処理時間</th><th>エラー概要</th></tr></thead><tbody>"
        ]

        def get_status_display_class(status_text: str) -> str:
            status_text_lower = status_text.lower()
            if "fail" in status_text_lower or "error" in status_text_lower: return "status-error"
            if "skip" in status_text_lower or "warn" in status_text_lower: return "status-warn"
            if "success" in status_text_lower or "完了" in status_text: return "status-ok"
            return "" # デフォルトクラスなし

        # 結果をソート: 失敗 > スキップ > 成功 (スコア降順)
        def batch_results_sort_key_func(result_item: JsonDict):
            status_lower = result_item.get("status", "").lower()
            overall_score = result_item.get("final_score_overall", 0.0)
            # スコアがNoneや非数値の場合のフォールバック
            if not isinstance(overall_score, (int, float)): overall_score = 0.0
            
            if "fail" in status_lower or "error" in status_lower: return (2, -overall_score, result_item.get("file", ""))
            if "skip" in status_lower or "warn" in status_lower: return (1, -overall_score, result_item.get("file", ""))
            return (0, -overall_score, result_item.get("file", "")) # 成功はスコア降順

        sorted_individual_results = sorted(
            batch_summary_data.get("individual_job_results_summary", []),
            key=batch_results_sort_key_func
        )

        for result_item_data in sorted_individual_results:
            status_text_val = result_item_data.get('status', '不明')
            status_display_class = get_status_display_class(status_text_val)
            score_val_overall = result_item_data.get('final_score_overall')
            score_display_str = f"{score_val_overall:.2f}" if isinstance(score_val_overall, (int, float)) else "N/A"
            error_list_data = result_item_data.get('error_snippet', [])
            error_display_str = str(error_list_data[0]) if error_list_data and isinstance(error_list_data, list) else ""
            duration_seconds_val = result_item_data.get('duration_seconds', "N/A")
            duration_display_str = f"{duration_seconds_val:.2f}秒" if isinstance(duration_seconds_val, (int, float)) else "N/A"
            
            job_id_val_str = result_item_data.get('job_id', 'N/A')
            file_name_val_str = result_item_data.get('file', 'N/A')
            
            # 個別ジョブレポートへの相対リンク (ジョブ成功時かつIDが存在する場合)
            individual_report_link_str = ""
            # ステータスが成功系で、かつjob_idが有効な場合のみリンク生成
            if job_id_val_str != 'N/A' and "fail" not in status_text_val.lower() and "skip" not in status_text_val.lower():
                safe_job_id_for_link = re.sub(r'[^\w\-.]', '_', job_id_val_str)
                # バッチサマリーHTMLはバッチ実行ディレクトリ直下にあるため、サブディレクトリへの相対パス
                individual_report_link_str = f"./{safe_job_id_for_link}/{safe_job_id_for_link}_report.html"
            
            file_cell_content_html = f"<a href='{individual_report_link_str}'>{escape_html(file_name_val_str)}</a>" if individual_report_link_str else escape_html(file_name_val_str)

            html_parts_list.append(f"<tr><td>{file_cell_content_html}</td><td>{escape_html(job_id_val_str)}</td>")
            html_parts_list.append(f"<td class='{status_display_class}'>{escape_html(status_text_val)}</td><td>{score_display_str}</td><td>{duration_display_str}</td>")
            html_parts_list.append(f"<td class='error-details-column' title='{escape_html(error_display_str)}'>{escape_html(truncate_text(error_display_str, 75))}</td></tr>")

        html_parts_list.append("</tbody></table><footer>NGGS-Lite Batch Report</footer></div></body></html>")
        return "".join(html_parts_list)

    def _print_progress(self, is_final_progress: bool = False) -> None:
        """バッチ処理の進捗をコンソールに出力します。"""
        if self.total_files_to_process == 0 and not is_final_progress: return # 処理対象ファイル数未設定時は0除算回避
        
        percentage_complete = (self.processed_files_count / self.total_files_to_process) * 100 \
            if self.total_files_to_process > 0 else (100.0 if is_final_progress else 0.0)
        
        elapsed_time_seconds = 0.0
        if self.batch_start_time_monotonic > 0:
            elapsed_time_seconds = time.monotonic() - self.batch_start_time_monotonic
        
        elapsed_time_str = self._format_duration_for_html(elapsed_time_seconds) # HTML用を流用
        estimated_remaining_time_str = "N/A"

        if self.processed_files_count > 0 and not is_final_progress and \
           elapsed_time_seconds > 0 and self.total_files_to_process > self.processed_files_count:
            time_per_file_processed = elapsed_time_seconds / self.processed_files_count
            remaining_seconds_estimated = time_per_file_processed * (self.total_files_to_process - self.processed_files_count)
            estimated_remaining_time_str = self._format_duration_for_html(remaining_seconds_estimated)
        
        progress_bar_length = 30
        num_filled_chars = int(progress_bar_length * self.processed_files_count // self.total_files_to_process) \
            if self.total_files_to_process > 0 else (progress_bar_length if is_final_progress else 0)
        progress_bar_str = '█' * num_filled_chars + '-' * (progress_bar_length - num_filled_chars)
        
        status_counts_line = (
            f"処理済: {self.processed_files_count}/{self.total_files_to_process} | "
            f"成功: {self.successful_job_count}, 失敗: {self.failed_job_count}, スキップ: {self.skipped_file_count}"
        )
        time_info_line = f"経過: {elapsed_time_str}, 残り推定: {estimated_remaining_time_str}" \
            if not is_final_progress else f"総時間: {elapsed_time_str}"
        
        # 動的更新のためにキャリッジリターンを使用、最終表示は改行
        line_end_char = '\n' if is_final_progress else ''
        # ターミナルで行がクリアされるように、十分な長さの空白で上書き
        clear_previous_line_str = '\r' + ' ' * 130 + '\r' # 幅は適宜調整
        
        output_str = f"{clear_previous_line_str}[{progress_bar_str}] {percentage_complete:.1f}% | {status_counts_line} | {time_info_line}{line_end_char}"
        try:
            sys.stdout.write(output_str)
            sys.stdout.flush()
        except BlockingIOError: # CI環境などで発生する可能性
            print(output_str) # フォールバック
        except Exception: # その他のstdoutエラー
            print(f"進捗: {percentage_complete:.1f}%, {status_counts_line}, {time_info_line}{line_end_char}")

    def _format_duration_for_html(self, total_seconds: float) -> str:
        """秒数を H時間MM分SS.s秒 または M分SS.s秒 または S.s秒 形式にフォーマットします。"""
        if total_seconds < 0: return "0.0秒"
        
        seconds_int_part = int(total_seconds)
        milliseconds_part = int((total_seconds - seconds_int_part) * 10) # 秒の小数点以下1桁
        
        minutes_part, final_seconds_part = divmod(seconds_int_part, 60)
        hours_part, final_minutes_part = divmod(minutes_part, 60)
        
        if hours_part > 0:
            return f"{hours_part}時間{final_minutes_part:02d}分{final_seconds_part:02d}秒"
        elif final_minutes_part > 0:
            return f"{final_minutes_part}分{final_seconds_part:02d}.{milliseconds_part}秒"
        else:
            return f"{final_seconds_part}.{milliseconds_part}秒"

    def _create_empty_batch_summary(self, error_message: Optional[str] = None, no_files_found: bool = False) -> JsonDict:
        """ファイル未処理または早期エラー時に空のサマリー辞書を作成します。"""
        current_time_iso = datetime.now(timezone.utc).isoformat()
        summary_dict: JsonDict = {
            "batch_id": self.current_batch_output_dir.name if self.current_batch_output_dir else f"error_batch_{current_time_iso.replace(':','-')}",
            "batch_start_time_utc": self.batch_start_time_utc_iso if self.batch_start_time_utc_iso else current_time_iso,
            "batch_end_time_utc": current_time_iso,
            "total_execution_time_seconds": 0.0,
            "total_files_found_in_input": 0,
            "files_actually_processed": 0,
            "successful_processing_jobs": 0,
            "failed_processing_jobs": 0,
            "skipped_empty_or_invalid_files": 0,
            "individual_job_results_summary": [],
        }
        if error_message:
            summary_dict["batch_error_message"] = error_message
        if no_files_found:
            summary_dict["batch_notes"] = "指定されたパターンに一致するファイルが見つかりませんでした。"
        return summary_dict

# =============================================================================
# Part 16 End: Batch Processing Class
# =============================================================================
# =============================================================================
# Part 17: Main Execution Function and Entry Point (v1.8)
# (Continues from previous parts, assumes all necessary classes are defined)
# =============================================================================
# 以前のパートで定義されたクラスと型が利用可能であることを前提とします。
# NGGSConfig, TextProcessor, BatchProcessor, NDGSIntegration, GLCAIVocabularyFeedback,
# LLMClient, Evaluator, VocabularyManager, NarrativeFlowFramework,
# ExtendedETIEvaluator, RIEvaluator, SubjectiveEvaluator,
# Result, NGGSError, ConfigurationError, FileProcessingError, TemplateError,
# VocabularyError, IntegrationError, LLMError, EvaluationError,
# JsonDict, safe_read_file, safe_write_file, CompactJSONEncoder, load_template,
# validate_template, get_metric_display_name, truncate_text, setup_logging, loggerなど。

import argparse
import logging
import pathlib
import sys
import time
import traceback # main関数のエラーハンドリングで使用
import json # run_single_job で使用
import re # run_single_job で使用
from datetime import datetime, timezone # run_single_job で使用
from typing import (
    Dict, List, Any, Optional, Tuple, Union # Part 1からの型も含む
)

# 以前のパートで定義されたクラスをインポート (モジュール分割されている場合)
# from nggs_lite_part1 import NGGSConfig, JsonDict, ConfigurationError, BASE_DIR
# from nggs_lite_part2 import Result, NGGSError, FileProcessingError, TemplateError, VocabularyError, IntegrationError, LLMError, EvaluationError
# from nggs_lite_part3 import safe_read_file, safe_write_file, CompactJSONEncoder, setup_logging, logger, truncate_text, get_metric_display_name
# from nggs_lite_part4 import load_template, validate_template # LLMClientはPart 5で完成
# from nggs_lite_part5 import LLMClient
# from nggs_lite_part6 import VocabularyManager
# from nggs_lite_part8 import EvaluationResult, Evaluator
# from nggs_lite_part9 import ExtendedETIEvaluator, RIEvaluator
# from nggs_lite_part10 import NarrativeFlowFramework, SubjectiveEvaluator
# from nggs_lite_part14 import TextProcessor # TextProcessorはPart 14で完成
# from nggs_lite_part15 import NDGSIntegration, GLCAIVocabularyFeedback
# from nggs_lite_part16 import BatchProcessor, escape_html

# グローバルロガーインスタンス (Part 3で初期化される想定)
# logger = logging.getLogger("NGGS-Lite") # setup_loggingで取得されるインスタンス

def parse_arguments() -> argparse.Namespace:
    """NGGS-Lite v1.8のコマンドライン引数を解析します。"""
    # NGGSConfigを一時的にインスタンス化してヘルプメッセージのデフォルト値を取得
    # これは、NGGSConfigが外部ファイルなしでデフォルト値で初期化できることを前提とします。
    try:
        temp_config_for_help = NGGSConfig() # Part 1で定義
        default_paths_for_help = temp_config_for_help.get_default_paths()
        default_output_dir_str = str(default_paths_for_help.get("DEFAULT_OUTPUT_DIR", temp_config_for_help.DEFAULT_OUTPUT_DIR))
        default_max_loops_val = temp_config_for_help.DEFAULT_MAX_LOOPS
        default_threshold_val = temp_config_for_help.DEFAULT_IMPROVEMENT_THRESHOLD
        default_target_length_val = temp_config_for_help.DEFAULT_TARGET_LENGTH
        default_gemini_model_str = temp_config_for_help.GEMINI_MODEL_NAME
        default_log_file_str = temp_config_for_help.DEFAULT_LOG_FILE or "無効"
        default_templates_dir_str = str(default_paths_for_help.get("DEFAULT_TEMPLATES_DIR", temp_config_for_help.DEFAULT_TEMPLATES_DIR))
        default_nggs_vocab_str = str(default_paths_for_help.get("DEFAULT_VOCAB_PATH", temp_config_for_help.DEFAULT_VOCAB_PATH))
        default_glcai_vocab_str = str(default_paths_for_help.get("GLCAI_VOCAB_PATH", temp_config_for_help.GLCAI_VOCAB_PATH))
        default_perspective_str = getattr(temp_config_for_help, 'PERSPECTIVE_MODE_DEFAULT', "subjective_first_person")
        default_phase_focus_str = getattr(temp_config_for_help, 'PHASE_FOCUS_DEFAULT', "balanced")
        default_colloquial_level_str = getattr(temp_config_for_help, 'COLLOQUIAL_LEVEL_DEFAULT', "medium")
        default_max_workers_val = getattr(temp_config_for_help, 'BATCH_MAX_WORKERS_DEFAULT', 1)

    except Exception as e_cfg_help:
        # NGGSConfigの初期化が引数解析セットアップ中に失敗した場合のフォールバック
        print(f"警告: ヘルプテキスト用のNGGSConfigデフォルト値のロードに失敗 ({e_cfg_help})。ハードコードされたフォールバック値を使用します。", file=sys.stderr)
        default_output_dir_str = "./nggs_lite_output_v1.8_fb"
        default_max_loops_val = 3; default_threshold_val = 4.3; default_target_length_val = 1200
        default_gemini_model_str = "models/gemini-1.5-pro-latest"; default_log_file_str = "nggs_lite_v1_8_fb.log"
        default_templates_dir_str = "./templates_v1.8_fb"; default_nggs_vocab_str = "./data/gothic_vocab_v1.8_fb.json"
        default_glcai_vocab_str = "./data/glcai_export_latest_fb.json"
        default_perspective_str = "subjective_first_person"; default_phase_focus_str = "balanced"
        default_colloquial_level_str = "medium"; default_max_workers_val = 1

    # NGGSConfig.VERSION はクラス変数としてアクセス可能と仮定
    nggs_version_str = getattr(NGGSConfig, 'VERSION', '1.8.0') # フォールバックバージョン

    arg_parser = argparse.ArgumentParser(
        description=f"NGGS-Lite v{nggs_version_str}: 新ゴシック文体生成システム (v1.8)。",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter # デフォルト値をヘルプに表示
    )

    # --- 入力/出力オプション ---
    input_source_group = arg_parser.add_mutually_exclusive_group(required=False)
    input_source_group.add_argument("--input", "-i", type=str,
                                    help="シングルモード用の入力テキストファイルパス (UTF-8)。")
    input_source_group.add_argument("--text", "-t", type=str,
                                    help="シングルモード用の直接入力テキスト文字列。")
    input_source_group.add_argument("--batch", "-b", type=str, metavar="INPUT_DIR",
                                    help="バッチモード。INPUT_DIR内のファイルを処理。")
    
    arg_parser.add_argument("--ndgs-input", "-n", type=str, metavar="NDGS_JSON_FILE",
                            help="NDGS出力JSONファイルへのパス。提供された場合、これが主要な入力ソースとなり、"
                                 "--input/--textの初期コンテキストを上書きする可能性があります。")
    
    arg_parser.add_argument("--output", "-o", type=str, default=None, # デフォルトはconfigで処理
                            help=f"結果の出力ディレクトリ。(デフォルト: '{default_output_dir_str}')")

    # --- コア処理パラメータ ---
    arg_parser.add_argument("--loops", "-l", type=int, default=None, choices=range(0, 11), metavar="[0-10]",
                            help=f"最大改善ループ数。0で評価のみ。(デフォルト: {default_max_loops_val})")
    arg_parser.add_argument("--length", "--len", type=int, default=None, metavar="CHARS",
                            help=f"おおよその目標テキスト長（文字数）。(デフォルト: {default_target_length_val})")
    arg_parser.add_argument("--threshold", "--thresh", type=float, default=None, metavar="[0.0-5.0]",
                            help=f"早期停止のためのスコア閾値。(デフォルト: {default_threshold_val:.1f})")

    # --- ファイルパス関連 ---
    arg_parser.add_argument("--templates", type=str, default=None, metavar="DIR_PATH",
                            help=f"プロンプトテンプレートを含むディレクトリ。(デフォルト: '{default_templates_dir_str}')")
    arg_parser.add_argument("--nggs-vocab", type=str, default=None, metavar="FILE_PATH",
                            help=f"NGGSデフォルト語彙JSONファイルへのパス。(デフォルト: '{default_nggs_vocab_str}')")
    arg_parser.add_argument("--glcai-vocab", type=str, default=None, metavar="FILE_PATH",
                            help=f"GLCAIエクスポート語彙JSONファイルへのパス。(デフォルト: '{default_glcai_vocab_str}')")

    # --- スタイル制御 ---
    arg_parser.add_argument("--perspective", type=str,
                            choices=["subjective_first_person", "perspective_shift", "dream_perspective", "objective_third_person"], # "objective" -> "objective_third_person"
                            default=None, help=f"視点モード。(デフォルト: '{default_perspective_str}')")
    arg_parser.add_argument("--phase-focus", type=str,
                            choices=["balanced", "serif", "monologue", "narration", "live_report"],
                            default=None, help=f"優先する物語の位相。(デフォルト: '{default_phase_focus_str}')")
    arg_parser.add_argument("--colloquial-level", type=str, choices=["high", "medium", "low"],
                            default=None, help=f"口語表現のレベル。(デフォルト: '{default_colloquial_level_str}')")
    arg_parser.add_argument("--emotion-arc", type=str, default=None, metavar="ARC_STR",
                            help="望ましい感情の弧の文字列 (例: '違和感->恐怖->啓示')。")

    # --- LLMエンジン設定 ---
    arg_parser.add_argument("--model", type=str, default=None, metavar="MODEL_NAME",
                            help=f"使用するLLMモデル名。(デフォルト: '{default_gemini_model_str}')")

    # --- 実行モードとオプション ---
    arg_parser.add_argument("--skip-initial-generation", "--skip-gen", action="store_true",
                            help="初期生成をスキップし、入力テキストを直接評価します。")
    arg_parser.add_argument("--max-workers", type=int, default=None, metavar="N",
                            help=f"バッチモードでの並列ワーカー数 (1で逐次処理)。(デフォルト: {default_max_workers_val})")
    arg_parser.add_argument("--no-individual-results", "--no-indiv", action="store_true",
                            help="バッチモード時、個々のジョブ結果ファイル(JSON/HTML/TXT)を保存しません。")
    arg_parser.add_argument("--verbose", "-v", action="count", default=0,
                            help="ログの詳細度を上げます (-v でINFO, -vv でDEBUG)。")
    arg_parser.add_argument("--log-file", type=str, default=None, # デフォルトNoneはconfigのデフォルトを使用
                            help=f"ログファイルへのパス。「NONE」でファイルログ無効。(デフォルト: '{default_log_file_str}')")
    arg_parser.add_argument("--mock", action="store_true",
                            help="テスト用にLLMモックモードを強制します。")

    parsed_arguments = arg_parser.parse_args()

    # 入力ソースの事後検証
    if not parsed_arguments.batch and not parsed_arguments.input and \
       not parsed_arguments.text and not parsed_arguments.ndgs_input:
        arg_parser.error("入力ソースが指定されていません。--input, --text, --batch, または --ndgs-input のいずれかを使用してください。")

    return parsed_arguments


def setup_components(
    effective_config: NGGSConfig, # コマンドライン引数で更新済みのconfig
    parsed_args: argparse.Namespace
) -> Tuple[TextProcessor, Optional[GLCAIVocabularyFeedback], Optional[NDGSIntegration]]: # NDGSIntegrationも返す
    """NGGS-Lite v1.8の必須コンポーネントを初期化し、接続します。"""
    setup_logger = logging.getLogger("NGGS-Lite.Setup") # Part 3で定義されたグローバルloggerから派生
    setup_logger.info(f"--- NGGS-Lite v{effective_config.VERSION} コンポーネントセットアップ開始 ---")

    # --- LLMクライアント ---
    setup_logger.info(f"LLMClient初期化中 (エンジン: {effective_config.LLM_ENGINE}, モック: {parsed_args.mock})...")
    # LLMClientはPart 5で完成
    llm_client_instance = LLMClient(config=effective_config, use_mock=parsed_args.mock)
    # 実際に使用されるエンジンでconfigを更新 (モックモードへのフォールバックなど考慮)
    effective_config.LLM_ENGINE = llm_client_instance.current_engine

    # --- プロンプトテンプレートのロード ---
    # template_dirは args > config の優先順位
    templates_directory_path_str = parsed_args.templates if parsed_args.templates else effective_config.DEFAULT_TEMPLATES_DIR
    templates_dir_resolved_path = pathlib.Path(templates_directory_path_str).resolve()
    setup_logger.info(f"プロンプトテンプレートを '{templates_dir_resolved_path}' からロード中...")
    
    # DEFAULT_TEMPLATESはPart 4で定義されたグローバル辞書
    loaded_templates_map: Dict[str, str] = {}
    for template_name_key in ["generation", "evaluation", "improvement"]:
        # load_templateはPart 4で定義
        template_file_path = templates_dir_resolved_path / f"{template_name_key}.txt" # .txt拡張子を想定
        load_template_result = load_template(template_file_path, template_name_key, effective_config) # configを渡すように変更
        if load_template_result.is_err:
            template_load_error = load_template_result.error
            setup_logger.critical(f"必須テンプレート '{template_name_key}' のロード失敗: {template_load_error}")
            raise template_load_error # TemplateErrorを送出
        loaded_templates_map[template_name_key] = load_template_result.unwrap()
    setup_logger.info("プロンプトテンプレートのロード成功。")

    # --- VocabularyManager ---
    setup_logger.info("VocabularyManager初期化中...")
    # VocabularyManagerはPart 6&7で完成
    vocab_manager_instance = VocabularyManager(effective_config)
    if not vocab_manager_instance.has_vocabulary(): # has_vocabularyはPart 7で定義
        setup_logger.warning("VocabularyManagerは初期化されましたが、語彙がロードされていません。")
    else:
        setup_logger.info(f"VocabularyManager初期化成功。ロード元: {vocab_manager_instance.loaded_source_description}")

    # --- NarrativeFlowFramework ---
    setup_logger.info("NarrativeFlowFramework初期化中...")
    # NarrativeFlowFrameworkはPart 10で完成
    narrative_flow_instance = NarrativeFlowFramework(vocab_manager_instance, effective_config)
    setup_logger.info("NarrativeFlowFramework初期化完了。")

    # --- 各種評価者 ---
    setup_logger.info("各種評価者初期化中...")
    # EvaluatorはPart 8で完成
    base_llm_evaluator = Evaluator(llm_client_instance, effective_config, loaded_templates_map["evaluation"])
    # ExtendedETIEvaluator, RIEvaluatorはPart 9で完成
    eti_evaluator_instance = ExtendedETIEvaluator(effective_config, narrative_flow_instance)
    ri_evaluator_instance = RIEvaluator(effective_config)
    # SubjectiveEvaluatorはPart 10で完成
    subjective_evaluator_instance = SubjectiveEvaluator(effective_config)
    setup_logger.info("各種評価者の初期化完了。")

    # --- NDGSIntegrationパーサー (オプション) ---
    ndgs_parser_instance: Optional[NDGSIntegration] = None # NDGSIntegrationはPart 15で定義
    if parsed_args.ndgs_input or getattr(effective_config, 'ENABLE_NDGS_PARSER_DEFAULT', False): # 設定でも有効化可能
        setup_logger.info("NDGSIntegrationパーサー初期化中...")
        ndgs_parser_instance = NDGSIntegration(effective_config)
        setup_logger.info("NDGSIntegrationパーサー初期化完了。")
    else:
        setup_logger.info("NDGSIntegrationパーサーは無効です。")

    # --- GLCAIVocabularyFeedback (オプション) ---
    # 出力ディレクトリパス (args.output > config.DEFAULT_OUTPUT_DIR)
    # このディレクトリは、個々のジョブ出力やバッチ実行のベースとなる
    # GLCAIフィードバックは、この下に専用サブディレクトリを作成する
    final_output_dir_path = pathlib.Path(parsed_args.output if parsed_args.output else effective_config.DEFAULT_OUTPUT_DIR).resolve()
    glcai_feedback_instance: Optional[GLCAIVocabularyFeedback] = None # GLCAIVocabularyFeedbackはPart 15で定義
    if getattr(effective_config, 'ENABLE_GLCAI_FEEDBACK_DEFAULT', True): # 設定で有効化可能 (デフォルトTrue)
        setup_logger.info(f"GLCAIVocabularyFeedback初期化中 (出力ベースディレクトリ: {final_output_dir_path})...")
        # GLCAIVocabularyFeedbackの__init__は、この下に専用サブディレクトリを作成する
        glcai_feedback_instance = GLCAIVocabularyFeedback(effective_config, final_output_dir_path)
        setup_logger.info("GLCAIVocabularyFeedback初期化完了。")
    else:
        setup_logger.info("GLCAIVocabularyFeedbackは無効です。")
        
    # --- TextProcessor ---
    setup_logger.info("TextProcessor初期化中...")
    # TextProcessorはPart 14で完成
    text_processor_instance = TextProcessor(
        config=effective_config, llm_client=llm_client_instance, evaluator=base_llm_evaluator,
        vocab_manager=vocab_manager_instance, narrative_flow=narrative_flow_instance,
        eti_evaluator=eti_evaluator_instance, ri_evaluator=ri_evaluator_instance,
        subjective_evaluator=subjective_evaluator_instance,
        generation_template=loaded_templates_map["generation"],
        improvement_template=loaded_templates_map["improvement"],
        ndgs_parser=ndgs_parser_instance, # NDGSパーサーを渡す
        glcai_feedback=glcai_feedback_instance # GLCAIフィードバックを渡す (TextProcessor.__init__の修正が必要)
    )
    setup_logger.info("TextProcessor初期化完了。")
    setup_logger.info("--- 全コンポーネントのセットアップ完了 ---")
    
    # GLCAIVocabularyFeedbackはTextProcessorに渡すので、個別に戻す必要はないかもしれないが、
    # mainから直接使うケース（バッチ処理以外での単独フィードバックなど）を考慮するなら戻す。
    # プランではrun_single_jobに渡しているので、戻り値に含める。
    return text_processor_instance, glcai_feedback_instance, ndgs_parser_instance


def run_single_job(
    parsed_args: argparse.Namespace,
    effective_config: NGGSConfig,
    text_processor: TextProcessor,
    # GLCAIVocabularyFeedbackとNDGSIntegrationを引数に追加
    glcai_feedback_tracker: Optional[GLCAIVocabularyFeedback],
    ndgs_integration_parser: Optional[NDGSIntegration],
    base_output_directory: pathlib.Path # 個別ジョブ結果の保存先ベース
) -> JsonDict: # 処理結果の辞書を返す
    """単一の生成ジョブを実行し、結果を保存し、フィードバックを処理します。"""
    single_job_logger = logging.getLogger("NGGS-Lite.SingleJobRunner")
    single_job_logger.info("--- シングルジョブ処理開始 ---")

    initial_text_content_for_process: str = ""
    ndgs_parsed_data_for_process: Optional[JsonDict] = None
    input_source_description: str = "不明"

    # 入力ソースの決定 (NDGS > text > input)
    if parsed_args.ndgs_input:
        ndgs_file_path = pathlib.Path(parsed_args.ndgs_input).resolve()
        input_source_description = f"NDGS入力ファイル: {ndgs_file_path.name}"
        single_job_logger.info(f"NDGS入力ファイルを処理中: {ndgs_file_path}")
        if not ndgs_integration_parser:
            raise ConfigurationError("NDGS入力が指定されましたが、NDGSパーサーが初期化されていません。")
        
        # NDGSIntegration.parse_from_fileはResult[JsonDict, IntegrationError]を返す
        # IntegrationErrorはPart 2で定義
        parse_ndgs_result = ndgs_integration_parser.parse_from_file(ndgs_file_path)
        if parse_ndgs_result.is_err:
            parse_error = parse_ndgs_result.error
            single_job_logger.error(f"NDGS入力ファイルの解析に失敗しました: {parse_error}")
            raise IntegrationError(f"NDGS入力の解析失敗: {parse_error}") from parse_error
        ndgs_parsed_data_for_process = parse_ndgs_result.unwrap()
        # initial_textはTextProcessor.process内でndgs_parsed_data_for_processから取得される
        # ここではログ用に取得
        initial_text_content_for_process = ndgs_parsed_data_for_process.get("initial_text", "")
    elif parsed_args.text:
        initial_text_content_for_process = parsed_args.text
        input_source_description = "コマンドライン引数 (--text)"
    elif parsed_args.input:
        input_file_path = pathlib.Path(parsed_args.input).resolve()
        input_source_description = f"入力ファイル: {input_file_path.name}"
        single_job_logger.info(f"入力テキストファイルを読み込み中: {input_file_path}")
        # safe_read_fileはResult[str, FileProcessingError]を返す (Part 3で定義)
        read_file_result = safe_read_file(input_file_path)
        if read_file_result.is_err:
            read_error = read_file_result.error
            single_job_logger.error(f"入力ファイル {input_file_path} の読み込み失敗: {read_error}")
            raise read_error # FileProcessingErrorを送出
        initial_text_content_for_process = read_file_result.unwrap()
    else: # このケースはargparseで捕捉されるはず
        raise ConfigurationError("シングルジョブの有効な入力ソースが提供されていません。")

    single_job_logger.info(f"入力ソース: {input_source_description}")
    if initial_text_content_for_process and len(initial_text_content_for_process) < 250: # 短い入力はログに
        single_job_logger.debug(f"入力テキストプレビュー: {initial_text_content_for_process[:120]}...")
    elif ndgs_parsed_data_for_process:
        single_job_logger.debug(f"NDGSデータ辞書で処理。NDGSからの初期テキストプレビュー: {truncate_text(initial_text_content_for_process, 60)}")

    # TextProcessor.processを呼び出し
    # TextProcessor.processはResult[JsonDict, NGGSError]を返す (Part 12で定義)
    # job_idはTextProcessor.process内で生成される
    processing_result_obj = text_processor.process(
        initial_text_input=initial_text_content_for_process, # ファイルから読み込んだテキスト
        target_length_override=parsed_args.length,
        perspective_mode_override=parsed_args.perspective,
        phase_focus_override=parsed_args.phase_focus,
        colloquial_level_override=parsed_args.colloquial_level,
        emotion_arc_override=parsed_args.emotion_arc,
        max_loops_override=parsed_args.loops,
        improvement_threshold_override=parsed_args.threshold,
        narrative_flow_prompt_override=None, # シングルジョブでは通常CLIから直接は指定しない想定
        skip_initial_generation_flag=parsed_args.skip_initial_generation,
        ndgs_input_data_dict=ndgs_parsed_data_for_process # 解析済みのNDGSデータを渡す
    )

    # 結果の処理
    if processing_result_obj.is_ok:
        job_result_dict = processing_result_obj.unwrap()
        current_job_id = job_result_dict.get("job_id", "unknown_single_job")
        single_job_logger.info(f"シングルジョブ ({current_job_id}) 完了。ステータス: {job_result_dict.get('overall_status', '不明')}")

        # 結果保存ディレクトリの準備 (ジョブIDごとのサブディレクトリ)
        # base_output_directoryはmain()から渡される (例: ./nggs_lite_output_v1.8/)
        safe_job_id_for_filename = re.sub(r'[^\w\-.]', '_', current_job_id)
        job_specific_output_path = base_output_directory / safe_job_id_for_filename
        try:
            job_specific_output_path.mkdir(parents=True, exist_ok=True)
        except OSError as e_mkdir_single_job:
            single_job_logger.error(f"ジョブ固有出力ディレクトリ {job_specific_output_path} の作成失敗: {e_mkdir_single_job}。ベース出力ディレクトリに保存します。")
            job_specific_output_path = base_output_directory # フォールバック

        # JSON結果保存 (CompactJSONEncoderはPart 3で定義)
        json_output_file_path = job_specific_output_path / f"{safe_job_id_for_filename}_results.json"
        save_json_op_result = safe_write_file(json_output_file_path, json.dumps(job_result_dict, ensure_ascii=False, indent=2, cls=CompactJSONEncoder))
        if save_json_op_result.is_ok: single_job_logger.info(f"ジョブ結果保存完了: {json_output_file_path.name}")
        else: single_job_logger.error(f"ジョブ {current_job_id} のJSON結果保存失敗: {save_json_op_result.error}")

        # HTMLレポート保存 (TextProcessorが生成した場合)
        # html_report_content_path = job_result_dict.get("html_report_path") # TextProcessorがパスを返す場合
        # または、HTML内容そのものが含まれる場合:
        html_report_content = job_result_dict.get("html_report") # Part 14でTextProcessor._finalize_resultsに追加予定
        if html_report_content and isinstance(html_report_content, str):
            html_output_file_path = job_specific_output_path / f"{safe_job_id_for_filename}_report.html"
            save_html_op_result = safe_write_file(html_output_file_path, html_report_content)
            if save_html_op_result.is_ok: single_job_logger.info(f"HTMLレポート保存完了: {html_output_file_path.name}")
            else: single_job_logger.error(f"ジョブ {current_job_id} のHTMLレポート保存失敗: {save_html_op_result.error}")
        elif "Fail" not in job_result_dict.get("overall_status", ""): # 失敗ジョブでなければデバッグログ
             single_job_logger.debug(f"ジョブ {current_job_id} のHTMLレポート内容が見つかりません。")


        # 最終テキスト保存
        final_generated_text = job_result_dict.get("final_generated_text")
        if final_generated_text and isinstance(final_generated_text, str):
            text_output_file_path = job_specific_output_path / f"{safe_job_id_for_filename}_final.txt"
            save_text_op_result = safe_write_file(text_output_file_path, final_generated_text)
            if save_text_op_result.is_ok: single_job_logger.info(f"最終テキスト保存完了: {text_output_file_path.name}")
            else: single_job_logger.error(f"ジョブ {current_job_id} の最終テキスト保存失敗: {save_text_op_result.error}")
        elif "Fail" not in job_result_dict.get("overall_status", ""):
            single_job_logger.debug(f"ジョブ {current_job_id} の最終テキスト内容が見つかりません。")

        # GLCAIフィードバック処理 (TextProcessor.process内でjob_id指定で実行されるため、ここでは不要)
        # ただし、TextProcessorにglcai_feedbackインスタンスが渡されていることが前提
        # (setup_componentsとTextProcessor.__init__で対応済み)
        # もしTextProcessor.processがjob_idを返さない設計なら、ここでglcai_feedbackを呼び出す
        if glcai_feedback_tracker: # インスタンスが渡されていれば
            # TextProcessor.processがGLCAIフィードバックを内部で処理しない場合、ここで呼び出す
            # (現在のプランではTextProcessor.processが内部で処理)
            # glcai_feedback_tracker.start_tracking_for_job(current_job_id)
            # if final_generated_text and text_processor.vocab_manager:
            #     glcai_feedback_tracker.track_vocabulary_usage(final_generated_text, text_processor.vocab_manager.items)
            #     save_glcai_res = glcai_feedback_tracker.save_feedback_to_file()
            #     if save_glcai_res.is_err: single_job_logger.error(f"GLCAIフィードバック保存失敗({current_job_id}): {save_glcai_res.error}")
            pass # TextProcessor.process内で処理される想定

        # コンソールへのサマリー出力
        print("\n" + "="*35 + f"\n--- 生成テキスト (ジョブID: {current_job_id}) ---\n" + "="*35)
        print(final_generated_text if final_generated_text else "(エラーまたはテキスト生成なし)")
        print("="*35 + "\n最終主要スコア:")
        final_aggregated_scores_map = job_result_dict.get('final_aggregated_scores', {})
        if final_aggregated_scores_map:
            # 表示順序を定義
            score_display_order = [
                "overall_quality", "eti_total_calculated", "ri_total_calculated", "subjective_score",
                "phase_score", "layer_balance_score", "emotion_arc_score", "colloquial_score",
                # LLM基本評価もいくつか表示
                "gothic_atmosphere", "stylistic_gravity", "subjective_depth"
            ]
            displayed_keys_set = set()
            for key_to_display in score_display_order:
                if key_to_display in final_aggregated_scores_map:
                    # get_metric_display_nameはPart 3で定義
                    print(f"  - {get_metric_display_name(key_to_display)}: {final_aggregated_scores_map[key_to_display]:.2f}")
                    displayed_keys_set.add(key_to_display)
            # 残りのスコアも表示 (もしあれば)
            for remaining_key, score_value in final_aggregated_scores_map.items():
                if remaining_key not in displayed_keys_set:
                    print(f"  - {get_metric_display_name(remaining_key)}: {score_value:.2f}")
        else:
            print("  利用可能なスコアなし")
        print(f"\n完全なレポートは次の場所に保存されました: {job_specific_output_path.resolve()}")
        print("="*35 + "\n")
        return job_result_dict # 成功時は結果辞書を返す
    else: # TextProcessor.processが致命的エラーを返した場合
        processor_fatal_error = processing_result_obj.error
        single_job_logger.critical(f"シングルジョブが致命的エラーで失敗しました: {processor_fatal_error}")
        # 失敗時の最小限の辞書を返す
        return {
            "job_id": effective_config.generate_job_id("critical_single_fail_"),
            "status": "失敗 (TextProcessor致命的エラー)",
            "errors": [str(processor_fatal_error)],
            "start_time_utc": datetime.now(timezone.utc).isoformat(),
            "completion_time_utc": datetime.now(timezone.utc).isoformat(),
            "parameters_used": vars(parsed_args) # argparseの結果をそのまま記録
        }


def main() -> int:
    """NGGS-Lite v1.8のメイン実行関数。"""
    # グローバルロガーインスタンス (Part 3で初期化される想定)
    # この関数内で再度setup_loggingを呼び、引数に基づいて再設定する
    global logger # pylint: disable=global-statement
    
    # 引数解析の前に、非常に基本的なフォールバックロガーを設定
    # これにより、argparseや初期NGGSConfigロード中のエラーも記録試行可能
    logging.basicConfig(level=logging.WARNING, format='%(asctime)s [%(levelname)-7s] %(message)s')
    logger = logging.getLogger("NGGS-Lite.PreMain") # 仮のロガー名

    parsed_cli_args = parse_arguments()
    exit_code_status = 0 # 成功をデフォルトとする
    global_start_time_monotonic = time.monotonic()
    
    effective_nggs_config: Optional[NGGSConfig] = None # スコープ外エラー時の参照用

    try:
        # --- 基本設定とロギングのセットアップ ---
        effective_nggs_config = NGGSConfig() # デフォルトで基本設定をロード (Part 1で定義)
        
        # コマンドライン引数に基づいてログレベルを決定
        log_level_to_set_str: str
        if parsed_cli_args.verbose == 1: log_level_to_set_str = "INFO"
        elif parsed_cli_args.verbose >= 2: log_level_to_set_str = "DEBUG"
        else: log_level_to_set_str = effective_nggs_config.DEFAULT_LOG_LEVEL
        
        # ログファイルパスの決定 (引数 > configデフォルト)
        # 引数で "NONE" が指定された場合はファイルログ無効
        log_file_path_from_args = parsed_cli_args.log_file
        final_log_file_path: Optional[str]
        if log_file_path_from_args is not None: # 引数で明示的に指定された場合
            final_log_file_path = None if log_file_path_from_args.upper() == 'NONE' else log_file_path_from_args
        else: # 引数指定なし -> configのデフォルトを使用
            final_log_file_path = effective_nggs_config.DEFAULT_LOG_FILE

        # setup_loggingを呼び出してロガーを正式に設定 (Part 3で定義)
        # これによりグローバル`logger`が再設定される
        logger = setup_logging(
            log_level_str=log_level_to_set_str,
            log_file=final_log_file_path,
            name="NGGS-Lite", # ルートロガー名
            config_instance=effective_nggs_config
        )
        logger.info(f"--- NGGS-Lite v{effective_nggs_config.VERSION} 初期化開始 ---")
        logger.debug(f"コマンドライン引数 (Raw): {sys.argv}")
        logger.debug(f"コマンドライン引数 (Parsed): {parsed_cli_args}")

        # --- NGGSConfigをコマンドライン引数でオーバーライド ---
        if parsed_cli_args.model is not None: effective_nggs_config.GEMINI_MODEL_NAME = parsed_cli_args.model
        if parsed_cli_args.nggs_vocab is not None: effective_nggs_config.DEFAULT_VOCAB_PATH = parsed_cli_args.nggs_vocab
        if parsed_cli_args.glcai_vocab is not None: effective_nggs_config.GLCAI_VOCAB_PATH = parsed_cli_args.glcai_vocab
        if parsed_cli_args.output is not None: effective_nggs_config.DEFAULT_OUTPUT_DIR = parsed_cli_args.output
        if parsed_cli_args.loops is not None: effective_nggs_config.DEFAULT_MAX_LOOPS = parsed_cli_args.loops
        if parsed_cli_args.length is not None: effective_nggs_config.DEFAULT_TARGET_LENGTH = parsed_cli_args.length
        if parsed_cli_args.threshold is not None: effective_nggs_config.DEFAULT_IMPROVEMENT_THRESHOLD = parsed_cli_args.threshold
        if parsed_cli_args.perspective is not None: setattr(effective_nggs_config, 'PERSPECTIVE_MODE_DEFAULT', parsed_cli_args.perspective) # configに存在すれば
        if parsed_cli_args.phase_focus is not None: setattr(effective_nggs_config, 'PHASE_FOCUS_DEFAULT', parsed_cli_args.phase_focus)
        if parsed_cli_args.colloquial_level is not None: setattr(effective_nggs_config, 'COLLOQUIAL_LEVEL_DEFAULT', parsed_cli_args.colloquial_level)
        if parsed_cli_args.max_workers is not None: setattr(effective_nggs_config, 'BATCH_MAX_WORKERS_DEFAULT', parsed_cli_args.max_workers)
        # 他のconfigオーバーライドもここに追加

        logger.debug(f"有効な設定 (コマンドライン引数適用後): {effective_nggs_config}")

        # 出力ディレクトリの決定と作成 (args.output > config.DEFAULT_OUTPUT_DIR)
        final_output_base_dir = pathlib.Path(
            parsed_cli_args.output if parsed_cli_args.output else effective_nggs_config.DEFAULT_OUTPUT_DIR
        ).resolve()
        logger.info(f"メイン出力ディレクトリ: {final_output_base_dir}")
        try:
            final_output_base_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e_mkdir_main_output:
            logger.critical(f"メイン出力ディレクトリ {final_output_base_dir} の作成に失敗: {e_mkdir_main_output}")
            # ConfigurationErrorはPart 2で定義
            raise ConfigurationError(f"メイン出力ディレクトリ作成失敗: {e_mkdir_main_output}") from e_mkdir_main_output

        # --- 全コンポーネントの初期化と接続 ---
        # setup_componentsはTextProcessor, GLCAIVocabularyFeedback, NDGSIntegrationを返す
        text_processor_main, glcai_feedback_main, ndgs_parser_main = setup_components(effective_nggs_config, parsed_cli_args)

        # --- 実行モードの分岐 ---
        if parsed_cli_args.batch: # バッチモード
            input_dir_for_batch = pathlib.Path(parsed_cli_args.batch).resolve()
            # BatchProcessorはPart 16で定義
            batch_processor_instance = BatchProcessor(effective_nggs_config, text_processor_main, final_output_base_dir)
            
            # バッチ処理用のTextProcessor.process共通引数を準備 (None値を除外)
            batch_common_processor_args: Dict[str, Any] = {
                "target_length_override": parsed_cli_args.length,
                "perspective_mode_override": parsed_cli_args.perspective,
                "phase_focus_override": parsed_cli_args.phase_focus,
                "colloquial_level_override": parsed_cli_args.colloquial_level,
                "emotion_arc_override": parsed_cli_args.emotion_arc,
                "max_loops_override": parsed_cli_args.loops,
                "improvement_threshold_override": parsed_cli_args.threshold,
                "skip_initial_generation_flag": parsed_cli_args.skip_initial_generation
                # narrative_flow_promptはバッチでは通常共通指定しない (ファイルごとに異なる可能性)
                # ndgs_input_data_dictは_process_single_file内でファイルごとに処理
            }
            batch_common_processor_args_filtered = {
                k: v for k, v in batch_common_processor_args.items() if v is not None
            }

            batch_summary_results = batch_processor_instance.process_batch(
                input_directory_path_str=input_dir_for_batch,
                max_workers_override=parsed_cli_args.max_workers, # 引数で指定されたワーカー数
                file_glob_pattern=getattr(effective_nggs_config, 'BATCH_FILE_PATTERN_DEFAULT', "*.txt"), # configから取得
                save_individual_job_results=(not parsed_cli_args.no_individual_results),
                **batch_common_processor_args_filtered
            )
            # バッチ処理の結果に基づいて終了コードを設定
            if batch_summary_results.get("failed_processing_jobs", 0) > 0:
                exit_code_status = 1
        else: # シングルジョブモード (input, text, または ndgs_input のいずれかが指定されているはず)
            single_job_final_results = run_single_job(
                parsed_args=parsed_cli_args,
                effective_config=effective_nggs_config,
                text_processor=text_processor_main,
                glcai_feedback_tracker=glcai_feedback_main,
                ndgs_integration_parser=ndgs_parser_main,
                base_output_directory=final_output_base_dir # シングルジョブの結果もこの下に
            )
            # シングルジョブの結果に基づいて終了コードを設定
            if "fail" in single_job_final_results.get("status", "").lower() or \
               "error" in single_job_final_results.get("status", "").lower():
                exit_code_status = 1
        
    # --- エラーハンドリング (main関数全体をカバー) ---
    # NGGSErrorとそのサブクラスを具体的に捕捉 (Part 2で定義)
    except (ConfigurationError, FileProcessingError, TemplateError, VocabularyError,
            IntegrationError, LLMError, EvaluationError, NGGSError) as e_nggs_specific:
        logger.critical(f"処理中にNGGSシステムエラーが発生しました: {type(e_nggs_specific).__name__} - {e_nggs_specific}", exc_info=False) # 詳細ログはエラー発生源で
        if isinstance(e_nggs_specific, NGGSError) and e_nggs_specific.get_context():
            logger.critical(f"エラーコンテキスト: {e_nggs_specific.get_context()}")
        print(f"\nエラー: {e_nggs_specific}", file=sys.stderr)
        exit_code_status = 1
    # Python標準のファイル関連エラー
    except (FileNotFoundError, PermissionError) as e_file_access:
        logger.critical(f"ファイルアクセスエラー: {type(e_file_access).__name__} - {e_file_access}", exc_info=False)
        print(f"\nファイルアクセスエラー: {e_file_access}", file=sys.stderr)
        exit_code_status = 1
    # その他の予期せぬエラー
    except KeyboardInterrupt: # Ctrl+Cによる中断
        logger.warning("処理がユーザーによって中断されました (Ctrl+C)。")
        print("\n処理が中断されました。", file=sys.stderr)
        exit_code_status = 130 # SIGINTの標準終了コード
    except Exception as e_unexpected_main:
        logger.critical(f"予期せぬ致命的なエラーがmain関数で発生しました: {type(e_unexpected_main).__name__} - {e_unexpected_main}", exc_info=True)
        print(f"\n予期せぬ致命的エラー: {type(e_unexpected_main).__name__} - {e_unexpected_main}。", file=sys.stderr)
        print("詳細はログファイルを確認するか、-vv オプション付きで再実行してください。", file=sys.stderr)
        exit_code_status = 1
    finally:
        total_duration_seconds = time.monotonic() - global_start_time_monotonic
        # effective_nggs_configが初期化されていればバージョン情報を使用
        system_version_for_log = getattr(effective_nggs_config, 'VERSION', 'UNKNOWN') if effective_nggs_config else 'UNKNOWN_CFG_FAIL'
        
        # loggerが利用可能か確認してからログ出力
        if logger and logger.handlers:
            logger.info(f"総実行時間: {total_duration_seconds:.2f} 秒")
            logger.info(f"--- NGGS-Lite v{system_version_for_log} 実行終了 (終了コード: {exit_code_status}) ---")
        else: # ロガーが完全に失敗した場合のフォールバック
            print(f"総実行時間: {total_duration_seconds:.2f} 秒", file=sys.stderr)
            print(f"--- NGGS-Lite v{system_version_for_log} 実行終了 (終了コード: {exit_code_status}) ---", file=sys.stderr)
        
        if logging.getLogger().handlers: # ルートロガーにハンドラがあればシャットダウン
            logging.shutdown()

    return exit_code_status

# =============================================================================
# Part 17 End: Main Execution Function and Entry Point
# (The `if __name__ == "__main__":` block will be in Part 18)
# =============================================================================
# =============================================================================
# Part 18: Script Entry Point (v1.8)
# (Completes the NGGS-Lite v1.8 script)
# =============================================================================
# 以前のパートで定義された main() 関数、および CONCURRENT_FUTURES_AVAILABLE, logger,
# signal, sys, logging, traceback といったモジュールが利用可能であることを前提とします。

import signal # Part 1でインポート済みだが、このスコープで直接使用するため明示
import sys # Part 1でインポート済み
import logging # Part 1でインポート済み
import traceback # main関数のエラーハンドリングで使用 (Part 17で定義)

# CONCURRENT_FUTURES_AVAILABLE はPart 1で定義済みと仮定
# from nggs_lite_part1 import CONCURRENT_FUTURES_AVAILABLE
# main 関数はPart 17で定義済みと仮定
# from nggs_lite_part17 import main
# logger はPart 3でグローバル初期化され、Part 17のmain内で再設定される想定
# global logger # これはmain()の中で宣言される

if __name__ == "__main__":
    # --- 基本的なシグナルハンドリング ---
    # 一般的な終了シグナルに対して、可能な限り安全な終了を試みるシグナルハンドラを設定します。
    # この機能は、プラットフォームによっては完全にはサポートされない場合があります
    # (例: Windowsは全てのシグナルをサポートしていません)。

    # グローバルなloggerインスタンスを参照 (main関数内で設定される)
    # ここでは、main関数が呼び出される前にloggerが基本的な設定で利用可能であることを期待
    # (Part 17のmain関数の先頭で基本的なロギング設定がされる想定)
    entry_point_logger = logging.getLogger("NGGS-Lite.EntryPoint")

    def graceful_signal_handler(signal_number: int, frame: Optional[Any]) -> None:
        """終了シグナルを適切に処理します。"""
        # ロガーが既にシャットダウンされているか、不安定な状態にある可能性があるため、printも使用します。
        print(f"\nシグナル {signal_number} を受信しました。NGGS-Liteを安全に終了しようとしています...", file=sys.stderr)
        
        # 可能であれば、ログのフラッシュなどを試みますが、シグナルハンドラ内での処理は限定的です。
        try:
            # main関数で設定されたグローバルloggerインスタンスを使用
            if 'logger' in globals() and isinstance(logger, logging.Logger) and logger.handlers:
                logger.warning(f"シグナル {signal_number} により終了処理を開始します。")
                # logging.shutdown() # ここでshutdownすると、mainのfinally節と競合する可能性
            elif entry_point_logger.handlers: # フォールバックとしてこのロガーを使用
                entry_point_logger.warning(f"シグナル {signal_number} により終了処理を開始します (エントリーポイントロガー使用)。")
            else: # ロガーが全く利用できない場合
                print("警告: シグナル受信時にロガーが利用できませんでした。", file=sys.stderr)
        except Exception as e_log_signal:
            print(f"緊急シャットダウン中のロギングエラー: {e_log_signal}", file=sys.stderr)
        
        # シグナルに応じた終了コードを決定
        # SIGINT (Ctrl+C) は通常130
        # SIGTERM は通常143
        exit_code_from_signal = 128 + signal_number # 一般的な慣習
        sys.exit(exit_code_from_signal)

    try:
        signal.signal(signal.SIGINT, graceful_signal_handler)  # Ctrl+C を処理
        signal.signal(signal.SIGTERM, graceful_signal_handler) # 'kill' コマンドを処理
    except (AttributeError, ValueError, OSError, ImportError) as e_signal_setup:
        # AttributeError: signalモジュールが完全でない (一部のWindows Python環境など)
        # ValueError: 無効なシグナル番号
        # OSError: OSレベルのエラー
        # ImportError: signalモジュール自体が見つからない (標準Pythonでは稀)
        warning_msg_signal = (
            f"警告: シグナルハンドラの設定に失敗しました ({type(e_signal_setup).__name__}: {e_signal_setup})。"
            "SIGINT/SIGTERMによる安全な終了が機能しない可能性があります。"
        )
        print(warning_msg_signal, file=sys.stderr)
        if entry_point_logger.handlers:
            entry_point_logger.warning(warning_msg_signal)


    # --- オプションモジュールの利用可能性チェックとユーザーへの通知 ---
    # これらのチェックは、オプション機能が制限される可能性があることをユーザーに通知します。
    # モジュール欠落の実際の処理は、それらを使用するコンポーネント内で行われるべきです。

    # statisticsモジュールの確認 (RIEvaluatorで使用)
    if 'statistics' not in sys.modules:
        try:
            import statistics # インポート試行
        except ImportError:
            # このメッセージはmain()内のロガーが設定された後に表示される方が望ましいが、
            # エントリーポイントでの早期チェックとしてここにも配置
            missing_stats_msg = (
                "警告: 'statistics' モジュールが見つかりません。"
                "RIEvaluatorの一部の計算（標準偏差など）の精度が低下するか、機能しない可能性があります。"
            )
            print(missing_stats_msg, file=sys.stderr)
            if entry_point_logger.handlers: entry_point_logger.warning(missing_stats_msg)

    # concurrent.futuresモジュールの確認 (BatchProcessorで使用)
    # CONCURRENT_FUTURES_AVAILABLE はPart 1のインポートセクションで設定される想定
    if not CONCURRENT_FUTURES_AVAILABLE:
        missing_concurrent_msg = (
            "警告: 'concurrent.futures' モジュールが見つからないか、インポートに失敗しました。"
            "バッチ処理の並列実行 (--max-workers > 1) は無効になり、逐次処理で実行されます。"
        )
        print(missing_concurrent_msg, file=sys.stderr)
        if entry_point_logger.handlers: entry_point_logger.warning(missing_concurrent_msg)
    
    # json_repairモジュールの確認 (Evaluatorでオプションとして使用検討)
    try:
        import json_repair # type: ignore
        JSON_REPAIR_AVAILABLE = True
        if entry_point_logger.handlers: entry_point_logger.debug("json_repairライブラリが利用可能です。")
    except ImportError:
        JSON_REPAIR_AVAILABLE = False
        if entry_point_logger.handlers: entry_point_logger.info("json_repairライブラリは見つかりませんでした。JSON修復機能は無効です。")


    # --- メイン実行ロジックの呼び出し ---
    # main()関数を呼び出し、返されたコードで終了します。
    final_script_exit_code = 0 # デフォルトは0 (成功)
    try:
        # main() 関数はPart 17で定義済み
        final_script_exit_code = main()
    except Exception as e_main_uncaught:
        # これは、main()内でキャッチされなかった本当に予期しないエラーのためのフォールバックです。
        # main()自体が広範なエラーハンドリングを持っています。
        critical_error_msg_top_level = f"スクリプト実行の最上位レベルで予期せぬエラーが発生しました: {type(e_main_uncaught).__name__} - {e_main_uncaught}"
        print(critical_error_msg_top_level, file=sys.stderr)
        traceback.print_exc() # スタックトレースも出力

        # この段階ではloggerが機能しているか不明なため、標準エラー出力も使用
        try:
            # グローバルloggerが設定されていれば、そこにも記録
            if 'logger' in globals() and isinstance(logger, logging.Logger) and logger.handlers:
                 logger.critical("スクリプトエントリーポイントでキャッチされない例外が発生しました。", exc_info=True)
            elif entry_point_logger.handlers: # main()前のロガーがまだ使える場合
                 entry_point_logger.critical("スクリプトエントリーポイントでキャッチされない例外が発生しました。", exc_info=True)
        except Exception as e_log_critical: # クリティカルエラー中のロギングエラーは無視
            print(f"クリティカルエラー発生時のロギング試行中にもエラー: {e_log_critical}", file=sys.stderr)
        
        final_script_exit_code = 1 # 一般的なエラーを示す終了コード
    finally:
        # 常にsys.exitが呼び出されることを保証
        if entry_point_logger.handlers: # main()が呼ばれなかった場合でもログ試行
            entry_point_logger.info(f"スクリプト終了処理。最終終了コード: {final_script_exit_code}")
        else:
            print(f"スクリプト終了処理。最終終了コード: {final_script_exit_code}", file=sys.stderr)
        
        # logging.shutdown() は main() の finally ブロックで呼び出される想定
        # ここで再度呼び出すと問題を起こす可能性があるため、main()に任せる
        sys.exit(final_script_exit_code)

# =============================================================================
# End of Script (NGGS-Lite v1.8.0 Complete Refactored)
# =============================================================================


